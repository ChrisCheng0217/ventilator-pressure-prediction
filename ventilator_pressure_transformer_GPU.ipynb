{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ventilator-pressure-transformer_GPU.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 5851.539198,
      "end_time": "2021-10-05T21:57:27.162714",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2021-10-05T20:19:55.623516",
      "version": "2.3.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bobbercheng/ventilator-pressure-prediction/blob/master/ventilator_pressure_transformer_GPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRRDmcGpsh4u"
      },
      "source": [
        "V1: Add PositionalEncoding"
      ],
      "id": "TRRDmcGpsh4u"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPJR-B_AhTBC"
      },
      "source": [
        "#!pip install pandas==1.3.2"
      ],
      "id": "EPJR-B_AhTBC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oH5W_yWm-P8P",
        "outputId": "9b64a639-5c21-4d4c-b364-f840b7595786"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/gdrive\", force_remount=True)"
      ],
      "id": "oH5W_yWm-P8P",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhjzjzp5etAG",
        "outputId": "85490501-7f68-45f6-cc65-a7c5465c5397"
      },
      "source": [
        "!pip install kaggle\n",
        "!mkdir /root/.kaggle\n",
        "!cp /gdrive/MyDrive/ventilator-pressure-prediction/kaggle.json /root/.kaggle\n",
        "!kaggle competitions download -c ventilator-pressure-prediction\n",
        "!mkdir -p /kaggle/input/ventilator-pressure-prediction\n",
        "!unzip '*.zip' -d /kaggle/input/ventilator-pressure-prediction\n",
        "!ls /kaggle/input/ventilator-pressure-prediction"
      ],
      "id": "mhjzjzp5etAG",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.5.30)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.62.3)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4)\n",
            "Downloading test.csv.zip to /content\n",
            " 74% 56.0M/75.4M [00:00<00:00, 96.8MB/s]\n",
            "100% 75.4M/75.4M [00:00<00:00, 119MB/s] \n",
            "Downloading sample_submission.csv.zip to /content\n",
            "  0% 0.00/8.50M [00:00<?, ?B/s]\n",
            "100% 8.50M/8.50M [00:00<00:00, 139MB/s]\n",
            "Downloading train.csv.zip to /content\n",
            " 98% 137M/139M [00:00<00:00, 148MB/s]\n",
            "100% 139M/139M [00:00<00:00, 151MB/s]\n",
            "Archive:  sample_submission.csv.zip\n",
            "  inflating: /kaggle/input/ventilator-pressure-prediction/sample_submission.csv  \n",
            "\n",
            "Archive:  test.csv.zip\n",
            "  inflating: /kaggle/input/ventilator-pressure-prediction/test.csv  \n",
            "\n",
            "Archive:  train.csv.zip\n",
            "  inflating: /kaggle/input/ventilator-pressure-prediction/train.csv  \n",
            "\n",
            "3 archives were successfully processed.\n",
            "sample_submission.csv  test.csv  train.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a16844ae"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# import optuna\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler, ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error as mae\n",
        "from sklearn.preprocessing import RobustScaler, normalize\n",
        "from sklearn.model_selection import train_test_split, GroupKFold, KFold\n",
        "\n",
        "from IPython.display import display\n",
        "\n",
        "model_folder = '/gdrive/MyDrive/ventilator-pressure-prediction/transformer/'"
      ],
      "id": "a16844ae",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEHQxSneTcVX"
      },
      "source": [
        "import os\n",
        "import random\n",
        "# Deep Learning\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "# Metrics\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Random Seed Initialize\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "def seed_everything(seed=RANDOM_SEED):\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    \n",
        "seed_everything()"
      ],
      "id": "LEHQxSneTcVX",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "564144b2"
      },
      "source": [
        "DEBUG = False\n",
        "\n",
        "train = pd.read_csv('/kaggle/input/ventilator-pressure-prediction/train.csv')\n",
        "test = pd.read_csv('/kaggle/input/ventilator-pressure-prediction/test.csv')\n",
        "submission = pd.read_csv('/kaggle/input/ventilator-pressure-prediction/sample_submission.csv')\n",
        "\n",
        "if DEBUG:\n",
        "    train = train[:80*1000]"
      ],
      "id": "564144b2",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08fd8094",
        "outputId": "74824d59-d6db-49f8-9cc4-785ff0eb68e0"
      },
      "source": [
        "def add_features(df):\n",
        "    df['cross']= df['u_in'] * df['u_out']\n",
        "    df['cross2']= df['time_step'] * df['u_out']\n",
        "    df['area'] = df['time_step'] * df['u_in']\n",
        "    df['area'] = df.groupby('breath_id')['area'].cumsum()\n",
        "    df['u_in_cumsum'] = (df['u_in']).groupby(df['breath_id']).cumsum()\n",
        "    print(\"Step-1...Completed\")\n",
        "    \n",
        "    df['u_in_lag1'] = df.groupby('breath_id')['u_in'].shift(1)\n",
        "    df['u_out_lag1'] = df.groupby('breath_id')['u_out'].shift(1)\n",
        "    df['u_in_lag_back1'] = df.groupby('breath_id')['u_in'].shift(-1)\n",
        "    df['u_out_lag_back1'] = df.groupby('breath_id')['u_out'].shift(-1)\n",
        "    df['u_in_lag2'] = df.groupby('breath_id')['u_in'].shift(2)\n",
        "    df['u_out_lag2'] = df.groupby('breath_id')['u_out'].shift(2)\n",
        "    df['u_in_lag_back2'] = df.groupby('breath_id')['u_in'].shift(-2)\n",
        "    df['u_out_lag_back2'] = df.groupby('breath_id')['u_out'].shift(-2)\n",
        "    df['u_in_lag3'] = df.groupby('breath_id')['u_in'].shift(3)\n",
        "    df['u_out_lag3'] = df.groupby('breath_id')['u_out'].shift(3)\n",
        "    df['u_in_lag_back3'] = df.groupby('breath_id')['u_in'].shift(-3)\n",
        "    df['u_out_lag_back3'] = df.groupby('breath_id')['u_out'].shift(-3)\n",
        "    df['u_in_lag4'] = df.groupby('breath_id')['u_in'].shift(4)\n",
        "    df['u_out_lag4'] = df.groupby('breath_id')['u_out'].shift(4)\n",
        "    df['u_in_lag_back4'] = df.groupby('breath_id')['u_in'].shift(-4)\n",
        "    df['u_out_lag_back4'] = df.groupby('breath_id')['u_out'].shift(-4)\n",
        "    df = df.fillna(0)\n",
        "    print(\"Step-2...Completed\")\n",
        "    \n",
        "    df['breath_id__u_in__max'] = df.groupby(['breath_id'])['u_in'].transform('max')\n",
        "    df['breath_id__u_out__max'] = df.groupby(['breath_id'])['u_out'].transform('max')\n",
        "    df['breath_id__u_in__diffmax'] = df.groupby(['breath_id'])['u_in'].transform('max') - df['u_in']\n",
        "    df['breath_id__u_in__diffmean'] = df.groupby(['breath_id'])['u_in'].transform('mean') - df['u_in']\n",
        "    df['breath_id__u_in__diffmax'] = df.groupby(['breath_id'])['u_in'].transform('max') - df['u_in']\n",
        "    df['breath_id__u_in__diffmean'] = df.groupby(['breath_id'])['u_in'].transform('mean') - df['u_in']\n",
        "    print(\"Step-3...Completed\")\n",
        "    \n",
        "    df['u_in_diff1'] = df['u_in'] - df['u_in_lag1']\n",
        "    df['u_out_diff1'] = df['u_out'] - df['u_out_lag1']\n",
        "    df['u_in_diff2'] = df['u_in'] - df['u_in_lag2']\n",
        "    df['u_out_diff2'] = df['u_out'] - df['u_out_lag2']\n",
        "    df['u_in_diff3'] = df['u_in'] - df['u_in_lag3']\n",
        "    df['u_out_diff3'] = df['u_out'] - df['u_out_lag3']\n",
        "    df['u_in_diff4'] = df['u_in'] - df['u_in_lag4']\n",
        "    df['u_out_diff4'] = df['u_out'] - df['u_out_lag4']\n",
        "    print(\"Step-4...Completed\")\n",
        "    \n",
        "    df['one'] = 1\n",
        "    df['count'] = (df['one']).groupby(df['breath_id']).cumsum()\n",
        "    df['u_in_cummean'] =df['u_in_cumsum'] /df['count']\n",
        "    \n",
        "    df['breath_id_lag']=df['breath_id'].shift(1).fillna(0)\n",
        "    df['breath_id_lag2']=df['breath_id'].shift(2).fillna(0)\n",
        "    df['breath_id_lagsame']=np.select([df['breath_id_lag']==df['breath_id']],[1],0)\n",
        "    df['breath_id_lag2same']=np.select([df['breath_id_lag2']==df['breath_id']],[1],0)\n",
        "    df['breath_id__u_in_lag'] = df['u_in'].shift(1).fillna(0)\n",
        "    df['breath_id__u_in_lag'] = df['breath_id__u_in_lag'] * df['breath_id_lagsame']\n",
        "    df['breath_id__u_in_lag2'] = df['u_in'].shift(2).fillna(0)\n",
        "    df['breath_id__u_in_lag2'] = df['breath_id__u_in_lag2'] * df['breath_id_lag2same']\n",
        "    \n",
        "    df['R'] = df['R'].astype(str)\n",
        "    df['C'] = df['C'].astype(str)\n",
        "    df['R__C'] = df[\"R\"].astype(str) + '__' + df[\"C\"].astype(str)\n",
        "    df = pd.get_dummies(df)\n",
        "    print(\"Step-5...Completed\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "\n",
        "train = add_features(train)\n",
        "test = add_features(test)"
      ],
      "id": "08fd8094",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step-1...Completed\n",
            "Step-2...Completed\n",
            "Step-3...Completed\n",
            "Step-4...Completed\n",
            "Step-5...Completed\n",
            "Step-1...Completed\n",
            "Step-2...Completed\n",
            "Step-3...Completed\n",
            "Step-4...Completed\n",
            "Step-5...Completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "7d9dcaa1",
        "outputId": "d057b29e-ceef-4851-c436-13c462c2b394"
      },
      "source": [
        "train.head()"
      ],
      "id": "7d9dcaa1",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>breath_id</th>\n",
              "      <th>time_step</th>\n",
              "      <th>u_in</th>\n",
              "      <th>u_out</th>\n",
              "      <th>pressure</th>\n",
              "      <th>cross</th>\n",
              "      <th>cross2</th>\n",
              "      <th>area</th>\n",
              "      <th>u_in_cumsum</th>\n",
              "      <th>u_in_lag1</th>\n",
              "      <th>u_out_lag1</th>\n",
              "      <th>u_in_lag_back1</th>\n",
              "      <th>u_out_lag_back1</th>\n",
              "      <th>u_in_lag2</th>\n",
              "      <th>u_out_lag2</th>\n",
              "      <th>u_in_lag_back2</th>\n",
              "      <th>u_out_lag_back2</th>\n",
              "      <th>u_in_lag3</th>\n",
              "      <th>u_out_lag3</th>\n",
              "      <th>u_in_lag_back3</th>\n",
              "      <th>u_out_lag_back3</th>\n",
              "      <th>u_in_lag4</th>\n",
              "      <th>u_out_lag4</th>\n",
              "      <th>u_in_lag_back4</th>\n",
              "      <th>u_out_lag_back4</th>\n",
              "      <th>breath_id__u_in__max</th>\n",
              "      <th>breath_id__u_out__max</th>\n",
              "      <th>breath_id__u_in__diffmax</th>\n",
              "      <th>breath_id__u_in__diffmean</th>\n",
              "      <th>u_in_diff1</th>\n",
              "      <th>u_out_diff1</th>\n",
              "      <th>u_in_diff2</th>\n",
              "      <th>u_out_diff2</th>\n",
              "      <th>u_in_diff3</th>\n",
              "      <th>u_out_diff3</th>\n",
              "      <th>u_in_diff4</th>\n",
              "      <th>u_out_diff4</th>\n",
              "      <th>one</th>\n",
              "      <th>count</th>\n",
              "      <th>u_in_cummean</th>\n",
              "      <th>breath_id_lag</th>\n",
              "      <th>breath_id_lag2</th>\n",
              "      <th>breath_id_lagsame</th>\n",
              "      <th>breath_id_lag2same</th>\n",
              "      <th>breath_id__u_in_lag</th>\n",
              "      <th>breath_id__u_in_lag2</th>\n",
              "      <th>R_20</th>\n",
              "      <th>R_5</th>\n",
              "      <th>R_50</th>\n",
              "      <th>C_10</th>\n",
              "      <th>C_20</th>\n",
              "      <th>C_50</th>\n",
              "      <th>R__C_20__10</th>\n",
              "      <th>R__C_20__20</th>\n",
              "      <th>R__C_20__50</th>\n",
              "      <th>R__C_50__10</th>\n",
              "      <th>R__C_50__20</th>\n",
              "      <th>R__C_50__50</th>\n",
              "      <th>R__C_5__10</th>\n",
              "      <th>R__C_5__20</th>\n",
              "      <th>R__C_5__50</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.083334</td>\n",
              "      <td>0</td>\n",
              "      <td>5.837492</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.083334</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.383041</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>22.509278</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>22.808822</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>25.355850</td>\n",
              "      <td>0.0</td>\n",
              "      <td>28.313036</td>\n",
              "      <td>1</td>\n",
              "      <td>28.229702</td>\n",
              "      <td>10.062673</td>\n",
              "      <td>0.083334</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.083334</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.083334</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.083334</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.083334</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0.033652</td>\n",
              "      <td>18.383041</td>\n",
              "      <td>0</td>\n",
              "      <td>5.907794</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.618632</td>\n",
              "      <td>18.466375</td>\n",
              "      <td>0.083334</td>\n",
              "      <td>0.0</td>\n",
              "      <td>22.509278</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>22.808822</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>25.355850</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>27.259866</td>\n",
              "      <td>0.0</td>\n",
              "      <td>28.313036</td>\n",
              "      <td>1</td>\n",
              "      <td>9.929994</td>\n",
              "      <td>-8.237035</td>\n",
              "      <td>18.299707</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.383041</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.383041</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.383041</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>9.233188</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.083334</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0.067514</td>\n",
              "      <td>22.509278</td>\n",
              "      <td>0</td>\n",
              "      <td>7.876254</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.138333</td>\n",
              "      <td>40.975653</td>\n",
              "      <td>18.383041</td>\n",
              "      <td>0.0</td>\n",
              "      <td>22.808822</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.083334</td>\n",
              "      <td>0.0</td>\n",
              "      <td>25.355850</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>27.259866</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>27.127486</td>\n",
              "      <td>0.0</td>\n",
              "      <td>28.313036</td>\n",
              "      <td>1</td>\n",
              "      <td>5.803758</td>\n",
              "      <td>-12.363271</td>\n",
              "      <td>4.126236</td>\n",
              "      <td>0.0</td>\n",
              "      <td>22.425944</td>\n",
              "      <td>0.0</td>\n",
              "      <td>22.509278</td>\n",
              "      <td>0.0</td>\n",
              "      <td>22.509278</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>13.658551</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>18.383041</td>\n",
              "      <td>0.083334</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0.101542</td>\n",
              "      <td>22.808822</td>\n",
              "      <td>0</td>\n",
              "      <td>11.742872</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.454391</td>\n",
              "      <td>63.784476</td>\n",
              "      <td>22.509278</td>\n",
              "      <td>0.0</td>\n",
              "      <td>25.355850</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.383041</td>\n",
              "      <td>0.0</td>\n",
              "      <td>27.259866</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.083334</td>\n",
              "      <td>0.0</td>\n",
              "      <td>27.127486</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>26.807732</td>\n",
              "      <td>0.0</td>\n",
              "      <td>28.313036</td>\n",
              "      <td>1</td>\n",
              "      <td>5.504214</td>\n",
              "      <td>-12.662816</td>\n",
              "      <td>0.299544</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.425781</td>\n",
              "      <td>0.0</td>\n",
              "      <td>22.725488</td>\n",
              "      <td>0.0</td>\n",
              "      <td>22.808822</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>15.946119</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>22.509278</td>\n",
              "      <td>18.383041</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0.135756</td>\n",
              "      <td>25.355850</td>\n",
              "      <td>0</td>\n",
              "      <td>12.234987</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.896588</td>\n",
              "      <td>89.140326</td>\n",
              "      <td>22.808822</td>\n",
              "      <td>0.0</td>\n",
              "      <td>27.259866</td>\n",
              "      <td>0.0</td>\n",
              "      <td>22.509278</td>\n",
              "      <td>0.0</td>\n",
              "      <td>27.127486</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.383041</td>\n",
              "      <td>0.0</td>\n",
              "      <td>26.807732</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.083334</td>\n",
              "      <td>0.0</td>\n",
              "      <td>27.864715</td>\n",
              "      <td>0.0</td>\n",
              "      <td>28.313036</td>\n",
              "      <td>1</td>\n",
              "      <td>2.957185</td>\n",
              "      <td>-15.209844</td>\n",
              "      <td>2.547028</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.846573</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.972809</td>\n",
              "      <td>0.0</td>\n",
              "      <td>25.272516</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>17.828065</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>22.808822</td>\n",
              "      <td>22.509278</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  breath_id  time_step  ...  R__C_5__10  R__C_5__20  R__C_5__50\n",
              "0   1          1   0.000000  ...           0           0           0\n",
              "1   2          1   0.033652  ...           0           0           0\n",
              "2   3          1   0.067514  ...           0           0           0\n",
              "3   4          1   0.101542  ...           0           0           0\n",
              "4   5          1   0.135756  ...           0           0           0\n",
              "\n",
              "[5 rows x 62 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2da0be1",
        "outputId": "76cbb467-bd42-4aee-f9a1-0c51389d1288"
      },
      "source": [
        "train.shape"
      ],
      "id": "e2da0be1",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6036000, 62)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "431a7ee7"
      },
      "source": [
        "targets = train[['pressure']].to_numpy().reshape(-1, 80)\n",
        "#u_outs = train[['u_out']].to_numpy().reshape(-1, 80)\n",
        "train.drop(['pressure', 'id', 'breath_id'], axis=1, inplace=True)\n",
        "test = test.drop(['id', 'breath_id'], axis=1)"
      ],
      "id": "431a7ee7",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d24ef65c"
      },
      "source": [
        "RS = RobustScaler()\n",
        "train = RS.fit_transform(train)\n",
        "test = RS.transform(test)"
      ],
      "id": "d24ef65c",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6f1a3e19"
      },
      "source": [
        "train = train.reshape(-1, 80, train.shape[-1])\n",
        "test = test.reshape(-1, 80, train.shape[-1])"
      ],
      "id": "6f1a3e19",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b223fa1b"
      },
      "source": [
        "def GBVPP_loss(y_true, y_pred, cols = 80):\n",
        "    u_out = y_true[:, cols: ]\n",
        "    y = y_true[:, :cols ]\n",
        "\n",
        "    w = 1 - u_out\n",
        "    mae = w * tf.abs(y - y_pred)\n",
        "    return tf.reduce_sum(mae, axis=-1) / tf.reduce_sum(w, axis=-1)"
      ],
      "id": "b223fa1b",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6htVxW6c-Tvn"
      },
      "source": [
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "class Attention(Layer):\n",
        "    \n",
        "    def __init__(self, return_sequences=True):\n",
        "        self.return_sequences = return_sequences\n",
        "        super(Attention,self).__init__()\n",
        "        \n",
        "    def build(self, input_shape):\n",
        "        \n",
        "        self.W=self.add_weight(name=\"att_weight\", shape=(input_shape[-1],1),\n",
        "                               initializer=\"normal\")\n",
        "        self.b=self.add_weight(name=\"att_bias\", shape=(input_shape[1],1),\n",
        "                               initializer=\"zeros\")\n",
        "        \n",
        "        super(Attention,self).build(input_shape)\n",
        "        \n",
        "    def call(self, x):\n",
        "        \n",
        "        e = K.tanh(K.dot(x,self.W)+self.b)\n",
        "        a = K.softmax(e, axis=1)\n",
        "        output = x*a\n",
        "        \n",
        "        if self.return_sequences:\n",
        "            return output\n",
        "        \n",
        "        return K.sum(output, axis=1)\n",
        "\n",
        "    def get_config(self):\n",
        "        return super(Attention, self).get_config()"
      ],
      "id": "6htVxW6c-Tvn",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqwDxf7k729C"
      },
      "source": [
        "def get_cnn_model(train):\n",
        "  inputs = keras.layers.Input(shape=train.shape[-2:])\n",
        "  X = inputs\n",
        "\n",
        "  X = keras.layers.Conv1D(filters=2048, kernel_size=3, activation='relu')(X)\n",
        "  X = keras.layers.BatchNormalization()(X)\n",
        "  X = keras.layers.Conv1D(filters=2048, kernel_size=3, activation='relu')(X)\n",
        "  X = keras.layers.BatchNormalization()(X)\n",
        "  X = keras.layers.AveragePooling1D(pool_size=2)(X)\n",
        "\n",
        "  X = Attention(return_sequences=True)(X)\n",
        "\n",
        "  X = keras.layers.Conv1D(filters=1024, kernel_size=3, activation='relu')(X)\n",
        "  X = keras.layers.BatchNormalization()(X)\n",
        "  X = keras.layers.Conv1D(filters=1024, kernel_size=3, activation='relu')(X)\n",
        "  X = keras.layers.BatchNormalization()(X)\n",
        "  X = keras.layers.AveragePooling1D(pool_size=2)(X)\n",
        "\n",
        "  X = Attention(return_sequences=True)(X)\n",
        "\n",
        "\n",
        "  X = keras.layers.Conv1D(filters=512, kernel_size=3, activation='relu')(X)\n",
        "  X = keras.layers.BatchNormalization()(X)\n",
        "  X = keras.layers.Conv1D(filters=512, kernel_size=3, activation='relu')(X)\n",
        "  X = keras.layers.BatchNormalization()(X)\n",
        "  X = keras.layers.AveragePooling1D(pool_size=2)(X)\n",
        "\n",
        "  X = Attention(return_sequences=True)(X)\n",
        "\n",
        "  X = keras.layers.Conv1D(filters=256, kernel_size=3, activation='relu')(X)\n",
        "  X = keras.layers.BatchNormalization()(X)\n",
        "  X = keras.layers.Conv1D(filters=256, kernel_size=3, activation='relu')(X)\n",
        "  X = keras.layers.BatchNormalization()(X)\n",
        "  X = keras.layers.AveragePooling1D(pool_size=2)(X)\n",
        "\n",
        "  X = Attention(return_sequences=True)(X)\n",
        "\n",
        "  X = keras.layers.Flatten()(X) #256\n",
        "\n",
        "  input_X = keras.layers.Flatten()(inputs)\n",
        "  input_X = keras.layers.Dense(512, activation='selu', kernel_initializer='lecun_normal')(input_X)\n",
        "  input_X = keras.layers.BatchNormalization()(input_X)\n",
        "  input_X = keras.layers.Dense(256, activation='selu', kernel_initializer='lecun_normal')(input_X)\n",
        "  input_X = keras.layers.BatchNormalization()(input_X) #256\n",
        "\n",
        "  X = keras.layers.Concatenate()([X, input_X])\n",
        "\n",
        "  X = keras.layers.Dense(256, activation='selu', kernel_initializer='lecun_normal')(X)\n",
        "  X = keras.layers.BatchNormalization()(X)\n",
        "  outputs = keras.layers.Dense(80)(X)\n",
        "  model  = keras.Model(inputs=inputs, outputs=outputs)\n",
        "  model.summary()\n",
        "  return model\n"
      ],
      "id": "aqwDxf7k729C",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4lOIi9HdExc"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras"
      ],
      "id": "m4lOIi9HdExc",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqA2324UJKb_"
      },
      "source": [
        "def get_angles(pos, i, d_model):\n",
        "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "  return pos * angle_rates\n",
        "\n",
        "def get_positional_encoding(position, d_model):\n",
        "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model)\n",
        "\n",
        "  # apply sin to even indices in the array; 2i\n",
        "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "  # apply cos to odd indices in the array; 2i+1\n",
        "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "  pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "id": "dqA2324UJKb_",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5MeMc-0ddTj"
      },
      "source": [
        "#refer to https://keras.io/examples/timeseries/timeseries_classification_transformer/\n",
        "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
        "    # Normalization and Attention\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
        "    x = layers.MultiHeadAttention(\n",
        "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
        "    )(x, x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    res = x + inputs\n",
        "\n",
        "    # Feed Forward Part\n",
        "    # Feed forward can be Den lay or RNN as long as it keep final dimension\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
        "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
        "    return x + res"
      ],
      "id": "i5MeMc-0ddTj",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1uCDdywdhRH"
      },
      "source": [
        "# refer to https://www.tensorflow.org/text/tutorials/transformer#encoder\n",
        "# https://rubikscode.net/2019/08/19/transformer-with-python-and-tensorflow-2-0-encoder-decoder/\n",
        "# https://trungtran.io/2019/04/29/create-the-transformer-with-tensorflow-2-0/\n",
        "# https://github.com/ChunML/NLP/blob/master/chatbot/model.py\n",
        "def build_transfer_model(\n",
        "    input_shape,\n",
        "    head_size=256,\n",
        "    num_heads=4,\n",
        "    ff_dim=512,\n",
        "    num_transformer_blocks=4,\n",
        "    mlp_units=[128, 1],\n",
        "    dropout=0.01,\n",
        "    mlp_dropout=0.01,\n",
        "):\n",
        "\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    time_lenght = input_shape[0]\n",
        "    positional_encoding = get_positional_encoding(time_lenght, input_shape[-1])\n",
        "    print('positional_encoding:', positional_encoding)\n",
        "\n",
        "    x = inputs\n",
        "\n",
        "    # increase x then add position encoding.\n",
        "    '''\n",
        "    The reason we increase the embedding values before the addition is to make \n",
        "    the positional encoding relatively smaller. This means the original meaning \n",
        "    in the embedding vector won’t be lost when we add them together.\n",
        "    '''\n",
        "    x *= tf.math.sqrt(tf.cast(input_shape[-1], tf.float32))\n",
        "    x += positional_encoding[:, :time_lenght, :]\n",
        "\n",
        "    for _ in range(num_transformer_blocks):\n",
        "        x = transformer_encoder(x, input_shape[-1], num_heads, ff_dim, dropout)\n",
        "\n",
        "    x_pool = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
        "    x_pool = tf.expand_dims(x_pool, -1)\n",
        "\n",
        "    for dim in mlp_units:\n",
        "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
        "        x = layers.Dropout(mlp_dropout)(x)\n",
        "\n",
        "    outputs = layers.Dense(dim, activation=\"relu\")(x + x_pool)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    model.summary()\n",
        "    return model"
      ],
      "id": "b1uCDdywdhRH",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvagsDTSiXYs"
      },
      "source": [
        "# model = build_transfer_model(train.shape[1:])\n",
        "# del model"
      ],
      "id": "BvagsDTSiXYs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WeSVrUJUWs-"
      },
      "source": [
        "# Function to get hardware strategy\n",
        "def get_hardware_strategy():\n",
        "    try:\n",
        "        # TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
        "        # set: this is always the case on Kaggle.\n",
        "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "        print('Running on TPU ', tpu.master())\n",
        "    except ValueError:\n",
        "        tpu = None\n",
        "\n",
        "    if tpu:\n",
        "        tf.config.experimental_connect_to_cluster(tpu)\n",
        "        tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "        tf.config.optimizer.set_jit(True)\n",
        "    else:\n",
        "        # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
        "        strategy = tf.distribute.get_strategy()\n",
        "\n",
        "    return tpu, strategy\n",
        "\n",
        "tpu, strategy = get_hardware_strategy()"
      ],
      "id": "2WeSVrUJUWs-",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12a089a5",
        "outputId": "a398c6a2-9197-4830-e2a8-d08d2e3b917e"
      },
      "source": [
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
        "EPOCH = 300\n",
        "BATCH_SIZE = 512\n",
        "NUM_FOLDS = 5\n",
        "\n",
        "with strategy.scope():\n",
        "    kf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=2021)\n",
        "    test_preds = []\n",
        "    train_preds = []\n",
        "    for fold, (train_idx, test_idx) in enumerate(kf.split(train, targets)):\n",
        "        print('-'*15, '>', f'Fold {fold+1}', '<', '-'*15)\n",
        "        X_train, X_valid = train[train_idx], train[test_idx]\n",
        "        y_train, y_valid = targets[train_idx], targets[test_idx]\n",
        "        #u_out_train, u_out_valid = u_outs[train_idx], u_outs[test_idx]\n",
        "        \n",
        "        #model = get_cnn_model(train)\n",
        "        model = build_transfer_model(train.shape[1:])\n",
        "        model.compile(optimizer='adam', \n",
        "                      # loss=GBVPP_loss,\n",
        "                      loss=\"mae\"\n",
        "                      )\n",
        "\n",
        "        #scheduler = ExponentialDecay(1e-3, 40*((len(train)*0.8)/BATCH_SIZE), 1e-5)\n",
        "        #lr = LearningRateScheduler(scheduler, verbose=1)\n",
        "        lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.75, patience=10, verbose=1)\n",
        "        es = EarlyStopping(monitor=\"val_loss\", patience=50, verbose=1, mode=\"min\", restore_best_weights=True)\n",
        "        checkpoint_filepath = f\"folds{fold}.hdf5\"\n",
        "        sv = keras.callbacks.ModelCheckpoint(\n",
        "            checkpoint_filepath, monitor='val_loss', verbose=1, save_best_only=True,\n",
        "            save_weights_only=False, mode='auto', save_freq='epoch',\n",
        "            options=None)\n",
        "\n",
        "        history = model.fit(X_train,\n",
        "                            y_train,\n",
        "                            # validation_data=(X_valid, np.append(y_valid, u_out_valid, axis =1)), \n",
        "                            validation_data=(X_valid, y_valid), \n",
        "                            epochs=EPOCH, \n",
        "                            batch_size=1024, \n",
        "                            callbacks=[lr,\n",
        "                                       es,\n",
        "                                       sv])\n",
        "        \n",
        "        with open(model_folder+checkpoint_filepath, 'wb') as f:\n",
        "          f.write(open(checkpoint_filepath, 'rb').read())\n",
        "    \n",
        "        test_preds.append(model.predict(test).squeeze().reshape(-1, 1).squeeze())\n",
        "        train_preds.append(model.predict(train).squeeze().reshape(-1, 1).squeeze())\n"
      ],
      "id": "12a089a5",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------- > Fold 1 < ---------------\n",
            "positional_encoding: tf.Tensor(\n",
            "[[[ 0.0000000e+00  1.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
            "    1.0000000e+00  0.0000000e+00]\n",
            "  [ 8.4147096e-01  5.4030228e-01  6.6822791e-01 ...  1.5973122e-04\n",
            "    1.0000000e+00  1.1689518e-04]\n",
            "  [ 9.0929741e-01 -4.1614684e-01  9.9426514e-01 ...  3.1946244e-04\n",
            "    9.9999994e-01  2.3379036e-04]\n",
            "  ...\n",
            "  [ 9.9952018e-01 -3.0975033e-02 -1.9690752e-01 ...  1.2298995e-02\n",
            "    9.9992436e-01  9.0008071e-03]\n",
            "  [ 5.1397848e-01 -8.5780311e-01  5.0865471e-01 ...  1.2458714e-02\n",
            "    9.9992239e-01  9.1176983e-03]\n",
            "  [-4.4411266e-01 -8.9597094e-01  9.5374161e-01 ...  1.2618432e-02\n",
            "    9.9992037e-01  9.2345877e-03]]], shape=(1, 80, 59), dtype=float32)\n",
            "Model: \"model_8\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_9 (InputLayer)            [(None, 80, 59)]     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.multiply_8 (TFOpLambda) (None, 80, 59)       0           input_9[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_128 (TFOpL (None, 80, 59)       0           tf.math.multiply_8[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_112 (LayerN (None, 80, 59)       118         tf.__operators__.add_128[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_56 (MultiH (None, 80, 59)       56463       layer_normalization_112[0][0]    \n",
            "                                                                 layer_normalization_112[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_128 (Dropout)           (None, 80, 59)       0           multi_head_attention_56[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_129 (TFOpL (None, 80, 59)       0           dropout_128[0][0]                \n",
            "                                                                 tf.__operators__.add_128[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_113 (LayerN (None, 80, 59)       118         tf.__operators__.add_129[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_112 (Conv1D)             (None, 80, 512)      30720       layer_normalization_113[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_129 (Dropout)           (None, 80, 512)      0           conv1d_112[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_113 (Conv1D)             (None, 80, 59)       30267       dropout_129[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_130 (TFOpL (None, 80, 59)       0           conv1d_113[0][0]                 \n",
            "                                                                 tf.__operators__.add_129[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_114 (LayerN (None, 80, 59)       118         tf.__operators__.add_130[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_57 (MultiH (None, 80, 59)       56463       layer_normalization_114[0][0]    \n",
            "                                                                 layer_normalization_114[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_130 (Dropout)           (None, 80, 59)       0           multi_head_attention_57[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_131 (TFOpL (None, 80, 59)       0           dropout_130[0][0]                \n",
            "                                                                 tf.__operators__.add_130[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_115 (LayerN (None, 80, 59)       118         tf.__operators__.add_131[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_114 (Conv1D)             (None, 80, 512)      30720       layer_normalization_115[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_131 (Dropout)           (None, 80, 512)      0           conv1d_114[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_115 (Conv1D)             (None, 80, 59)       30267       dropout_131[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_132 (TFOpL (None, 80, 59)       0           conv1d_115[0][0]                 \n",
            "                                                                 tf.__operators__.add_131[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_116 (LayerN (None, 80, 59)       118         tf.__operators__.add_132[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_58 (MultiH (None, 80, 59)       56463       layer_normalization_116[0][0]    \n",
            "                                                                 layer_normalization_116[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_132 (Dropout)           (None, 80, 59)       0           multi_head_attention_58[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_133 (TFOpL (None, 80, 59)       0           dropout_132[0][0]                \n",
            "                                                                 tf.__operators__.add_132[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_117 (LayerN (None, 80, 59)       118         tf.__operators__.add_133[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_116 (Conv1D)             (None, 80, 512)      30720       layer_normalization_117[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_133 (Dropout)           (None, 80, 512)      0           conv1d_116[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_117 (Conv1D)             (None, 80, 59)       30267       dropout_133[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_134 (TFOpL (None, 80, 59)       0           conv1d_117[0][0]                 \n",
            "                                                                 tf.__operators__.add_133[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_118 (LayerN (None, 80, 59)       118         tf.__operators__.add_134[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_59 (MultiH (None, 80, 59)       56463       layer_normalization_118[0][0]    \n",
            "                                                                 layer_normalization_118[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_134 (Dropout)           (None, 80, 59)       0           multi_head_attention_59[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_135 (TFOpL (None, 80, 59)       0           dropout_134[0][0]                \n",
            "                                                                 tf.__operators__.add_134[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_119 (LayerN (None, 80, 59)       118         tf.__operators__.add_135[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_118 (Conv1D)             (None, 80, 512)      30720       layer_normalization_119[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_135 (Dropout)           (None, 80, 512)      0           conv1d_118[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_119 (Conv1D)             (None, 80, 59)       30267       dropout_135[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_136 (TFOpL (None, 80, 59)       0           conv1d_119[0][0]                 \n",
            "                                                                 tf.__operators__.add_135[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "dense_24 (Dense)                (None, 80, 128)      7680        tf.__operators__.add_136[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_136 (Dropout)           (None, 80, 128)      0           dense_24[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_25 (Dense)                (None, 80, 1)        129         dropout_136[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_8 (Glo (None, 80)           0           tf.__operators__.add_136[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_137 (Dropout)           (None, 80, 1)        0           dense_25[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "tf.expand_dims_8 (TFOpLambda)   (None, 80, 1)        0           global_average_pooling1d_8[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_137 (TFOpL (None, 80, 1)        0           dropout_137[0][0]                \n",
            "                                                                 tf.expand_dims_8[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dense_26 (Dense)                (None, 80, 1)        2           tf.__operators__.add_137[0][0]   \n",
            "==================================================================================================\n",
            "Total params: 478,555\n",
            "Trainable params: 478,555\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/300\n",
            "59/59 [==============================] - 21s 283ms/step - loss: 6.0491 - val_loss: 3.2439\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 3.24390, saving model to folds0.hdf5\n",
            "Epoch 2/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 2.8418 - val_loss: 2.2329\n",
            "\n",
            "Epoch 00002: val_loss improved from 3.24390 to 2.23291, saving model to folds0.hdf5\n",
            "Epoch 3/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 1.8984 - val_loss: 1.5573\n",
            "\n",
            "Epoch 00003: val_loss improved from 2.23291 to 1.55729, saving model to folds0.hdf5\n",
            "Epoch 4/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 1.4100 - val_loss: 1.2429\n",
            "\n",
            "Epoch 00004: val_loss improved from 1.55729 to 1.24293, saving model to folds0.hdf5\n",
            "Epoch 5/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 1.1821 - val_loss: 1.1046\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.24293 to 1.10456, saving model to folds0.hdf5\n",
            "Epoch 6/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 1.0564 - val_loss: 0.9694\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.10456 to 0.96941, saving model to folds0.hdf5\n",
            "Epoch 7/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.9698 - val_loss: 0.9373\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.96941 to 0.93735, saving model to folds0.hdf5\n",
            "Epoch 8/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.9132 - val_loss: 0.8684\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.93735 to 0.86841, saving model to folds0.hdf5\n",
            "Epoch 9/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.8720 - val_loss: 0.8132\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.86841 to 0.81322, saving model to folds0.hdf5\n",
            "Epoch 10/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.8426 - val_loss: 0.7863\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.81322 to 0.78633, saving model to folds0.hdf5\n",
            "Epoch 11/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.8124 - val_loss: 0.7903\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.78633\n",
            "Epoch 12/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.7872 - val_loss: 0.7414\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.78633 to 0.74138, saving model to folds0.hdf5\n",
            "Epoch 13/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.7683 - val_loss: 0.7292\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.74138 to 0.72923, saving model to folds0.hdf5\n",
            "Epoch 14/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.7487 - val_loss: 0.7064\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.72923 to 0.70640, saving model to folds0.hdf5\n",
            "Epoch 15/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.7288 - val_loss: 0.7050\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.70640 to 0.70496, saving model to folds0.hdf5\n",
            "Epoch 16/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.7167 - val_loss: 0.6995\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.70496 to 0.69946, saving model to folds0.hdf5\n",
            "Epoch 17/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.7014 - val_loss: 0.6564\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.69946 to 0.65638, saving model to folds0.hdf5\n",
            "Epoch 18/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.6908 - val_loss: 0.6488\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.65638 to 0.64876, saving model to folds0.hdf5\n",
            "Epoch 19/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.6808 - val_loss: 0.6578\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.64876\n",
            "Epoch 20/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.6826 - val_loss: 0.6446\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.64876 to 0.64464, saving model to folds0.hdf5\n",
            "Epoch 21/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.6631 - val_loss: 0.6245\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.64464 to 0.62452, saving model to folds0.hdf5\n",
            "Epoch 22/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.6506 - val_loss: 0.6152\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.62452 to 0.61517, saving model to folds0.hdf5\n",
            "Epoch 23/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.6421 - val_loss: 0.6119\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.61517 to 0.61188, saving model to folds0.hdf5\n",
            "Epoch 24/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.6343 - val_loss: 0.6024\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.61188 to 0.60238, saving model to folds0.hdf5\n",
            "Epoch 25/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.6325 - val_loss: 0.6163\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.60238\n",
            "Epoch 26/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.6214 - val_loss: 0.6025\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.60238\n",
            "Epoch 27/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.6167 - val_loss: 0.5836\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.60238 to 0.58357, saving model to folds0.hdf5\n",
            "Epoch 28/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.6116 - val_loss: 0.6007\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.58357\n",
            "Epoch 29/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.6099 - val_loss: 0.5898\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.58357\n",
            "Epoch 30/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.6064 - val_loss: 0.5688\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.58357 to 0.56884, saving model to folds0.hdf5\n",
            "Epoch 31/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.5946 - val_loss: 0.5951\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.56884\n",
            "Epoch 32/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.5942 - val_loss: 0.5640\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.56884 to 0.56403, saving model to folds0.hdf5\n",
            "Epoch 33/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.5882 - val_loss: 0.5639\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.56403 to 0.56388, saving model to folds0.hdf5\n",
            "Epoch 34/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.5867 - val_loss: 0.5514\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.56388 to 0.55136, saving model to folds0.hdf5\n",
            "Epoch 35/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.5768 - val_loss: 0.5543\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.55136\n",
            "Epoch 36/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.5847 - val_loss: 0.5608\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.55136\n",
            "Epoch 37/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.5732 - val_loss: 0.5417\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.55136 to 0.54166, saving model to folds0.hdf5\n",
            "Epoch 38/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.5777 - val_loss: 0.5414\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.54166 to 0.54143, saving model to folds0.hdf5\n",
            "Epoch 39/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.5623 - val_loss: 0.5411\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.54143 to 0.54105, saving model to folds0.hdf5\n",
            "Epoch 40/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.5597 - val_loss: 0.5405\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.54105 to 0.54053, saving model to folds0.hdf5\n",
            "Epoch 41/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.5524 - val_loss: 0.5282\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.54053 to 0.52821, saving model to folds0.hdf5\n",
            "Epoch 42/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.5526 - val_loss: 0.5190\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.52821 to 0.51904, saving model to folds0.hdf5\n",
            "Epoch 43/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.5456 - val_loss: 0.5329\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.51904\n",
            "Epoch 44/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.5475 - val_loss: 0.5163\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.51904 to 0.51627, saving model to folds0.hdf5\n",
            "Epoch 45/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.5431 - val_loss: 0.5184\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.51627\n",
            "Epoch 46/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.5387 - val_loss: 0.5145\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.51627 to 0.51447, saving model to folds0.hdf5\n",
            "Epoch 47/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.5421 - val_loss: 0.5532\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.51447\n",
            "Epoch 48/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.5406 - val_loss: 0.5036\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.51447 to 0.50356, saving model to folds0.hdf5\n",
            "Epoch 49/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.5292 - val_loss: 0.5059\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.50356\n",
            "Epoch 50/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.5293 - val_loss: 0.5257\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.50356\n",
            "Epoch 51/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.5246 - val_loss: 0.5015\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.50356 to 0.50153, saving model to folds0.hdf5\n",
            "Epoch 52/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.5393 - val_loss: 0.5167\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.50153\n",
            "Epoch 53/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.5190 - val_loss: 0.4964\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.50153 to 0.49641, saving model to folds0.hdf5\n",
            "Epoch 54/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.5176 - val_loss: 0.5158\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.49641\n",
            "Epoch 55/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.5187 - val_loss: 0.4949\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.49641 to 0.49495, saving model to folds0.hdf5\n",
            "Epoch 56/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.5130 - val_loss: 0.4845\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.49495 to 0.48452, saving model to folds0.hdf5\n",
            "Epoch 57/300\n",
            "59/59 [==============================] - 15s 262ms/step - loss: 0.5080 - val_loss: 0.4891\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.48452\n",
            "Epoch 58/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.5128 - val_loss: 0.5059\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.48452\n",
            "Epoch 59/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.5097 - val_loss: 0.4770\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.48452 to 0.47704, saving model to folds0.hdf5\n",
            "Epoch 60/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.5055 - val_loss: 0.4968\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.47704\n",
            "Epoch 61/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.5030 - val_loss: 0.4824\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.47704\n",
            "Epoch 62/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.5020 - val_loss: 0.4723\n",
            "\n",
            "Epoch 00062: val_loss improved from 0.47704 to 0.47227, saving model to folds0.hdf5\n",
            "Epoch 63/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.4968 - val_loss: 0.4801\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.47227\n",
            "Epoch 64/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.5092 - val_loss: 0.4750\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.47227\n",
            "Epoch 65/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.4978 - val_loss: 0.4719\n",
            "\n",
            "Epoch 00065: val_loss improved from 0.47227 to 0.47194, saving model to folds0.hdf5\n",
            "Epoch 66/300\n",
            "59/59 [==============================] - 15s 262ms/step - loss: 0.4910 - val_loss: 0.4795\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.47194\n",
            "Epoch 67/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.4937 - val_loss: 0.4680\n",
            "\n",
            "Epoch 00067: val_loss improved from 0.47194 to 0.46796, saving model to folds0.hdf5\n",
            "Epoch 68/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.4920 - val_loss: 0.4767\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.46796\n",
            "Epoch 69/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.4845 - val_loss: 0.4664\n",
            "\n",
            "Epoch 00069: val_loss improved from 0.46796 to 0.46644, saving model to folds0.hdf5\n",
            "Epoch 70/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.4868 - val_loss: 0.4700\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.46644\n",
            "Epoch 71/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.4883 - val_loss: 0.4595\n",
            "\n",
            "Epoch 00071: val_loss improved from 0.46644 to 0.45945, saving model to folds0.hdf5\n",
            "Epoch 72/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.4825 - val_loss: 0.4558\n",
            "\n",
            "Epoch 00072: val_loss improved from 0.45945 to 0.45583, saving model to folds0.hdf5\n",
            "Epoch 73/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.4779 - val_loss: 0.4787\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.45583\n",
            "Epoch 74/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.4803 - val_loss: 0.4706\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.45583\n",
            "Epoch 75/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.4754 - val_loss: 0.4496\n",
            "\n",
            "Epoch 00075: val_loss improved from 0.45583 to 0.44964, saving model to folds0.hdf5\n",
            "Epoch 76/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.4746 - val_loss: 0.4678\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.44964\n",
            "Epoch 77/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.4766 - val_loss: 0.4528\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.44964\n",
            "Epoch 78/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.4770 - val_loss: 0.4491\n",
            "\n",
            "Epoch 00078: val_loss improved from 0.44964 to 0.44905, saving model to folds0.hdf5\n",
            "Epoch 79/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.4734 - val_loss: 0.4460\n",
            "\n",
            "Epoch 00079: val_loss improved from 0.44905 to 0.44601, saving model to folds0.hdf5\n",
            "Epoch 80/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.4701 - val_loss: 0.4537\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.44601\n",
            "Epoch 81/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.4699 - val_loss: 0.4466\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.44601\n",
            "Epoch 82/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.4638 - val_loss: 0.4499\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.44601\n",
            "Epoch 83/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.4643 - val_loss: 0.4544\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.44601\n",
            "Epoch 84/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.4672 - val_loss: 0.4455\n",
            "\n",
            "Epoch 00084: val_loss improved from 0.44601 to 0.44551, saving model to folds0.hdf5\n",
            "Epoch 85/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.4640 - val_loss: 0.4511\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.44551\n",
            "Epoch 86/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.4599 - val_loss: 0.4378\n",
            "\n",
            "Epoch 00086: val_loss improved from 0.44551 to 0.43778, saving model to folds0.hdf5\n",
            "Epoch 87/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.4625 - val_loss: 0.4391\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.43778\n",
            "Epoch 88/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.4613 - val_loss: 0.4467\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.43778\n",
            "Epoch 89/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.4634 - val_loss: 0.4343\n",
            "\n",
            "Epoch 00089: val_loss improved from 0.43778 to 0.43432, saving model to folds0.hdf5\n",
            "Epoch 90/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.4567 - val_loss: 0.4329\n",
            "\n",
            "Epoch 00090: val_loss improved from 0.43432 to 0.43291, saving model to folds0.hdf5\n",
            "Epoch 91/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.4542 - val_loss: 0.4405\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.43291\n",
            "Epoch 92/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.4564 - val_loss: 0.4342\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.43291\n",
            "Epoch 93/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.4565 - val_loss: 0.4384\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.43291\n",
            "Epoch 94/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.4536 - val_loss: 0.4317\n",
            "\n",
            "Epoch 00094: val_loss improved from 0.43291 to 0.43174, saving model to folds0.hdf5\n",
            "Epoch 95/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.4577 - val_loss: 0.4472\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.43174\n",
            "Epoch 96/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.4675 - val_loss: 0.4518\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.43174\n",
            "Epoch 97/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.4578 - val_loss: 0.4232\n",
            "\n",
            "Epoch 00097: val_loss improved from 0.43174 to 0.42322, saving model to folds0.hdf5\n",
            "Epoch 98/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.4461 - val_loss: 0.4252\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.42322\n",
            "Epoch 99/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.4513 - val_loss: 0.4348\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.42322\n",
            "Epoch 100/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.4429 - val_loss: 0.4280\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.42322\n",
            "Epoch 101/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.4447 - val_loss: 0.4234\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.42322\n",
            "Epoch 102/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.4437 - val_loss: 0.4597\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.42322\n",
            "Epoch 103/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.4422 - val_loss: 0.4382\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.42322\n",
            "Epoch 104/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.4376 - val_loss: 0.4254\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.42322\n",
            "Epoch 105/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.4390 - val_loss: 0.4639\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.42322\n",
            "Epoch 106/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.4378 - val_loss: 0.4295\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.42322\n",
            "Epoch 107/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.4500 - val_loss: 0.4360\n",
            "\n",
            "Epoch 00107: ReduceLROnPlateau reducing learning rate to 0.0007500000356230885.\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.42322\n",
            "Epoch 108/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.4381 - val_loss: 0.4152\n",
            "\n",
            "Epoch 00108: val_loss improved from 0.42322 to 0.41523, saving model to folds0.hdf5\n",
            "Epoch 109/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.4304 - val_loss: 0.4156\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.41523\n",
            "Epoch 110/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.4296 - val_loss: 0.4099\n",
            "\n",
            "Epoch 00110: val_loss improved from 0.41523 to 0.40991, saving model to folds0.hdf5\n",
            "Epoch 111/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.4283 - val_loss: 0.4089\n",
            "\n",
            "Epoch 00111: val_loss improved from 0.40991 to 0.40890, saving model to folds0.hdf5\n",
            "Epoch 112/300\n",
            "59/59 [==============================] - 15s 262ms/step - loss: 0.4293 - val_loss: 0.4160\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.40890\n",
            "Epoch 113/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.4273 - val_loss: 0.4116\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.40890\n",
            "Epoch 114/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.4351 - val_loss: 0.4072\n",
            "\n",
            "Epoch 00114: val_loss improved from 0.40890 to 0.40718, saving model to folds0.hdf5\n",
            "Epoch 115/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.4246 - val_loss: 0.4107\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.40718\n",
            "Epoch 116/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.4259 - val_loss: 0.4071\n",
            "\n",
            "Epoch 00116: val_loss improved from 0.40718 to 0.40713, saving model to folds0.hdf5\n",
            "Epoch 117/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.4265 - val_loss: 0.4076\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.40713\n",
            "Epoch 118/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.4244 - val_loss: 0.4119\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.40713\n",
            "Epoch 119/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.4219 - val_loss: 0.4142\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.40713\n",
            "Epoch 120/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.4258 - val_loss: 0.4026\n",
            "\n",
            "Epoch 00120: val_loss improved from 0.40713 to 0.40264, saving model to folds0.hdf5\n",
            "Epoch 121/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.4231 - val_loss: 0.4174\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.40264\n",
            "Epoch 122/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.4242 - val_loss: 0.4022\n",
            "\n",
            "Epoch 00122: val_loss improved from 0.40264 to 0.40221, saving model to folds0.hdf5\n",
            "Epoch 123/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.4200 - val_loss: 0.4029\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.40221\n",
            "Epoch 124/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.4184 - val_loss: 0.4110\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.40221\n",
            "Epoch 125/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.4184 - val_loss: 0.4024\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.40221\n",
            "Epoch 126/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.4183 - val_loss: 0.4083\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.40221\n",
            "Epoch 127/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.4271 - val_loss: 0.4284\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.40221\n",
            "Epoch 128/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.4210 - val_loss: 0.4001\n",
            "\n",
            "Epoch 00128: val_loss improved from 0.40221 to 0.40007, saving model to folds0.hdf5\n",
            "Epoch 129/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.4217 - val_loss: 0.4078\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.40007\n",
            "Epoch 130/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.4152 - val_loss: 0.4034\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.40007\n",
            "Epoch 131/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.4189 - val_loss: 0.3980\n",
            "\n",
            "Epoch 00131: val_loss improved from 0.40007 to 0.39802, saving model to folds0.hdf5\n",
            "Epoch 132/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.4152 - val_loss: 0.3950\n",
            "\n",
            "Epoch 00132: val_loss improved from 0.39802 to 0.39496, saving model to folds0.hdf5\n",
            "Epoch 133/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.4154 - val_loss: 0.3982\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.39496\n",
            "Epoch 134/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.4155 - val_loss: 0.3958\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.39496\n",
            "Epoch 135/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.4144 - val_loss: 0.4042\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.39496\n",
            "Epoch 136/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.4159 - val_loss: 0.4134\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.39496\n",
            "Epoch 137/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.4145 - val_loss: 0.4150\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.39496\n",
            "Epoch 138/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.4127 - val_loss: 0.3990\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.39496\n",
            "Epoch 139/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.4133 - val_loss: 0.4097\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.39496\n",
            "Epoch 140/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.4118 - val_loss: 0.3953\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.39496\n",
            "Epoch 141/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.4112 - val_loss: 0.3974\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.39496\n",
            "Epoch 142/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.4148 - val_loss: 0.4022\n",
            "\n",
            "Epoch 00142: ReduceLROnPlateau reducing learning rate to 0.0005625000048894435.\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.39496\n",
            "Epoch 143/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.4048 - val_loss: 0.3896\n",
            "\n",
            "Epoch 00143: val_loss improved from 0.39496 to 0.38956, saving model to folds0.hdf5\n",
            "Epoch 144/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.4023 - val_loss: 0.3879\n",
            "\n",
            "Epoch 00144: val_loss improved from 0.38956 to 0.38793, saving model to folds0.hdf5\n",
            "Epoch 145/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.4041 - val_loss: 0.3888\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.38793\n",
            "Epoch 146/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.4052 - val_loss: 0.3908\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.38793\n",
            "Epoch 147/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.4026 - val_loss: 0.3939\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.38793\n",
            "Epoch 148/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.4030 - val_loss: 0.3867\n",
            "\n",
            "Epoch 00148: val_loss improved from 0.38793 to 0.38674, saving model to folds0.hdf5\n",
            "Epoch 149/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.4033 - val_loss: 0.3848\n",
            "\n",
            "Epoch 00149: val_loss improved from 0.38674 to 0.38483, saving model to folds0.hdf5\n",
            "Epoch 150/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.4035 - val_loss: 0.3899\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.38483\n",
            "Epoch 151/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.4004 - val_loss: 0.3871\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.38483\n",
            "Epoch 152/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.4024 - val_loss: 0.3858\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.38483\n",
            "Epoch 153/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.4004 - val_loss: 0.3936\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.38483\n",
            "Epoch 154/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.4037 - val_loss: 0.3866\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.38483\n",
            "Epoch 155/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.3994 - val_loss: 0.3864\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.38483\n",
            "Epoch 156/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.4038 - val_loss: 0.3984\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.38483\n",
            "Epoch 157/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.4000 - val_loss: 0.3846\n",
            "\n",
            "Epoch 00157: val_loss improved from 0.38483 to 0.38459, saving model to folds0.hdf5\n",
            "Epoch 158/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.4000 - val_loss: 0.3826\n",
            "\n",
            "Epoch 00158: val_loss improved from 0.38459 to 0.38263, saving model to folds0.hdf5\n",
            "Epoch 159/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.4009 - val_loss: 0.3838\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.38263\n",
            "Epoch 160/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.4004 - val_loss: 0.3852\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.38263\n",
            "Epoch 161/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3969 - val_loss: 0.3833\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.38263\n",
            "Epoch 162/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3982 - val_loss: 0.3858\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.38263\n",
            "Epoch 163/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3945 - val_loss: 0.3816\n",
            "\n",
            "Epoch 00163: val_loss improved from 0.38263 to 0.38157, saving model to folds0.hdf5\n",
            "Epoch 164/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3982 - val_loss: 0.3856\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.38157\n",
            "Epoch 165/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3995 - val_loss: 0.3921\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.38157\n",
            "Epoch 166/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.3990 - val_loss: 0.3820\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.38157\n",
            "Epoch 167/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.3964 - val_loss: 0.3866\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.38157\n",
            "Epoch 168/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.3955 - val_loss: 0.3823\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.38157\n",
            "Epoch 169/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3950 - val_loss: 0.3798\n",
            "\n",
            "Epoch 00169: val_loss improved from 0.38157 to 0.37976, saving model to folds0.hdf5\n",
            "Epoch 170/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3954 - val_loss: 0.3815\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.37976\n",
            "Epoch 171/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3932 - val_loss: 0.3807\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.37976\n",
            "Epoch 172/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.3954 - val_loss: 0.3792\n",
            "\n",
            "Epoch 00172: val_loss improved from 0.37976 to 0.37921, saving model to folds0.hdf5\n",
            "Epoch 173/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3942 - val_loss: 0.3817\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.37921\n",
            "Epoch 174/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3966 - val_loss: 0.3851\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.37921\n",
            "Epoch 175/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3919 - val_loss: 0.3810\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.37921\n",
            "Epoch 176/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3925 - val_loss: 0.3779\n",
            "\n",
            "Epoch 00176: val_loss improved from 0.37921 to 0.37790, saving model to folds0.hdf5\n",
            "Epoch 177/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.3909 - val_loss: 0.3919\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.37790\n",
            "Epoch 178/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3931 - val_loss: 0.3776\n",
            "\n",
            "Epoch 00178: val_loss improved from 0.37790 to 0.37759, saving model to folds0.hdf5\n",
            "Epoch 179/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3974 - val_loss: 0.3762\n",
            "\n",
            "Epoch 00179: val_loss improved from 0.37759 to 0.37619, saving model to folds0.hdf5\n",
            "Epoch 180/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3900 - val_loss: 0.3780\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.37619\n",
            "Epoch 181/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3883 - val_loss: 0.3759\n",
            "\n",
            "Epoch 00181: val_loss improved from 0.37619 to 0.37587, saving model to folds0.hdf5\n",
            "Epoch 182/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3889 - val_loss: 0.3767\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.37587\n",
            "Epoch 183/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3885 - val_loss: 0.3737\n",
            "\n",
            "Epoch 00183: val_loss improved from 0.37587 to 0.37374, saving model to folds0.hdf5\n",
            "Epoch 184/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3896 - val_loss: 0.3754\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.37374\n",
            "Epoch 185/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3915 - val_loss: 0.3834\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.37374\n",
            "Epoch 186/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3891 - val_loss: 0.3760\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.37374\n",
            "Epoch 187/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.3892 - val_loss: 0.3769\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.37374\n",
            "Epoch 188/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.3898 - val_loss: 0.3755\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.37374\n",
            "Epoch 189/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3868 - val_loss: 0.3803\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.37374\n",
            "Epoch 190/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.3889 - val_loss: 0.3723\n",
            "\n",
            "Epoch 00190: val_loss improved from 0.37374 to 0.37235, saving model to folds0.hdf5\n",
            "Epoch 191/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.3845 - val_loss: 0.3735\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.37235\n",
            "Epoch 192/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3855 - val_loss: 0.3715\n",
            "\n",
            "Epoch 00192: val_loss improved from 0.37235 to 0.37151, saving model to folds0.hdf5\n",
            "Epoch 193/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3878 - val_loss: 0.3742\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.37151\n",
            "Epoch 194/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3889 - val_loss: 0.3715\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.37151\n",
            "Epoch 195/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3850 - val_loss: 0.3736\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.37151\n",
            "Epoch 196/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3842 - val_loss: 0.3832\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.37151\n",
            "Epoch 197/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3847 - val_loss: 0.3719\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.37151\n",
            "Epoch 198/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3852 - val_loss: 0.3738\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.37151\n",
            "Epoch 199/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3868 - val_loss: 0.3752\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.37151\n",
            "Epoch 200/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3819 - val_loss: 0.3707\n",
            "\n",
            "Epoch 00200: val_loss improved from 0.37151 to 0.37072, saving model to folds0.hdf5\n",
            "Epoch 201/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3829 - val_loss: 0.3818\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.37072\n",
            "Epoch 202/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3828 - val_loss: 0.3688\n",
            "\n",
            "Epoch 00202: val_loss improved from 0.37072 to 0.36881, saving model to folds0.hdf5\n",
            "Epoch 203/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3841 - val_loss: 0.3705\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.36881\n",
            "Epoch 204/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3824 - val_loss: 0.3766\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.36881\n",
            "Epoch 205/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3866 - val_loss: 0.3758\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.36881\n",
            "Epoch 206/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3838 - val_loss: 0.3692\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.36881\n",
            "Epoch 207/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3796 - val_loss: 0.3667\n",
            "\n",
            "Epoch 00207: val_loss improved from 0.36881 to 0.36674, saving model to folds0.hdf5\n",
            "Epoch 208/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3832 - val_loss: 0.3769\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.36674\n",
            "Epoch 209/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3809 - val_loss: 0.3691\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.36674\n",
            "Epoch 210/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3803 - val_loss: 0.3708\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.36674\n",
            "Epoch 211/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3821 - val_loss: 0.3695\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.36674\n",
            "Epoch 212/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3808 - val_loss: 0.3711\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.36674\n",
            "Epoch 213/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3815 - val_loss: 0.3685\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.36674\n",
            "Epoch 214/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3791 - val_loss: 0.3646\n",
            "\n",
            "Epoch 00214: val_loss improved from 0.36674 to 0.36460, saving model to folds0.hdf5\n",
            "Epoch 215/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3819 - val_loss: 0.3689\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.36460\n",
            "Epoch 216/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3792 - val_loss: 0.3670\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.36460\n",
            "Epoch 217/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3782 - val_loss: 0.3653\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.36460\n",
            "Epoch 218/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.3799 - val_loss: 0.3713\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.36460\n",
            "Epoch 219/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3782 - val_loss: 0.3641\n",
            "\n",
            "Epoch 00219: val_loss improved from 0.36460 to 0.36411, saving model to folds0.hdf5\n",
            "Epoch 220/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.3784 - val_loss: 0.3751\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.36411\n",
            "Epoch 221/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3769 - val_loss: 0.3699\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.36411\n",
            "Epoch 222/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.3777 - val_loss: 0.3666\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.36411\n",
            "Epoch 223/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3773 - val_loss: 0.3654\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.36411\n",
            "Epoch 224/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3772 - val_loss: 0.3635\n",
            "\n",
            "Epoch 00224: val_loss improved from 0.36411 to 0.36353, saving model to folds0.hdf5\n",
            "Epoch 225/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.3761 - val_loss: 0.3732\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.36353\n",
            "Epoch 226/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3741 - val_loss: 0.3649\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.36353\n",
            "Epoch 227/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3750 - val_loss: 0.3663\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.36353\n",
            "Epoch 228/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3764 - val_loss: 0.3649\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.36353\n",
            "Epoch 229/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3787 - val_loss: 0.3629\n",
            "\n",
            "Epoch 00229: val_loss improved from 0.36353 to 0.36286, saving model to folds0.hdf5\n",
            "Epoch 230/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3750 - val_loss: 0.3702\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.36286\n",
            "Epoch 231/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.3785 - val_loss: 0.3637\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.36286\n",
            "Epoch 232/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.3761 - val_loss: 0.3645\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.36286\n",
            "Epoch 233/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3761 - val_loss: 0.3623\n",
            "\n",
            "Epoch 00233: val_loss improved from 0.36286 to 0.36232, saving model to folds0.hdf5\n",
            "Epoch 234/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3712 - val_loss: 0.3644\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.36232\n",
            "Epoch 235/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3717 - val_loss: 0.3625\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.36232\n",
            "Epoch 236/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.3778 - val_loss: 0.3647\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.36232\n",
            "Epoch 237/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.3722 - val_loss: 0.3596\n",
            "\n",
            "Epoch 00237: val_loss improved from 0.36232 to 0.35956, saving model to folds0.hdf5\n",
            "Epoch 238/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3757 - val_loss: 0.3691\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.35956\n",
            "Epoch 239/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3761 - val_loss: 0.3739\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.35956\n",
            "Epoch 240/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3704 - val_loss: 0.3652\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.35956\n",
            "Epoch 241/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3712 - val_loss: 0.3625\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.35956\n",
            "Epoch 242/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.3712 - val_loss: 0.3621\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.35956\n",
            "Epoch 243/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3722 - val_loss: 0.3629\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.35956\n",
            "Epoch 244/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.3708 - val_loss: 0.3600\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.35956\n",
            "Epoch 245/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.3689 - val_loss: 0.3596\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.35956\n",
            "Epoch 246/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.3717 - val_loss: 0.3665\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.35956\n",
            "Epoch 247/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3689 - val_loss: 0.3612\n",
            "\n",
            "Epoch 00247: ReduceLROnPlateau reducing learning rate to 0.0004218749818392098.\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.35956\n",
            "Epoch 248/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.3658 - val_loss: 0.3551\n",
            "\n",
            "Epoch 00248: val_loss improved from 0.35956 to 0.35514, saving model to folds0.hdf5\n",
            "Epoch 249/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.3667 - val_loss: 0.3559\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.35514\n",
            "Epoch 250/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3648 - val_loss: 0.3545\n",
            "\n",
            "Epoch 00250: val_loss improved from 0.35514 to 0.35453, saving model to folds0.hdf5\n",
            "Epoch 251/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.3646 - val_loss: 0.3546\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.35453\n",
            "Epoch 252/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.3654 - val_loss: 0.3564\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.35453\n",
            "Epoch 253/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3648 - val_loss: 0.3640\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.35453\n",
            "Epoch 254/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3665 - val_loss: 0.3540\n",
            "\n",
            "Epoch 00254: val_loss improved from 0.35453 to 0.35404, saving model to folds0.hdf5\n",
            "Epoch 255/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3658 - val_loss: 0.3548\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.35404\n",
            "Epoch 256/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3646 - val_loss: 0.3612\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.35404\n",
            "Epoch 257/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3673 - val_loss: 0.3594\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.35404\n",
            "Epoch 258/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3636 - val_loss: 0.3548\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.35404\n",
            "Epoch 259/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.3649 - val_loss: 0.3589\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.35404\n",
            "Epoch 260/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.3632 - val_loss: 0.3603\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.35404\n",
            "Epoch 261/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.3636 - val_loss: 0.3528\n",
            "\n",
            "Epoch 00261: val_loss improved from 0.35404 to 0.35285, saving model to folds0.hdf5\n",
            "Epoch 262/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.3634 - val_loss: 0.3633\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.35285\n",
            "Epoch 263/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.3625 - val_loss: 0.3518\n",
            "\n",
            "Epoch 00263: val_loss improved from 0.35285 to 0.35178, saving model to folds0.hdf5\n",
            "Epoch 264/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.3644 - val_loss: 0.3571\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.35178\n",
            "Epoch 265/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.3631 - val_loss: 0.3532\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.35178\n",
            "Epoch 266/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3625 - val_loss: 0.3552\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.35178\n",
            "Epoch 267/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3629 - val_loss: 0.3568\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.35178\n",
            "Epoch 268/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3631 - val_loss: 0.3562\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.35178\n",
            "Epoch 269/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3633 - val_loss: 0.3550\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.35178\n",
            "Epoch 270/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3608 - val_loss: 0.3536\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.35178\n",
            "Epoch 271/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3619 - val_loss: 0.3517\n",
            "\n",
            "Epoch 00271: val_loss improved from 0.35178 to 0.35170, saving model to folds0.hdf5\n",
            "Epoch 272/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.3636 - val_loss: 0.3737\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.35170\n",
            "Epoch 273/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3638 - val_loss: 0.3527\n",
            "\n",
            "Epoch 00273: ReduceLROnPlateau reducing learning rate to 0.00031640623637940735.\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.35170\n",
            "Epoch 274/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.3593 - val_loss: 0.3529\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.35170\n",
            "Epoch 275/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3576 - val_loss: 0.3493\n",
            "\n",
            "Epoch 00275: val_loss improved from 0.35170 to 0.34931, saving model to folds0.hdf5\n",
            "Epoch 276/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3588 - val_loss: 0.3551\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.34931\n",
            "Epoch 277/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3591 - val_loss: 0.3493\n",
            "\n",
            "Epoch 00277: val_loss improved from 0.34931 to 0.34926, saving model to folds0.hdf5\n",
            "Epoch 278/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3577 - val_loss: 0.3496\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.34926\n",
            "Epoch 279/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3574 - val_loss: 0.3486\n",
            "\n",
            "Epoch 00279: val_loss improved from 0.34926 to 0.34856, saving model to folds0.hdf5\n",
            "Epoch 280/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3583 - val_loss: 0.3521\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.34856\n",
            "Epoch 281/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3576 - val_loss: 0.3503\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.34856\n",
            "Epoch 282/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3582 - val_loss: 0.3485\n",
            "\n",
            "Epoch 00282: val_loss improved from 0.34856 to 0.34845, saving model to folds0.hdf5\n",
            "Epoch 283/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3582 - val_loss: 0.3494\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.34845\n",
            "Epoch 284/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3569 - val_loss: 0.3479\n",
            "\n",
            "Epoch 00284: val_loss improved from 0.34845 to 0.34793, saving model to folds0.hdf5\n",
            "Epoch 285/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3565 - val_loss: 0.3484\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.34793\n",
            "Epoch 286/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.3556 - val_loss: 0.3466\n",
            "\n",
            "Epoch 00286: val_loss improved from 0.34793 to 0.34662, saving model to folds0.hdf5\n",
            "Epoch 287/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3563 - val_loss: 0.3484\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.34662\n",
            "Epoch 288/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.3565 - val_loss: 0.3507\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.34662\n",
            "Epoch 289/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3581 - val_loss: 0.3591\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.34662\n",
            "Epoch 290/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3574 - val_loss: 0.3494\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.34662\n",
            "Epoch 291/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3563 - val_loss: 0.3534\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.34662\n",
            "Epoch 292/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3562 - val_loss: 0.3462\n",
            "\n",
            "Epoch 00292: val_loss improved from 0.34662 to 0.34622, saving model to folds0.hdf5\n",
            "Epoch 293/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3568 - val_loss: 0.3489\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.34622\n",
            "Epoch 294/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3550 - val_loss: 0.3473\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.34622\n",
            "Epoch 295/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3550 - val_loss: 0.3512\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.34622\n",
            "Epoch 296/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3565 - val_loss: 0.3483\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.34622\n",
            "Epoch 297/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3554 - val_loss: 0.3590\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.34622\n",
            "Epoch 298/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3572 - val_loss: 0.3499\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.34622\n",
            "Epoch 299/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3540 - val_loss: 0.3481\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.34622\n",
            "Epoch 300/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.3538 - val_loss: 0.3480\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.34622\n",
            "--------------- > Fold 2 < ---------------\n",
            "positional_encoding: tf.Tensor(\n",
            "[[[ 0.0000000e+00  1.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
            "    1.0000000e+00  0.0000000e+00]\n",
            "  [ 8.4147096e-01  5.4030228e-01  6.6822791e-01 ...  1.5973122e-04\n",
            "    1.0000000e+00  1.1689518e-04]\n",
            "  [ 9.0929741e-01 -4.1614684e-01  9.9426514e-01 ...  3.1946244e-04\n",
            "    9.9999994e-01  2.3379036e-04]\n",
            "  ...\n",
            "  [ 9.9952018e-01 -3.0975033e-02 -1.9690752e-01 ...  1.2298995e-02\n",
            "    9.9992436e-01  9.0008071e-03]\n",
            "  [ 5.1397848e-01 -8.5780311e-01  5.0865471e-01 ...  1.2458714e-02\n",
            "    9.9992239e-01  9.1176983e-03]\n",
            "  [-4.4411266e-01 -8.9597094e-01  9.5374161e-01 ...  1.2618432e-02\n",
            "    9.9992037e-01  9.2345877e-03]]], shape=(1, 80, 59), dtype=float32)\n",
            "Model: \"model_9\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_10 (InputLayer)           [(None, 80, 59)]     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.multiply_9 (TFOpLambda) (None, 80, 59)       0           input_10[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_138 (TFOpL (None, 80, 59)       0           tf.math.multiply_9[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_120 (LayerN (None, 80, 59)       118         tf.__operators__.add_138[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_60 (MultiH (None, 80, 59)       56463       layer_normalization_120[0][0]    \n",
            "                                                                 layer_normalization_120[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_138 (Dropout)           (None, 80, 59)       0           multi_head_attention_60[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_139 (TFOpL (None, 80, 59)       0           dropout_138[0][0]                \n",
            "                                                                 tf.__operators__.add_138[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_121 (LayerN (None, 80, 59)       118         tf.__operators__.add_139[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_120 (Conv1D)             (None, 80, 512)      30720       layer_normalization_121[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_139 (Dropout)           (None, 80, 512)      0           conv1d_120[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_121 (Conv1D)             (None, 80, 59)       30267       dropout_139[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_140 (TFOpL (None, 80, 59)       0           conv1d_121[0][0]                 \n",
            "                                                                 tf.__operators__.add_139[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_122 (LayerN (None, 80, 59)       118         tf.__operators__.add_140[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_61 (MultiH (None, 80, 59)       56463       layer_normalization_122[0][0]    \n",
            "                                                                 layer_normalization_122[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_140 (Dropout)           (None, 80, 59)       0           multi_head_attention_61[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_141 (TFOpL (None, 80, 59)       0           dropout_140[0][0]                \n",
            "                                                                 tf.__operators__.add_140[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_123 (LayerN (None, 80, 59)       118         tf.__operators__.add_141[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_122 (Conv1D)             (None, 80, 512)      30720       layer_normalization_123[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_141 (Dropout)           (None, 80, 512)      0           conv1d_122[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_123 (Conv1D)             (None, 80, 59)       30267       dropout_141[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_142 (TFOpL (None, 80, 59)       0           conv1d_123[0][0]                 \n",
            "                                                                 tf.__operators__.add_141[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_124 (LayerN (None, 80, 59)       118         tf.__operators__.add_142[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_62 (MultiH (None, 80, 59)       56463       layer_normalization_124[0][0]    \n",
            "                                                                 layer_normalization_124[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_142 (Dropout)           (None, 80, 59)       0           multi_head_attention_62[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_143 (TFOpL (None, 80, 59)       0           dropout_142[0][0]                \n",
            "                                                                 tf.__operators__.add_142[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_125 (LayerN (None, 80, 59)       118         tf.__operators__.add_143[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_124 (Conv1D)             (None, 80, 512)      30720       layer_normalization_125[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_143 (Dropout)           (None, 80, 512)      0           conv1d_124[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_125 (Conv1D)             (None, 80, 59)       30267       dropout_143[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_144 (TFOpL (None, 80, 59)       0           conv1d_125[0][0]                 \n",
            "                                                                 tf.__operators__.add_143[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_126 (LayerN (None, 80, 59)       118         tf.__operators__.add_144[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "multi_head_attention_63 (MultiH (None, 80, 59)       56463       layer_normalization_126[0][0]    \n",
            "                                                                 layer_normalization_126[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_144 (Dropout)           (None, 80, 59)       0           multi_head_attention_63[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_145 (TFOpL (None, 80, 59)       0           dropout_144[0][0]                \n",
            "                                                                 tf.__operators__.add_144[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_127 (LayerN (None, 80, 59)       118         tf.__operators__.add_145[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_126 (Conv1D)             (None, 80, 512)      30720       layer_normalization_127[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_145 (Dropout)           (None, 80, 512)      0           conv1d_126[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_127 (Conv1D)             (None, 80, 59)       30267       dropout_145[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_146 (TFOpL (None, 80, 59)       0           conv1d_127[0][0]                 \n",
            "                                                                 tf.__operators__.add_145[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "dense_27 (Dense)                (None, 80, 128)      7680        tf.__operators__.add_146[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_146 (Dropout)           (None, 80, 128)      0           dense_27[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_28 (Dense)                (None, 80, 1)        129         dropout_146[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_9 (Glo (None, 80)           0           tf.__operators__.add_146[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_147 (Dropout)           (None, 80, 1)        0           dense_28[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "tf.expand_dims_9 (TFOpLambda)   (None, 80, 1)        0           global_average_pooling1d_9[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_147 (TFOpL (None, 80, 1)        0           dropout_147[0][0]                \n",
            "                                                                 tf.expand_dims_9[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dense_29 (Dense)                (None, 80, 1)        2           tf.__operators__.add_147[0][0]   \n",
            "==================================================================================================\n",
            "Total params: 478,555\n",
            "Trainable params: 478,555\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/300\n",
            "59/59 [==============================] - 21s 279ms/step - loss: 4.3257 - val_loss: 2.7830\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 2.78303, saving model to folds1.hdf5\n",
            "Epoch 2/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 2.6002 - val_loss: 1.9899\n",
            "\n",
            "Epoch 00002: val_loss improved from 2.78303 to 1.98993, saving model to folds1.hdf5\n",
            "Epoch 3/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 1.9995 - val_loss: 1.5625\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.98993 to 1.56246, saving model to folds1.hdf5\n",
            "Epoch 4/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 1.6034 - val_loss: 1.2923\n",
            "\n",
            "Epoch 00004: val_loss improved from 1.56246 to 1.29228, saving model to folds1.hdf5\n",
            "Epoch 5/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 1.3613 - val_loss: 1.0572\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.29228 to 1.05723, saving model to folds1.hdf5\n",
            "Epoch 6/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 1.2386 - val_loss: 0.9539\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.05723 to 0.95391, saving model to folds1.hdf5\n",
            "Epoch 7/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 1.1177 - val_loss: 0.8590\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.95391 to 0.85902, saving model to folds1.hdf5\n",
            "Epoch 8/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 1.0583 - val_loss: 0.8591\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.85902\n",
            "Epoch 9/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.9931 - val_loss: 0.7691\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.85902 to 0.76914, saving model to folds1.hdf5\n",
            "Epoch 10/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.9522 - val_loss: 0.7297\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.76914 to 0.72975, saving model to folds1.hdf5\n",
            "Epoch 11/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.9098 - val_loss: 0.7019\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.72975 to 0.70187, saving model to folds1.hdf5\n",
            "Epoch 12/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.8656 - val_loss: 0.7172\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.70187\n",
            "Epoch 13/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.8442 - val_loss: 0.6810\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.70187 to 0.68104, saving model to folds1.hdf5\n",
            "Epoch 14/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.8212 - val_loss: 0.6931\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.68104\n",
            "Epoch 15/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.8108 - val_loss: 0.6191\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.68104 to 0.61906, saving model to folds1.hdf5\n",
            "Epoch 16/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.7856 - val_loss: 0.6177\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.61906 to 0.61767, saving model to folds1.hdf5\n",
            "Epoch 17/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.7560 - val_loss: 0.6129\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.61767 to 0.61288, saving model to folds1.hdf5\n",
            "Epoch 18/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.7453 - val_loss: 0.6140\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.61288\n",
            "Epoch 19/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.7264 - val_loss: 0.6299\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.61288\n",
            "Epoch 20/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.7213 - val_loss: 0.6031\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.61288 to 0.60306, saving model to folds1.hdf5\n",
            "Epoch 21/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.6987 - val_loss: 0.6123\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.60306\n",
            "Epoch 22/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.6866 - val_loss: 0.5578\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.60306 to 0.55777, saving model to folds1.hdf5\n",
            "Epoch 23/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.6728 - val_loss: 0.5794\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.55777\n",
            "Epoch 24/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.6673 - val_loss: 0.5612\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.55777\n",
            "Epoch 25/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.6549 - val_loss: 0.5433\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.55777 to 0.54327, saving model to folds1.hdf5\n",
            "Epoch 26/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.6417 - val_loss: 0.5361\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.54327 to 0.53613, saving model to folds1.hdf5\n",
            "Epoch 27/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.6410 - val_loss: 0.5297\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.53613 to 0.52969, saving model to folds1.hdf5\n",
            "Epoch 28/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.6309 - val_loss: 0.5814\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.52969\n",
            "Epoch 29/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.6200 - val_loss: 0.5248\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.52969 to 0.52476, saving model to folds1.hdf5\n",
            "Epoch 30/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.6001 - val_loss: 0.5204\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.52476 to 0.52037, saving model to folds1.hdf5\n",
            "Epoch 31/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.6076 - val_loss: 0.5170\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.52037 to 0.51697, saving model to folds1.hdf5\n",
            "Epoch 32/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.6009 - val_loss: 0.5023\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.51697 to 0.50234, saving model to folds1.hdf5\n",
            "Epoch 33/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.5866 - val_loss: 0.5382\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.50234\n",
            "Epoch 34/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.5798 - val_loss: 0.4910\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.50234 to 0.49103, saving model to folds1.hdf5\n",
            "Epoch 35/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.5681 - val_loss: 0.4950\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.49103\n",
            "Epoch 36/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.5672 - val_loss: 0.5193\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.49103\n",
            "Epoch 37/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.5605 - val_loss: 0.4951\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.49103\n",
            "Epoch 38/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.5537 - val_loss: 0.4824\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.49103 to 0.48238, saving model to folds1.hdf5\n",
            "Epoch 39/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.5476 - val_loss: 0.4869\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.48238\n",
            "Epoch 40/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.5414 - val_loss: 0.4927\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.48238\n",
            "Epoch 41/300\n",
            "59/59 [==============================] - 15s 263ms/step - loss: 0.5386 - val_loss: 0.5024\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.48238\n",
            "Epoch 42/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.5451 - val_loss: 0.4715\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.48238 to 0.47151, saving model to folds1.hdf5\n",
            "Epoch 43/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.5348 - val_loss: 0.6373\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.47151\n",
            "Epoch 44/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.5311 - val_loss: 0.4719\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.47151\n",
            "Epoch 45/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.5121 - val_loss: 0.4634\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.47151 to 0.46343, saving model to folds1.hdf5\n",
            "Epoch 46/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.5125 - val_loss: 0.4679\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.46343\n",
            "Epoch 47/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.5135 - val_loss: 0.4555\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.46343 to 0.45546, saving model to folds1.hdf5\n",
            "Epoch 48/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.5111 - val_loss: 0.4606\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.45546\n",
            "Epoch 49/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.5046 - val_loss: 0.4540\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.45546 to 0.45405, saving model to folds1.hdf5\n",
            "Epoch 50/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.5035 - val_loss: 0.4815\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.45405\n",
            "Epoch 51/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.4923 - val_loss: 0.4503\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.45405 to 0.45030, saving model to folds1.hdf5\n",
            "Epoch 52/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.4990 - val_loss: 0.4669\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.45030\n",
            "Epoch 53/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.4958 - val_loss: 0.4608\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.45030\n",
            "Epoch 54/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.4846 - val_loss: 0.4514\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.45030\n",
            "Epoch 55/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.4853 - val_loss: 0.4443\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.45030 to 0.44434, saving model to folds1.hdf5\n",
            "Epoch 56/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.4819 - val_loss: 0.4427\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.44434 to 0.44273, saving model to folds1.hdf5\n",
            "Epoch 57/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.4768 - val_loss: 0.4367\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.44273 to 0.43670, saving model to folds1.hdf5\n",
            "Epoch 58/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.4757 - val_loss: 0.4377\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.43670\n",
            "Epoch 59/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.4682 - val_loss: 0.4300\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.43670 to 0.43001, saving model to folds1.hdf5\n",
            "Epoch 60/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.4687 - val_loss: 0.4589\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.43001\n",
            "Epoch 61/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.4744 - val_loss: 0.4328\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.43001\n",
            "Epoch 62/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.4754 - val_loss: 0.4278\n",
            "\n",
            "Epoch 00062: val_loss improved from 0.43001 to 0.42781, saving model to folds1.hdf5\n",
            "Epoch 63/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.4638 - val_loss: 0.4480\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.42781\n",
            "Epoch 64/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.4658 - val_loss: 0.4404\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.42781\n",
            "Epoch 65/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.4695 - val_loss: 0.4286\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.42781\n",
            "Epoch 66/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.4545 - val_loss: 0.4322\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.42781\n",
            "Epoch 67/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.4523 - val_loss: 0.4344\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.42781\n",
            "Epoch 68/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.4536 - val_loss: 0.4248\n",
            "\n",
            "Epoch 00068: val_loss improved from 0.42781 to 0.42481, saving model to folds1.hdf5\n",
            "Epoch 69/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.4523 - val_loss: 0.4268\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.42481\n",
            "Epoch 70/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.4623 - val_loss: 0.4379\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.42481\n",
            "Epoch 71/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.4628 - val_loss: 0.4231\n",
            "\n",
            "Epoch 00071: val_loss improved from 0.42481 to 0.42313, saving model to folds1.hdf5\n",
            "Epoch 72/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.4404 - val_loss: 0.4113\n",
            "\n",
            "Epoch 00072: val_loss improved from 0.42313 to 0.41135, saving model to folds1.hdf5\n",
            "Epoch 73/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.4474 - val_loss: 0.4108\n",
            "\n",
            "Epoch 00073: val_loss improved from 0.41135 to 0.41085, saving model to folds1.hdf5\n",
            "Epoch 74/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.4404 - val_loss: 0.4060\n",
            "\n",
            "Epoch 00074: val_loss improved from 0.41085 to 0.40600, saving model to folds1.hdf5\n",
            "Epoch 75/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.4368 - val_loss: 0.4073\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.40600\n",
            "Epoch 76/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.4374 - val_loss: 0.4124\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.40600\n",
            "Epoch 77/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.4400 - val_loss: 0.4392\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.40600\n",
            "Epoch 78/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.4362 - val_loss: 0.4125\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.40600\n",
            "Epoch 79/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.4355 - val_loss: 0.4060\n",
            "\n",
            "Epoch 00079: val_loss improved from 0.40600 to 0.40596, saving model to folds1.hdf5\n",
            "Epoch 80/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.4336 - val_loss: 0.4047\n",
            "\n",
            "Epoch 00080: val_loss improved from 0.40596 to 0.40467, saving model to folds1.hdf5\n",
            "Epoch 81/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.4299 - val_loss: 0.4193\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.40467\n",
            "Epoch 82/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.4368 - val_loss: 0.4044\n",
            "\n",
            "Epoch 00082: val_loss improved from 0.40467 to 0.40436, saving model to folds1.hdf5\n",
            "Epoch 83/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.4330 - val_loss: 0.4134\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.40436\n",
            "Epoch 84/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.4237 - val_loss: 0.4134\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.40436\n",
            "Epoch 85/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.4227 - val_loss: 0.3954\n",
            "\n",
            "Epoch 00085: val_loss improved from 0.40436 to 0.39544, saving model to folds1.hdf5\n",
            "Epoch 86/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.4239 - val_loss: 0.4018\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.39544\n",
            "Epoch 87/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.4182 - val_loss: 0.3947\n",
            "\n",
            "Epoch 00087: val_loss improved from 0.39544 to 0.39474, saving model to folds1.hdf5\n",
            "Epoch 88/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.4291 - val_loss: 0.4080\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.39474\n",
            "Epoch 89/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.4212 - val_loss: 0.4158\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.39474\n",
            "Epoch 90/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.4249 - val_loss: 0.4025\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.39474\n",
            "Epoch 91/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.4219 - val_loss: 0.4041\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.39474\n",
            "Epoch 92/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.4187 - val_loss: 0.3957\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.39474\n",
            "Epoch 93/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.4140 - val_loss: 0.3888\n",
            "\n",
            "Epoch 00093: val_loss improved from 0.39474 to 0.38884, saving model to folds1.hdf5\n",
            "Epoch 94/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.4100 - val_loss: 0.3927\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.38884\n",
            "Epoch 95/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.4127 - val_loss: 0.4085\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.38884\n",
            "Epoch 96/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.4163 - val_loss: 0.3940\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.38884\n",
            "Epoch 97/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.4115 - val_loss: 0.3887\n",
            "\n",
            "Epoch 00097: val_loss improved from 0.38884 to 0.38874, saving model to folds1.hdf5\n",
            "Epoch 98/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.4083 - val_loss: 0.3865\n",
            "\n",
            "Epoch 00098: val_loss improved from 0.38874 to 0.38650, saving model to folds1.hdf5\n",
            "Epoch 99/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.4093 - val_loss: 0.4084\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.38650\n",
            "Epoch 100/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.4115 - val_loss: 0.3860\n",
            "\n",
            "Epoch 00100: val_loss improved from 0.38650 to 0.38597, saving model to folds1.hdf5\n",
            "Epoch 101/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.4064 - val_loss: 0.4002\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.38597\n",
            "Epoch 102/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.4037 - val_loss: 0.3897\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.38597\n",
            "Epoch 103/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.4057 - val_loss: 0.3907\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.38597\n",
            "Epoch 104/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.4014 - val_loss: 0.3900\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.38597\n",
            "Epoch 105/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.4055 - val_loss: 0.3953\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.38597\n",
            "Epoch 106/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.4030 - val_loss: 0.3837\n",
            "\n",
            "Epoch 00106: val_loss improved from 0.38597 to 0.38374, saving model to folds1.hdf5\n",
            "Epoch 107/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.4002 - val_loss: 0.3833\n",
            "\n",
            "Epoch 00107: val_loss improved from 0.38374 to 0.38331, saving model to folds1.hdf5\n",
            "Epoch 108/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.4063 - val_loss: 0.3979\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.38331\n",
            "Epoch 109/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3968 - val_loss: 0.3759\n",
            "\n",
            "Epoch 00109: val_loss improved from 0.38331 to 0.37588, saving model to folds1.hdf5\n",
            "Epoch 110/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3990 - val_loss: 0.3982\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.37588\n",
            "Epoch 111/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3992 - val_loss: 0.3900\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.37588\n",
            "Epoch 112/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3946 - val_loss: 0.3791\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.37588\n",
            "Epoch 113/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.4004 - val_loss: 0.3740\n",
            "\n",
            "Epoch 00113: val_loss improved from 0.37588 to 0.37396, saving model to folds1.hdf5\n",
            "Epoch 114/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3985 - val_loss: 0.3976\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.37396\n",
            "Epoch 115/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.4005 - val_loss: 0.4125\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.37396\n",
            "Epoch 116/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3933 - val_loss: 0.3734\n",
            "\n",
            "Epoch 00116: val_loss improved from 0.37396 to 0.37345, saving model to folds1.hdf5\n",
            "Epoch 117/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3915 - val_loss: 0.3724\n",
            "\n",
            "Epoch 00117: val_loss improved from 0.37345 to 0.37238, saving model to folds1.hdf5\n",
            "Epoch 118/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3900 - val_loss: 0.3737\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.37238\n",
            "Epoch 119/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3887 - val_loss: 0.3712\n",
            "\n",
            "Epoch 00119: val_loss improved from 0.37238 to 0.37115, saving model to folds1.hdf5\n",
            "Epoch 120/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3918 - val_loss: 0.3737\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.37115\n",
            "Epoch 121/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3883 - val_loss: 0.3706\n",
            "\n",
            "Epoch 00121: val_loss improved from 0.37115 to 0.37063, saving model to folds1.hdf5\n",
            "Epoch 122/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3865 - val_loss: 0.3728\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.37063\n",
            "Epoch 123/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3871 - val_loss: 0.3817\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.37063\n",
            "Epoch 124/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3854 - val_loss: 0.3787\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.37063\n",
            "Epoch 125/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3842 - val_loss: 0.3677\n",
            "\n",
            "Epoch 00125: val_loss improved from 0.37063 to 0.36766, saving model to folds1.hdf5\n",
            "Epoch 126/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3858 - val_loss: 0.3661\n",
            "\n",
            "Epoch 00126: val_loss improved from 0.36766 to 0.36612, saving model to folds1.hdf5\n",
            "Epoch 127/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3874 - val_loss: 0.3720\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.36612\n",
            "Epoch 128/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3837 - val_loss: 0.3831\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.36612\n",
            "Epoch 129/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3810 - val_loss: 0.3698\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.36612\n",
            "Epoch 130/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3865 - val_loss: 0.3658\n",
            "\n",
            "Epoch 00130: val_loss improved from 0.36612 to 0.36576, saving model to folds1.hdf5\n",
            "Epoch 131/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3801 - val_loss: 0.3788\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.36576\n",
            "Epoch 132/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3800 - val_loss: 0.3641\n",
            "\n",
            "Epoch 00132: val_loss improved from 0.36576 to 0.36412, saving model to folds1.hdf5\n",
            "Epoch 133/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3835 - val_loss: 0.3719\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.36412\n",
            "Epoch 134/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3793 - val_loss: 0.3637\n",
            "\n",
            "Epoch 00134: val_loss improved from 0.36412 to 0.36372, saving model to folds1.hdf5\n",
            "Epoch 135/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3813 - val_loss: 0.3616\n",
            "\n",
            "Epoch 00135: val_loss improved from 0.36372 to 0.36156, saving model to folds1.hdf5\n",
            "Epoch 136/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3842 - val_loss: 0.3814\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.36156\n",
            "Epoch 137/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3795 - val_loss: 0.3665\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.36156\n",
            "Epoch 138/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3755 - val_loss: 0.3647\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.36156\n",
            "Epoch 139/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3764 - val_loss: 0.3727\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.36156\n",
            "Epoch 140/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3775 - val_loss: 0.3623\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.36156\n",
            "Epoch 141/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3751 - val_loss: 0.3690\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.36156\n",
            "Epoch 142/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3759 - val_loss: 0.3791\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.36156\n",
            "Epoch 143/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3770 - val_loss: 0.3681\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.36156\n",
            "Epoch 144/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3783 - val_loss: 0.3674\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.36156\n",
            "Epoch 145/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3712 - val_loss: 0.3792\n",
            "\n",
            "Epoch 00145: ReduceLROnPlateau reducing learning rate to 0.0007500000356230885.\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.36156\n",
            "Epoch 146/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3676 - val_loss: 0.3546\n",
            "\n",
            "Epoch 00146: val_loss improved from 0.36156 to 0.35457, saving model to folds1.hdf5\n",
            "Epoch 147/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3669 - val_loss: 0.3625\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.35457\n",
            "Epoch 148/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3652 - val_loss: 0.3528\n",
            "\n",
            "Epoch 00148: val_loss improved from 0.35457 to 0.35280, saving model to folds1.hdf5\n",
            "Epoch 149/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3627 - val_loss: 0.3535\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.35280\n",
            "Epoch 150/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3624 - val_loss: 0.3536\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.35280\n",
            "Epoch 151/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3600 - val_loss: 0.3537\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.35280\n",
            "Epoch 152/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3617 - val_loss: 0.3510\n",
            "\n",
            "Epoch 00152: val_loss improved from 0.35280 to 0.35098, saving model to folds1.hdf5\n",
            "Epoch 153/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3611 - val_loss: 0.3542\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.35098\n",
            "Epoch 154/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3634 - val_loss: 0.3496\n",
            "\n",
            "Epoch 00154: val_loss improved from 0.35098 to 0.34965, saving model to folds1.hdf5\n",
            "Epoch 155/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3630 - val_loss: 0.3528\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.34965\n",
            "Epoch 156/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3620 - val_loss: 0.3579\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.34965\n",
            "Epoch 157/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3598 - val_loss: 0.3502\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.34965\n",
            "Epoch 158/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3631 - val_loss: 0.3611\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.34965\n",
            "Epoch 159/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3648 - val_loss: 0.3594\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.34965\n",
            "Epoch 160/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3587 - val_loss: 0.3504\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.34965\n",
            "Epoch 161/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3627 - val_loss: 0.3561\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.34965\n",
            "Epoch 162/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3608 - val_loss: 0.3534\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.34965\n",
            "Epoch 163/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3561 - val_loss: 0.3520\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.34965\n",
            "Epoch 164/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3593 - val_loss: 0.3512\n",
            "\n",
            "Epoch 00164: ReduceLROnPlateau reducing learning rate to 0.0005625000048894435.\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.34965\n",
            "Epoch 165/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3522 - val_loss: 0.3441\n",
            "\n",
            "Epoch 00165: val_loss improved from 0.34965 to 0.34409, saving model to folds1.hdf5\n",
            "Epoch 166/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3519 - val_loss: 0.3505\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.34409\n",
            "Epoch 167/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3509 - val_loss: 0.3424\n",
            "\n",
            "Epoch 00167: val_loss improved from 0.34409 to 0.34238, saving model to folds1.hdf5\n",
            "Epoch 168/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3545 - val_loss: 0.3514\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.34238\n",
            "Epoch 169/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3514 - val_loss: 0.3417\n",
            "\n",
            "Epoch 00169: val_loss improved from 0.34238 to 0.34172, saving model to folds1.hdf5\n",
            "Epoch 170/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3525 - val_loss: 0.3426\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.34172\n",
            "Epoch 171/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3496 - val_loss: 0.3439\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.34172\n",
            "Epoch 172/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3502 - val_loss: 0.3430\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.34172\n",
            "Epoch 173/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3489 - val_loss: 0.3417\n",
            "\n",
            "Epoch 00173: val_loss improved from 0.34172 to 0.34168, saving model to folds1.hdf5\n",
            "Epoch 174/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3487 - val_loss: 0.3401\n",
            "\n",
            "Epoch 00174: val_loss improved from 0.34168 to 0.34015, saving model to folds1.hdf5\n",
            "Epoch 175/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3481 - val_loss: 0.3458\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.34015\n",
            "Epoch 176/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3506 - val_loss: 0.3459\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.34015\n",
            "Epoch 177/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3503 - val_loss: 0.3471\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.34015\n",
            "Epoch 178/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3513 - val_loss: 0.3440\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.34015\n",
            "Epoch 179/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3502 - val_loss: 0.3412\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.34015\n",
            "Epoch 180/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3512 - val_loss: 0.3391\n",
            "\n",
            "Epoch 00180: val_loss improved from 0.34015 to 0.33914, saving model to folds1.hdf5\n",
            "Epoch 181/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3504 - val_loss: 0.3457\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.33914\n",
            "Epoch 182/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3479 - val_loss: 0.3428\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.33914\n",
            "Epoch 183/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3506 - val_loss: 0.3406\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.33914\n",
            "Epoch 184/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3490 - val_loss: 0.3547\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.33914\n",
            "Epoch 185/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3480 - val_loss: 0.3390\n",
            "\n",
            "Epoch 00185: val_loss improved from 0.33914 to 0.33904, saving model to folds1.hdf5\n",
            "Epoch 186/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3493 - val_loss: 0.3408\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.33904\n",
            "Epoch 187/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3521 - val_loss: 0.3437\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.33904\n",
            "Epoch 188/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3448 - val_loss: 0.3424\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.33904\n",
            "Epoch 189/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3430 - val_loss: 0.3428\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.33904\n",
            "Epoch 190/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3455 - val_loss: 0.3410\n",
            "\n",
            "Epoch 00190: ReduceLROnPlateau reducing learning rate to 0.0004218749818392098.\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.33904\n",
            "Epoch 191/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3399 - val_loss: 0.3356\n",
            "\n",
            "Epoch 00191: val_loss improved from 0.33904 to 0.33563, saving model to folds1.hdf5\n",
            "Epoch 192/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3409 - val_loss: 0.3380\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.33563\n",
            "Epoch 193/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3396 - val_loss: 0.3361\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.33563\n",
            "Epoch 194/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3406 - val_loss: 0.3387\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.33563\n",
            "Epoch 195/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3413 - val_loss: 0.3355\n",
            "\n",
            "Epoch 00195: val_loss improved from 0.33563 to 0.33555, saving model to folds1.hdf5\n",
            "Epoch 196/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3416 - val_loss: 0.3411\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.33555\n",
            "Epoch 197/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3388 - val_loss: 0.3341\n",
            "\n",
            "Epoch 00197: val_loss improved from 0.33555 to 0.33413, saving model to folds1.hdf5\n",
            "Epoch 198/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3394 - val_loss: 0.3343\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.33413\n",
            "Epoch 199/300\n",
            "59/59 [==============================] - 16s 263ms/step - loss: 0.3404 - val_loss: 0.3414\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.33413\n",
            "Epoch 200/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3418 - val_loss: 0.3356\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.33413\n",
            "Epoch 201/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3393 - val_loss: 0.3336\n",
            "\n",
            "Epoch 00201: val_loss improved from 0.33413 to 0.33362, saving model to folds1.hdf5\n",
            "Epoch 202/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3395 - val_loss: 0.3355\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.33362\n",
            "Epoch 203/300\n",
            "59/59 [==============================] - 16s 264ms/step - loss: 0.3415 - val_loss: 0.3387\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.33362\n",
            "Epoch 204/300\n",
            "54/59 [==========================>...] - ETA: 1s - loss: 0.3393"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZR_KtDCXk-c"
      },
      "source": [
        "mae(targets, np.median(np.vstack(test_preds),axis=0))"
      ],
      "id": "0ZR_KtDCXk-c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7025c5f6"
      },
      "source": [
        "PRESSURE_STEP = 0.07030214545120961\n",
        "PRESSURE_MIN = -1.8957442945646408\n",
        "PRESSURE_MAX = 64.82099173863948"
      ],
      "id": "7025c5f6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41dffad5"
      },
      "source": [
        "# ENSEMBLE FOLDS WITH MEAN\n",
        "submission[\"pressure\"] = sum(test_preds)/NUM_FOLDS\n",
        "submission.to_csv('submission_mean.csv', index=False)"
      ],
      "id": "41dffad5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bb56003"
      },
      "source": [
        "# ENSEMBLE FOLDS WITH MEDIAN\n",
        "submission[\"pressure\"] = np.median(np.vstack(test_preds),axis=0)\n",
        "submission.to_csv('submission_median.csv', index=False)"
      ],
      "id": "6bb56003",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9236b88e"
      },
      "source": [
        "# ENSEMBLE FOLDS WITH MEDIAN AND ROUND PREDICTIONS\n",
        "submission[\"pressure\"] =\\\n",
        "    np.round( (submission.pressure - PRESSURE_MIN)/PRESSURE_STEP ) * PRESSURE_STEP + PRESSURE_MIN\n",
        "submission.pressure = np.clip(submission.pressure, PRESSURE_MIN, PRESSURE_MAX)\n",
        "submission.to_csv('submission_median_round.csv', index=False)"
      ],
      "id": "9236b88e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sA-BSjvMmwfT"
      },
      "source": [
        "!zip cnn_models.zip *.hdf5"
      ],
      "id": "sA-BSjvMmwfT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHBnNrQTAdx0"
      },
      "source": [
        "!zip submission.zip *.csv"
      ],
      "id": "FHBnNrQTAdx0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5z9iaJ--P8P"
      },
      "source": [
        "with open(model_folder + 'submission.zip', 'wb') as f:\n",
        "  f.write(open('submission.zip', 'rb').read())"
      ],
      "id": "K5z9iaJ--P8P",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWRQJ02F-lwx"
      },
      "source": [
        "with open(model_folder + 'cnn_models.zip', 'wb') as f:\n",
        "  f.write(open('cnn_models.zip', 'rb').read())"
      ],
      "id": "hWRQJ02F-lwx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oC5y05zH2Ci"
      },
      "source": [
        ""
      ],
      "id": "-oC5y05zH2Ci",
      "execution_count": null,
      "outputs": []
    }
  ]
}