{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "ventilator-pressure-transformer_V6.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 5851.539198,
      "end_time": "2021-10-05T21:57:27.162714",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2021-10-05T20:19:55.623516",
      "version": "2.3.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bobbercheng/ventilator-pressure-prediction/blob/master/ventilator_pressure_transformer_V6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRRDmcGpsh4u"
      },
      "source": [
        "V1: Add PositionalEncoding\n",
        "\n",
        "V2: Adjust Transformer parameters\n",
        "\n",
        "V3: Use Transformer class instead of Tensorflow version\n",
        "\n",
        "V4: Without EDA and only use 400 features that is very similar to Keras time serial transformer sample if features and time serial are flatten. TPU v2 supports 16 heads, 512 key_dim, 512 batch size. But converge is not good\n",
        "\n",
        "V4: Max TPU V2 config, 128 heads, 512 key_dim, 512 batch size. Try build_complex_transfer_model. LP: 0.217, CX: 0.21, Train: 0.13. It looks complex transfer model can remember train data.\n",
        "\n",
        "V5: Try to add decoder\n",
        "\n",
        "V6: Complete transformer training. Please notice it only support one GUP/TPU\n",
        "Batch size: 256. Bath size 512 crashes in Colab.\n",
        "\n",
        "Epoch 10 Batch 0 Loss 0.8326\n",
        "Epoch 10 Batch 50 Loss 1.1333\n",
        "Epoch 10 Batch 100 Loss 1.1530\n",
        "Epoch 10 Batch 150 Loss 1.1773\n",
        "Epoch 10 Batch 200 Loss 1.1096\n",
        "Epoch 10 Batch 250 Loss 1.0583\n",
        "Epoch 10 Batch 0 Valid Loss 0.7480\n",
        "Saving checkpoint for epoch 10 at /tmp/ckpt/train/transformer/ckpt-2\n",
        "Epoch 10 Loss 1.0570 Valid Loss 0.7291\n",
        "Time taken for 1 epoch: 2874.25 secs\n",
        "\n",
        "Epoch 11 Batch 0 Loss 0.8168\n",
        "Epoch 11 Batch 50 Loss 0.8682\n",
        "Epoch 11 Batch 100 Loss 0.8439\n",
        "Epoch 11 Batch 150 Loss 0.8419\n",
        "Epoch 11 Batch 200 Loss 0.8387\n",
        "Epoch 11 Batch 250 Loss 0.8345\n",
        "Epoch 11 Batch 0 Valid Loss 0.6681\n",
        "Epoch 11 Loss 0.8339 Valid Loss 0.6813\n",
        "Time taken for 1 epoch: 2820.10 secs\n",
        "\n",
        "Epoch 12 Batch 0 Loss 0.7892\n",
        "Epoch 12 Batch 50 Loss 0.8301\n",
        "Epoch 12 Batch 100 Loss 0.8450\n",
        "Epoch 12 Batch 150 Loss 0.8319\n",
        "Epoch 12 Batch 200 Loss 0.8287\n",
        "Epoch 12 Batch 250 Loss 0.8282\n",
        "Epoch 12 Batch 0 Valid Loss 0.6762\n",
        "Epoch 12 Loss 0.8276 Valid Loss 0.6780\n",
        "Time taken for 1 epoch: 2832.69 secs\n",
        "\n",
        "Epoch 13 Batch 0 Loss 0.7996\n",
        "Epoch 13 Batch 50 Loss 0.8137\n",
        "Epoch 13 Batch 100 Loss 0.8002\n",
        "Epoch 13 Batch 150 Loss 0.7959\n",
        "Epoch 13 Batch 200 Loss 0.7964\n",
        "Epoch 13 Batch 250 Loss 0.8010\n",
        "Epoch 13 Batch 0 Valid Loss 0.6993\n",
        "Epoch 13 Loss 0.8012 Valid Loss 0.7040\n",
        "Time taken for 1 epoch: 2848.61 secs\n",
        "\n",
        "Epoch 14 Batch 0 Loss 0.7913\n",
        "Epoch 14 Batch 50 Loss 0.8400\n",
        "Epoch 14 Batch 100 Loss 0.7992\n",
        "Epoch 14 Batch 150 Loss 0.8004\n",
        "Epoch 14 Batch 200 Loss 0.7989\n",
        "Epoch 14 Batch 250 Loss 0.9859\n",
        "Epoch 14 Batch 0 Valid Loss 3.7756\n",
        "Epoch 14 Loss 1.0062 Valid Loss 3.5996\n",
        "Time taken for 1 epoch: 2818.39 secs\n",
        "\n",
        "Epoch 15 Batch 0 Loss 3.7839\n",
        "Epoch 15 Batch 50 Loss 3.1948\n",
        "Epoch 15 Batch 100 Loss 2.5220\n",
        "Epoch 15 Batch 150 Loss 2.2678\n",
        "Epoch 15 Batch 200 Loss 2.1025\n",
        "\n",
        "\n"
      ],
      "id": "TRRDmcGpsh4u"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPJR-B_AhTBC"
      },
      "source": [
        "# Update pandas version for Colab TPU\n",
        "#!pip install pandas==1.3.2"
      ],
      "id": "EPJR-B_AhTBC",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJaAgiAOkUEt"
      },
      "source": [
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False"
      ],
      "id": "mJaAgiAOkUEt",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oH5W_yWm-P8P",
        "outputId": "3f57fcb1-1cfc-4a31-923a-537e197a66be"
      },
      "source": [
        "if IN_COLAB:\n",
        "  from google.colab import drive\n",
        "  drive.mount(\"/gdrive\", force_remount=True)"
      ],
      "id": "oH5W_yWm-P8P",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhjzjzp5etAG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13d0a13c-a297-4b4e-ad59-630ebdd5df1b"
      },
      "source": [
        "if IN_COLAB:\n",
        "  !pip install kaggle\n",
        "  !mkdir /root/.kaggle\n",
        "  !cp /gdrive/MyDrive/ventilator-pressure-prediction/kaggle.json /root/.kaggle\n",
        "  !kaggle competitions download -c ventilator-pressure-prediction\n",
        "  !mkdir -p /kaggle/input/ventilator-pressure-prediction\n",
        "  !unzip '*.zip' -d /kaggle/input/ventilator-pressure-prediction\n",
        "  !ls /kaggle/input/ventilator-pressure-prediction"
      ],
      "id": "mhjzjzp5etAG",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.5.30)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.62.3)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n",
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4)\n",
            "test.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "sample_submission.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "train.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "Archive:  sample_submission.csv.zip\n",
            "replace /kaggle/input/ventilator-pressure-prediction/sample_submission.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n",
            "\n",
            "Archive:  train.csv.zip\n",
            "\n",
            "Archive:  test.csv.zip\n",
            "\n",
            "3 archives were successfully processed.\n",
            "sample_submission.csv  test.csv  train.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a16844ae"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# import optuna\n",
        "import matplotlib.pyplot as plt\n",
        "import gc\n",
        "import time\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler, ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error as mae\n",
        "from sklearn.preprocessing import RobustScaler, normalize\n",
        "from sklearn.model_selection import train_test_split, GroupKFold, KFold\n",
        "\n",
        "from IPython.display import display\n",
        "\n",
        "model_folder = '/gdrive/MyDrive/ventilator-pressure-prediction/transformer/'\n",
        "checkpoints_folder = '/gdrive/MyDrive/ventilator-pressure-prediction/transformer/checkpoints/train'\n"
      ],
      "id": "a16844ae",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEHQxSneTcVX"
      },
      "source": [
        "import os\n",
        "import random\n",
        "# Deep Learning\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "# Metrics\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Random Seed Initialize\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "def seed_everything(seed=RANDOM_SEED):\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    \n",
        "seed_everything()"
      ],
      "id": "LEHQxSneTcVX",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "564144b2"
      },
      "source": [
        "DEBUG = False\n",
        "\n",
        "train = pd.read_csv('/kaggle/input/ventilator-pressure-prediction/train.csv')\n",
        "test = pd.read_csv('/kaggle/input/ventilator-pressure-prediction/test.csv')\n",
        "submission = pd.read_csv('/kaggle/input/ventilator-pressure-prediction/sample_submission.csv')\n",
        "\n",
        "if DEBUG:\n",
        "    train = train[:80*10000]"
      ],
      "id": "564144b2",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08fd8094",
        "outputId": "561a82d0-feea-47e3-c78d-440005dc92ca"
      },
      "source": [
        "def add_features(df):\n",
        "    df['cross']= df['u_in'] * df['u_out']\n",
        "    df['cross2']= df['time_step'] * df['u_out']\n",
        "    df['area'] = df['time_step'] * df['u_in']\n",
        "    df['area'] = df.groupby('breath_id')['area'].cumsum()\n",
        "    df['time_step_cumsum'] = df.groupby(['breath_id'])['time_step'].cumsum()\n",
        "    df['u_in_cumsum'] = (df['u_in']).groupby(df['breath_id']).cumsum()\n",
        "    print(\"Step-1...Completed\")\n",
        "    \n",
        "    df['u_in_lag1'] = df.groupby('breath_id')['u_in'].shift(1)\n",
        "    df['u_out_lag1'] = df.groupby('breath_id')['u_out'].shift(1)\n",
        "    df['u_in_lag_back1'] = df.groupby('breath_id')['u_in'].shift(-1)\n",
        "    df['u_out_lag_back1'] = df.groupby('breath_id')['u_out'].shift(-1)\n",
        "    df['u_in_lag2'] = df.groupby('breath_id')['u_in'].shift(2)\n",
        "    df['u_out_lag2'] = df.groupby('breath_id')['u_out'].shift(2)\n",
        "    df['u_in_lag_back2'] = df.groupby('breath_id')['u_in'].shift(-2)\n",
        "    df['u_out_lag_back2'] = df.groupby('breath_id')['u_out'].shift(-2)\n",
        "    df['u_in_lag3'] = df.groupby('breath_id')['u_in'].shift(3)\n",
        "    df['u_out_lag3'] = df.groupby('breath_id')['u_out'].shift(3)\n",
        "    df['u_in_lag_back3'] = df.groupby('breath_id')['u_in'].shift(-3)\n",
        "    df['u_out_lag_back3'] = df.groupby('breath_id')['u_out'].shift(-3)\n",
        "    df['u_in_lag4'] = df.groupby('breath_id')['u_in'].shift(4)\n",
        "    df['u_out_lag4'] = df.groupby('breath_id')['u_out'].shift(4)\n",
        "    df['u_in_lag_back4'] = df.groupby('breath_id')['u_in'].shift(-4)\n",
        "    df['u_out_lag_back4'] = df.groupby('breath_id')['u_out'].shift(-4)\n",
        "    df = df.fillna(0)\n",
        "    print(\"Step-2...Completed\")\n",
        "    \n",
        "    df['breath_id__u_in__max'] = df.groupby(['breath_id'])['u_in'].transform('max')\n",
        "    df['breath_id__u_out__max'] = df.groupby(['breath_id'])['u_out'].transform('max')\n",
        "    df['breath_id__u_in__diffmax'] = df.groupby(['breath_id'])['u_in'].transform('max') - df['u_in']\n",
        "    df['breath_id__u_in__diffmean'] = df.groupby(['breath_id'])['u_in'].transform('mean') - df['u_in']\n",
        "    df['breath_id__u_in__diffmax'] = df.groupby(['breath_id'])['u_in'].transform('max') - df['u_in']\n",
        "    df['breath_id__u_in__diffmean'] = df.groupby(['breath_id'])['u_in'].transform('mean') - df['u_in']\n",
        "    print(\"Step-3...Completed\")\n",
        "    \n",
        "    df['u_in_diff1'] = df['u_in'] - df['u_in_lag1']\n",
        "    df['u_out_diff1'] = df['u_out'] - df['u_out_lag1']\n",
        "    df['u_in_diff2'] = df['u_in'] - df['u_in_lag2']\n",
        "    df['u_out_diff2'] = df['u_out'] - df['u_out_lag2']\n",
        "    df['u_in_diff3'] = df['u_in'] - df['u_in_lag3']\n",
        "    df['u_out_diff3'] = df['u_out'] - df['u_out_lag3']\n",
        "    df['u_in_diff4'] = df['u_in'] - df['u_in_lag4']\n",
        "    df['u_out_diff4'] = df['u_out'] - df['u_out_lag4']\n",
        "    print(\"Step-4...Completed\")\n",
        "    \n",
        "    df['one'] = 1\n",
        "    df['count'] = (df['one']).groupby(df['breath_id']).cumsum()\n",
        "    df['u_in_cummean'] =df['u_in_cumsum'] /df['count']\n",
        "    \n",
        "    df['breath_id_lag']=df['breath_id'].shift(1).fillna(0)\n",
        "    df['breath_id_lag2']=df['breath_id'].shift(2).fillna(0)\n",
        "    df['breath_id_lagsame']=np.select([df['breath_id_lag']==df['breath_id']],[1],0)\n",
        "    df['breath_id_lag2same']=np.select([df['breath_id_lag2']==df['breath_id']],[1],0)\n",
        "    df['breath_id__u_in_lag'] = df['u_in'].shift(1).fillna(0)\n",
        "    df['breath_id__u_in_lag'] = df['breath_id__u_in_lag'] * df['breath_id_lagsame']\n",
        "    df['breath_id__u_in_lag2'] = df['u_in'].shift(2).fillna(0)\n",
        "    df['breath_id__u_in_lag2'] = df['breath_id__u_in_lag2'] * df['breath_id_lag2same']\n",
        "    print(\"Step-5...Completed\")\n",
        "    \n",
        "    df['time_step_diff'] = df.groupby('breath_id')['time_step'].diff().fillna(0)\n",
        "    df['ewm_u_in_mean'] = (df\\\n",
        "                           .groupby('breath_id')['u_in']\\\n",
        "                           .ewm(halflife=9)\\\n",
        "                           .mean()\\\n",
        "                           .reset_index(level=0,drop=True))\n",
        "    df[[\"15_in_sum\",\"15_in_min\",\"15_in_max\",\"15_in_mean\"]] = (df\\\n",
        "                                                              .groupby('breath_id')['u_in']\\\n",
        "                                                              .rolling(window=15,min_periods=1)\\\n",
        "                                                              .agg({\"15_in_sum\":\"sum\",\n",
        "                                                                    \"15_in_min\":\"min\",\n",
        "                                                                    \"15_in_max\":\"max\",\n",
        "                                                                    \"15_in_mean\":\"mean\"\n",
        "                                                                    #\"15_in_std\":\"std\"\n",
        "                                                               })\\\n",
        "                                                               .reset_index(level=0,drop=True))\n",
        "    print(\"Step-6...Completed\")\n",
        "    \n",
        "    #df['u_in_diff_1_2'] = df['u_in_lag1'] - df['u_in_lag2']\n",
        "    #df['u_out_diff_1_2'] = df['u_out_lag1'] - df['u_out_lag2']\n",
        "    #df['u_in_lagback_diff_1_2'] = df['u_in_lag_back1'] - df['u_in_lag_back2']\n",
        "    #df['u_out_lagback_diff_1_2'] = df['u_out_lag_back1'] - df['u_out_lag_back2']\n",
        "    \n",
        "    df['u_in_lagback_diff1'] = df['u_in'] - df['u_in_lag_back1']\n",
        "    df['u_out_lagback_diff1'] = df['u_out'] - df['u_out_lag_back1']\n",
        "    df['u_in_lagback_diff2'] = df['u_in'] - df['u_in_lag_back2']\n",
        "    df['u_out_lagback_diff2'] = df['u_out'] - df['u_out_lag_back2']\n",
        "    print(\"Step-7...Completed\")\n",
        "    \n",
        "    df['R'] = df['R'].astype(str)\n",
        "    df['C'] = df['C'].astype(str)\n",
        "    df['R__C'] = df[\"R\"].astype(str) + '__' + df[\"C\"].astype(str)\n",
        "    df = pd.get_dummies(df)\n",
        "    print(\"Step-8...Completed\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "print(\"Train data...\\n\")\n",
        "train = add_features(train)\n",
        "\n",
        "print(\"\\nTest data...\\n\")\n",
        "test = add_features(test)\n",
        "\n"
      ],
      "id": "08fd8094",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train data...\n",
            "\n",
            "Step-1...Completed\n",
            "Step-2...Completed\n",
            "Step-3...Completed\n",
            "Step-4...Completed\n",
            "Step-5...Completed\n",
            "Step-6...Completed\n",
            "Step-7...Completed\n",
            "Step-8...Completed\n",
            "\n",
            "Test data...\n",
            "\n",
            "Step-1...Completed\n",
            "Step-2...Completed\n",
            "Step-3...Completed\n",
            "Step-4...Completed\n",
            "Step-5...Completed\n",
            "Step-6...Completed\n",
            "Step-7...Completed\n",
            "Step-8...Completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "431a7ee7",
        "outputId": "fd05193f-d264-4ce6-cc3f-ffc0c9b025fa"
      },
      "source": [
        "targets = train[['pressure']].to_numpy().reshape(-1, 80)\n",
        "\n",
        "u_outs = train[['u_out']].to_numpy().reshape(-1, 80)\n",
        "\n",
        "train.drop(['pressure','id', 'breath_id'], axis=1, inplace=True)\n",
        "\n",
        "test = test.drop(['id', 'breath_id'], axis=1)\n",
        "\n",
        "train.head(), train.shape"
      ],
      "id": "431a7ee7",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(   time_step       u_in  u_out  ...  R__C_5__10  R__C_5__20  R__C_5__50\n",
              " 0   0.000000   0.083334      0  ...           0           0           0\n",
              " 1   0.033652  18.383041      0  ...           0           0           0\n",
              " 2   0.067514  22.509278      0  ...           0           0           0\n",
              " 3   0.101542  22.808822      0  ...           0           0           0\n",
              " 4   0.135756  25.355850      0  ...           0           0           0\n",
              " \n",
              " [5 rows x 70 columns], (6036000, 70))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEAbCWmOoLH6"
      },
      "source": [
        "train.drop(['one','count',\n",
        "            'breath_id_lag','breath_id_lag2','breath_id_lagsame',\n",
        "            'breath_id_lag2same'], axis=1, inplace=True)\n",
        "\n",
        "test = test.drop(['one','count','breath_id_lag',\n",
        "                  'breath_id_lag2','breath_id_lagsame',\n",
        "                  'breath_id_lag2same'], axis=1)"
      ],
      "id": "IEAbCWmOoLH6",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d24ef65c"
      },
      "source": [
        "RS = RobustScaler()\n",
        "train = RS.fit_transform(train)\n",
        "test = RS.transform(test)"
      ],
      "id": "d24ef65c",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6f1a3e19"
      },
      "source": [
        "train = train.reshape(-1, 80, train.shape[-1])\n",
        "test = test.reshape(-1, 80, train.shape[-1])"
      ],
      "id": "6f1a3e19",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4lOIi9HdExc"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras"
      ],
      "id": "m4lOIi9HdExc",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqA2324UJKb_"
      },
      "source": [
        "# refer to https://www.tensorflow.org/text/tutorials/transformer#encoder\n",
        "def get_angles(pos, i, d_model):\n",
        "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "  return pos * angle_rates\n",
        "\n",
        "def positional_encoding(position, d_model):\n",
        "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model)\n",
        "\n",
        "  # apply sin to even indices in the array; 2i\n",
        "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "  # apply cos to odd indices in the array; 2i+1\n",
        "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "  pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "  \"\"\"Calculate the attention weights.\n",
        "  q, k, v must have matching leading dimensions.\n",
        "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "  The mask has different shapes depending on its type(padding or look ahead)\n",
        "  but it must be broadcastable for addition.\n",
        "\n",
        "  Args:\n",
        "    q: query shape == (..., seq_len_q, depth)\n",
        "    k: key shape == (..., seq_len_k, depth)\n",
        "    v: value shape == (..., seq_len_v, depth_v)\n",
        "    mask: Float tensor with shape broadcastable\n",
        "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "\n",
        "  Returns:\n",
        "    output, attention_weights\n",
        "  \"\"\"\n",
        "\n",
        "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "  # scale matmul_qk\n",
        "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "  # add the mask to the scaled tensor.\n",
        "  if mask is not None:\n",
        "    scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "  # add up to 1.\n",
        "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "\n",
        "  return output, attention_weights\n",
        "\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, **kwargs):\n",
        "    super(MultiHeadAttention, self).__init__(**kwargs)\n",
        "    self.num_heads = num_heads\n",
        "    self.d_model = d_model\n",
        "    assert self.d_model % self.num_heads == 0\n",
        "    self.depth = d_model // self.num_heads\n",
        "    self.build_components()\n",
        "\n",
        "  def build_components(self):\n",
        "    self.wq = tf.keras.layers.Dense(self.d_model)\n",
        "    self.wk = tf.keras.layers.Dense(self.d_model)\n",
        "    self.wv = tf.keras.layers.Dense(self.d_model)\n",
        "\n",
        "    self.dense = tf.keras.layers.Dense(self.d_model)\n",
        "\n",
        "  def split_heads(self, x, batch_size):\n",
        "    \"\"\"Split the last dimension into (num_heads, depth).\n",
        "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "    \"\"\"\n",
        "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "  def call(self, v, k, q, mask):\n",
        "    batch_size = tf.shape(q)[0]\n",
        "\n",
        "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "\n",
        "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "        q, k, v, mask)\n",
        "\n",
        "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
        "\n",
        "    concat_attention = tf.reshape(scaled_attention,\n",
        "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "    return output, attention_weights\n",
        "\n",
        "  # def get_config(self):\n",
        "  #   config = super(MultiHeadAttention, self).get_config()\n",
        "  #   config.update({\"num_heads\": self.num_heads,\n",
        "  #           \"d_model\": self.d_model})\n",
        "  #   return config\n",
        "\n",
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "  # Feed forward can be Den lay or CNN as long as it keep final dimension\n",
        "  #x = layers.Conv1D(filters=ff_dims, kernel_size=1, activation=\"selu\")(x)\n",
        "  #x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
        "  return tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
        "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
        "  ])\n",
        "\n",
        "def create_look_ahead_mask(size):\n",
        "  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "  return mask  # (seq_len, seq_len)\n",
        "\n",
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, dff, rate=0.1, **kwargs):\n",
        "    super(EncoderLayer, self).__init__(**kwargs)\n",
        "    self.d_model = d_model\n",
        "    self.num_heads = num_heads\n",
        "    self.dff = dff\n",
        "    self.rate = rate\n",
        "    self.build_components()\n",
        "\n",
        "  def build_components(self):\n",
        "    self.mha = MultiHeadAttention(self.d_model, self.num_heads)\n",
        "    self.ffn = point_wise_feed_forward_network(self.d_model, self.dff)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(self.rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(self.rate)\n",
        "\n",
        "  def call(self, x, training, mask):\n",
        "\n",
        "    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
        "    attn_output = self.dropout1(attn_output, training=training)\n",
        "    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "    ffn_output = self.dropout2(ffn_output, training=training)\n",
        "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    return out2\n",
        "\n",
        "  # def get_config(self):\n",
        "  #   config = super(EncoderLayer, self).get_config()\n",
        "  #   config.update({\"num_heads\": self.num_heads,\n",
        "  #           \"d_model\": self.d_model,\n",
        "  #           'dff': self.dff,\n",
        "  #           'rate': self.rate\n",
        "  #           })\n",
        "  #   return config\n",
        "\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, \n",
        "               #input_vocab_size,\n",
        "               maximum_position_encoding, rate=0.1, **kwargs):\n",
        "    super(Encoder, self).__init__(**kwargs)\n",
        "\n",
        "    self.num_layers = num_layers\n",
        "    self.d_model = d_model\n",
        "    self.num_heads = num_heads\n",
        "    self.dff = dff\n",
        "    self.rate = rate\n",
        "    self.maximum_position_encoding = maximum_position_encoding\n",
        "    self.build_components()\n",
        "\n",
        "  def build_components(self):\n",
        "    # self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "    self.embedding = tf.keras.layers.Dense(self.d_model)\n",
        "    self.pos_encoding = positional_encoding(self.maximum_position_encoding,\n",
        "                                            self.d_model)\n",
        "\n",
        "    self.enc_layers = [EncoderLayer(self.d_model, self.num_heads, self.dff, self.rate)\n",
        "                       for _ in range(self.num_layers)]\n",
        "\n",
        "    self.dropout = tf.keras.layers.Dropout(self.rate)\n",
        "\n",
        "  def call(self, x, training, mask):\n",
        "\n",
        "    seq_len = tf.shape(x)[1]\n",
        "\n",
        "    # adding embedding and position encoding.\n",
        "    x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x, training, mask)\n",
        "\n",
        "    return x  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "  # def get_config(self):\n",
        "  #   config = super(Encoder, self).get_config()\n",
        "  #   config.update({'num_layers': self.num_layers,\n",
        "  #           \"num_heads\": self.num_heads,\n",
        "  #           \"d_model\": self.d_model,\n",
        "  #           'dff': self.dff,\n",
        "  #           'rate': self.rate,\n",
        "  #           'maximum_position_encoding': self.maximum_position_encoding\n",
        "  #           })\n",
        "  #   return config\n",
        "\n",
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, dff, rate=0.1, **kwargs):\n",
        "    super(DecoderLayer, self).__init__(**kwargs)\n",
        "    self.d_model = d_model\n",
        "    self.num_heads = num_heads\n",
        "    self.dff = dff\n",
        "    self.rate = rate\n",
        "    self.build_components()\n",
        "\n",
        "  def build_components(self):\n",
        "    self.mha1 = MultiHeadAttention(self.d_model, self.num_heads)\n",
        "    self.mha2 = MultiHeadAttention(self.d_model, self.num_heads)\n",
        "\n",
        "    self.ffn = point_wise_feed_forward_network(self.d_model, self.dff)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(self.rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(self.rate)\n",
        "    self.dropout3 = tf.keras.layers.Dropout(self.rate)\n",
        "\n",
        "  def call(self, x, enc_output, training,\n",
        "           look_ahead_mask, padding_mask):\n",
        "    # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
        "    attn1 = self.dropout1(attn1, training=training)\n",
        "    out1 = self.layernorm1(attn1 + x)\n",
        "\n",
        "    attn2, attn_weights_block2 = self.mha2(\n",
        "        enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
        "    attn2 = self.dropout2(attn2, training=training)\n",
        "    out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "    ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
        "    ffn_output = self.dropout3(ffn_output, training=training)\n",
        "    out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "    return out3, attn_weights_block1, attn_weights_block2\n",
        "\n",
        "  # def get_config(self):\n",
        "  #   config = super(DecoderLayer, self).get_config()\n",
        "  #   config.update({\"num_heads\": self.num_heads,\n",
        "  #           \"d_model\": self.d_model,\n",
        "  #           'dff': self.dff,\n",
        "  #           'rate': self.rate\n",
        "  #           })\n",
        "  #   return config\n",
        "\n",
        "\n",
        "\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, \n",
        "              #  target_vocab_size,\n",
        "               maximum_position_encoding, rate=0.1, **kwargs):\n",
        "    super(Decoder, self).__init__(**kwargs)\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "    self.num_heads = num_heads\n",
        "    self.dff = dff\n",
        "    self.rate = rate\n",
        "    self.maximum_position_encoding = maximum_position_encoding\n",
        "    self.build_components()\n",
        "\n",
        "  def build_components(self):\n",
        "    # self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "    self.embedding = tf.keras.layers.Dense(self.d_model)\n",
        "    self.pos_encoding = positional_encoding(self.maximum_position_encoding, self.d_model)\n",
        "\n",
        "    self.dec_layers = [DecoderLayer(self.d_model, self.num_heads, self.dff, self.rate)\n",
        "                       for _ in range(self.num_layers)]\n",
        "    self.dropout = tf.keras.layers.Dropout(self.rate)\n",
        "\n",
        "  def call(self, x, enc_output, training,\n",
        "           look_ahead_mask, padding_mask):\n",
        "\n",
        "    seq_len = tf.shape(x)[1]\n",
        "    attention_weights = {}\n",
        "\n",
        "    x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
        "                                             look_ahead_mask, padding_mask)\n",
        "\n",
        "      attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
        "      attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
        "\n",
        "    # x.shape == (batch_size, target_seq_len, d_model)\n",
        "    return x, attention_weights\n",
        "\n",
        "  # def get_config(self):\n",
        "  #   config = super(Decoder, self).get_config()\n",
        "  #   config.update({'num_layers': self.num_layers,\n",
        "  #           \"num_heads\": self.num_heads,\n",
        "  #           \"d_model\": self.d_model,\n",
        "  #           'dff': self.dff,\n",
        "  #           'rate': self.rate,\n",
        "  #           'maximum_position_encoding': self.maximum_position_encoding\n",
        "  #           })\n",
        "  #   return config\n",
        "\n",
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, \n",
        "              #  input_vocab_size, target_vocab_size,\n",
        "               pe_input, pe_target, rate=0.1, target_dim=1, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "    self.num_heads = num_heads\n",
        "    self.dff = dff\n",
        "    self.rate = rate\n",
        "    self.pe_input = pe_input\n",
        "    self.pe_target = pe_target\n",
        "    self.target_dim = target_dim\n",
        "    self.build_components()\n",
        "\n",
        "  def build_components(self):\n",
        "    self.encoder = Encoder(self.num_layers, self.d_model, self.num_heads, self.dff,\n",
        "                            #  input_vocab_size, \n",
        "                           self.pe_input, self.rate)\n",
        "\n",
        "    self.decoder = Decoder(self.num_layers, self.d_model, self.num_heads, self.dff,\n",
        "                          #  target_vocab_size, \n",
        "                           self.pe_target, self.rate)\n",
        "\n",
        "    self.final_layer = tf.keras.layers.Dense(self.target_dim)\n",
        "\n",
        "  def call(self, inputs, training):\n",
        "    # Keras models prefer if you pass all your inputs in the first argument\n",
        "    inp, tar = inputs\n",
        "\n",
        "    enc_padding_mask, look_ahead_mask, dec_padding_mask = self.create_masks(inp, tar)\n",
        "\n",
        "    enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
        "\n",
        "    # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
        "    dec_output, attention_weights = self.decoder(\n",
        "        tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "\n",
        "    final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
        "\n",
        "    return final_output, attention_weights\n",
        "\n",
        "  def create_masks(self, inp, tar):\n",
        "    # Encoder padding mask\n",
        "    # enc_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "    # Used in the 2nd attention block in the decoder.\n",
        "    # This padding mask is used to mask the encoder outputs.\n",
        "    # dec_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "    # Used in the 1st attention block in the decoder.\n",
        "    # It is used to pad and mask future tokens in the input received by\n",
        "    # the decoder.\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "    # dec_target_padding_mask = create_padding_mask(tar)\n",
        "    # look_ahead_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "\n",
        "    # return enc_padding_mask, look_ahead_mask, dec_padding_mask\n",
        "    return None, look_ahead_mask, None\n",
        "\n",
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=4000, **kwargs):\n",
        "    super(CustomSchedule, self).__init__(**kwargs)\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "\n",
        "  def __call__(self, step):\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "id": "dqA2324UJKb_",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJLs2RP98Zir"
      },
      "source": [
        "n, d = 80, 64\n",
        "pos_encoding = positional_encoding(n, d)\n",
        "print(pos_encoding.shape)\n",
        "pos_encoding = pos_encoding[0]\n",
        "\n",
        "# Juggle the dimensions for the plot\n",
        "pos_encoding = tf.reshape(pos_encoding, (n, d//2, 2))\n",
        "pos_encoding = tf.transpose(pos_encoding, (2, 1, 0))\n",
        "pos_encoding = tf.reshape(pos_encoding, (d, n))\n",
        "\n",
        "plt.pcolormesh(pos_encoding, cmap='RdBu')\n",
        "plt.ylabel('Depth')\n",
        "plt.xlabel('Position')\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "id": "AJLs2RP98Zir"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8U60IUEd6kal"
      },
      "source": [
        "# **** Can not print anything if loss_function is called by train_step\n",
        "def loss_function(tar_real_u_outs, pred, cols = 80):\n",
        "  real = tar_real_u_outs[:, :cols, :]\n",
        "  u_out = tar_real_u_outs[:, cols:, :]\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = 1 - u_out\n",
        "  mask = tf.squeeze(mask, [-1])\n",
        "  loss_ *= mask\n",
        "\n",
        "  #If all are masked, return 0\n",
        "  # if tf.math.count_nonzero(mask) == 0:\n",
        "  #   return 0.0\n",
        "  \n",
        "  return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
        "\n",
        "def accuracy_function(real, pred):\n",
        "  accuracies = tf.equal(real, pred)\n",
        "  accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
        "  # print(accuracies)\n",
        "  return tf.reduce_mean(accuracies)"
      ],
      "id": "8U60IUEd6kal",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7wCaptl1KWt"
      },
      "source": [
        "temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
        "y = tf.random.uniform((1, 60, 512))  # (batch_size, encoder_sequence, d_model)\n",
        "out, attn = temp_mha(y, k=y, q=y, mask=None)\n",
        "out.shape, attn.shape"
      ],
      "id": "v7wCaptl1KWt"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSeBs1ni4Q5d"
      },
      "source": [
        "sample_ffn = point_wise_feed_forward_network(512, 2048)\n",
        "sample_ffn(tf.random.uniform((64, 50, 512))).shape"
      ],
      "id": "aSeBs1ni4Q5d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5cQ-rK586oj"
      },
      "source": [
        "sample_encoder_layer = EncoderLayer(512, 8, 2048)\n",
        "\n",
        "sample_encoder_layer_output = sample_encoder_layer(\n",
        "    tf.random.uniform((64, 43, 512)), False, None)\n",
        "\n",
        "sample_encoder_layer_output.shape  # (batch_size, input_seq_len, d_model)"
      ],
      "id": "r5cQ-rK586oj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQRCtoP0-TOl"
      },
      "source": [
        "sample_decoder_layer = DecoderLayer(512, 8, 2048)\n",
        "config = sample_decoder_layer.get_config()\n",
        "print(config)\n",
        "new_layer = DecoderLayer.from_config(config)\n",
        "\n",
        "sample_decoder_layer_output, _, _ = new_layer(\n",
        "    tf.random.uniform((64, 50, 512)), sample_encoder_layer_output,\n",
        "    False, None, None)\n",
        "\n",
        "sample_decoder_layer_output.shape  # (batch_size, target_seq_len, d_model)\n"
      ],
      "id": "dQRCtoP0-TOl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYb6A00LMSJY"
      },
      "source": [
        "sample_encoder = Encoder(num_layers=2, d_model=512, num_heads=8,\n",
        "                         dff=2048, \n",
        "                        #  input_vocab_size=8500,\n",
        "                         maximum_position_encoding=10000)\n",
        "temp_input = tf.random.uniform((64, 62, 512), dtype=tf.int64, minval=0, maxval=200)\n",
        "\n",
        "sample_encoder_output = sample_encoder(temp_input, training=False, mask=None)\n",
        "\n",
        "print(sample_encoder_output.shape)  # (batch_size, input_seq_len, d_model)"
      ],
      "id": "YYb6A00LMSJY"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzrVsWnHUIpH"
      },
      "source": [
        "sample_decoder = Decoder(num_layers=2, d_model=512, num_heads=8,\n",
        "                         dff=2048, \n",
        "                        #  target_vocab_size=8000,\n",
        "                         maximum_position_encoding=5000)\n",
        "temp_input = tf.random.uniform((64, 26, 512), dtype=tf.int64, minval=0, maxval=200)\n",
        "\n",
        "output, attn = sample_decoder(temp_input,\n",
        "                              enc_output=sample_encoder_output,\n",
        "                              training=False,\n",
        "                              look_ahead_mask=None,\n",
        "                              padding_mask=None)\n",
        "\n",
        "output.shape, attn['decoder_layer2_block2'].shape"
      ],
      "id": "OzrVsWnHUIpH"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4d62Did6ZOIo"
      },
      "source": [
        "\n",
        "d_model = 512\n",
        "temp_learning_rate_schedule = CustomSchedule(d_model)\n",
        "\n",
        "plt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))\n",
        "plt.ylabel(\"Learning Rate\")\n",
        "plt.xlabel(\"Train Step\")"
      ],
      "id": "4d62Did6ZOIo"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEyIJqVcYiKv"
      },
      "source": [
        "sample_transformer = Transformer(\n",
        "    num_layers=2, d_model=512, num_heads=8, dff=2048,\n",
        "    # input_vocab_size=8500, target_vocab_size=8000,\n",
        "    pe_input=10000, pe_target=6000)\n",
        "\n",
        "temp_input = tf.random.uniform((64, 38, 512), dtype=tf.int64, minval=0, maxval=200)\n",
        "temp_target = tf.random.uniform((64, 36, 512), dtype=tf.int64, minval=0, maxval=200)\n",
        "\n",
        "fn_out, _ = sample_transformer([temp_input, temp_target], training=False)\n",
        "\n",
        "fn_out.shape  # (batch_size, tar_seq_len, target_vocab_size)"
      ],
      "id": "jEyIJqVcYiKv"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOcstL-LZuZZ"
      },
      "source": [
        "num_layers=6\n",
        "d_model = 512\n",
        "dff = 2048\n",
        "num_heads = 8\n",
        "dropout_rate = 0.1"
      ],
      "id": "jOcstL-LZuZZ",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wovssFGA1eet"
      },
      "source": [
        "START = -2\n",
        "def add_Start_to_targets(targets, u_outs):\n",
        "  new_targets = np.insert(np.append(targets, u_outs, axis=1), 0, START, axis=1).reshape(-1, targets.shape[1] + u_outs.shape[-1] + 1, 1)\n",
        "  return new_targets\n",
        "\n",
        "def get_targets_without_Start(targets):\n",
        "  return targets[:, 1:]"
      ],
      "id": "wovssFGA1eet",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_zBkOsH31PO"
      },
      "source": [
        "new_targets = add_Start_to_targets(targets)\n",
        "new_targets.shape, targets.shape"
      ],
      "id": "C_zBkOsH31PO"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--4JMOdJ4fxF"
      },
      "source": [
        "def getOptimizer():\n",
        "  learning_rate = CustomSchedule(d_model)\n",
        "\n",
        "  optimizer = tf.keras.optimizers.Adam(\n",
        "      learning_rate, \n",
        "      beta_1=0.9, beta_2=0.98,epsilon=1e-9)\n",
        "  return optimizer\n",
        "\n",
        "optimizer = getOptimizer()"
      ],
      "id": "--4JMOdJ4fxF",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvnVusZg4f6X"
      },
      "source": [
        "loss_object = tf.keras.losses.MeanAbsoluteError(reduction=tf.keras.losses.Reduction.NONE)"
      ],
      "id": "EvnVusZg4f6X",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-StJ03bs9To2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c85a6ada-6f36-42cb-b896-f67deda402a9"
      },
      "source": [
        "y_true = np.zeros([2, 160, 1])\n",
        "y_true[-1, 58, 0] = 1\n",
        "y_true[-1, -1, 0] = 1\n",
        "y_true = tf.convert_to_tensor(y_true, dtype=tf.float32)\n",
        "# pred and u_out\n",
        "y_pred = tf.ones([2, 80, 1], tf.float32)\n",
        "test_loss = loss_function(y_true, y_pred)\n",
        "print('loss:', test_loss)\n",
        "\n",
        "assert test_loss == 158/159"
      ],
      "id": "-StJ03bs9To2",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tf.Tensor(0.9937107, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InsS81eV8HEW"
      },
      "source": [
        "y_true = tf.constant([[[0.], [1.]], [[0.], [0.]]])\n",
        "y_pred = tf.constant([[[2.], [1.]], [[2.], [0.]]])\n",
        "y_true.shape, accuracy_function(y_true, y_pred)"
      ],
      "id": "InsS81eV8HEW"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7dNXdoL_FDH"
      },
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "valid_loss = tf.keras.metrics.Mean(name='valid_loss')\n",
        "# train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')"
      ],
      "id": "c7dNXdoL_FDH",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3L1_ptyU_Gw1"
      },
      "source": [
        "def get_transformer():\n",
        "  transformer = Transformer(\n",
        "      num_layers=num_layers,\n",
        "      d_model=d_model,\n",
        "      num_heads=num_heads,\n",
        "      dff=dff,\n",
        "      # input_vocab_size=tokenizers.pt.get_vocab_size().numpy(),\n",
        "      # target_vocab_size=tokenizers.en.get_vocab_size().numpy(),\n",
        "      pe_input=80,\n",
        "      pe_target=80,\n",
        "      rate=dropout_rate)\n",
        "  return transformer\n",
        "\n",
        "transformer = get_transformer()"
      ],
      "id": "3L1_ptyU_Gw1",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgzSVrCORJUm"
      },
      "source": [
        "checkpoint_path = \"/tmp/ckpt/train/transformer\""
      ],
      "id": "YgzSVrCORJUm",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFaL1uISRCcV"
      },
      "source": [
        "#copy latest checkpoint from google driver\n",
        "!mkdir -p /tmp/ckpt/train/transformer\n",
        "!cp /gdrive/MyDrive/ventilator-pressure-prediction/transformer/checkpoints/train/* /tmp/ckpt/train/transformer"
      ],
      "id": "eFaL1uISRCcV"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqNFfg8X_G0E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d68d893-d2cd-47f0-a49b-f02e8b706fea"
      },
      "source": [
        "if IN_COLAB:\n",
        "  save_options = tf.train.CheckpointOptions(experimental_io_device=\"/job:localhost\")\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "# if a checkpoint exists, restore the latest checkpoint.\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint, options=save_options)\n",
        "  print('Latest checkpoint restored!!')"
      ],
      "id": "MqNFfg8X_G0E",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Latest checkpoint restored!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nt7ahcTE_YFV"
      },
      "source": [
        "EPOCHS = 100\n",
        "BATCH_SIZE = 256#1024\n",
        "NUM_FOLDS = 7"
      ],
      "id": "nt7ahcTE_YFV",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WeSVrUJUWs-"
      },
      "source": [
        "# Function to get hardware strategy\n",
        "def get_hardware_strategy():\n",
        "    try:\n",
        "        # TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
        "        # set: this is always the case on Kaggle.\n",
        "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "        print('Running on TPU ', tpu.master())\n",
        "    except ValueError:\n",
        "        tpu = None\n",
        "\n",
        "    if tpu:\n",
        "        tf.config.experimental_connect_to_cluster(tpu)\n",
        "        tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "        tf.config.optimizer.set_jit(True)\n",
        "    else:\n",
        "        # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
        "        strategy = tf.distribute.get_strategy()\n",
        "\n",
        "    return tpu, strategy\n",
        "\n",
        "tpu, strategy = get_hardware_strategy()"
      ],
      "id": "2WeSVrUJUWs-"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDUQqHUv_YOb"
      },
      "source": [
        "# The @tf.function trace-compiles train_step into a TF graph for faster\n",
        "# execution. The function specializes to the precise shape of the argument\n",
        "# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n",
        "# batch sizes (the last batch is smaller), use input_signature to specify\n",
        "# more generic shapes.\n",
        "\n",
        "train_step_signature = [\n",
        "    tf.TensorSpec(shape=(None, 80, 64), dtype=tf.float32),\n",
        "    tf.TensorSpec(shape=(None, 161, 1), dtype=tf.float32),\n",
        "]\n",
        "\n",
        "\n",
        "@tf.function(input_signature=train_step_signature)\n",
        "def train_step(inp, tar):\n",
        "  tar_inp = tar[:, :80, :]\n",
        "  tar_real = tar[:, 1:81, :]\n",
        "  tar_real_u_outs = tar[:, 1:, :]\n",
        "  # print(inp.shape, tar_inp.shape, tar_real.shape, tar_real_u_outs.shape)\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions, _ = transformer([inp, tar_inp],\n",
        "                                 training = True)\n",
        "    loss = loss_function(tar_real_u_outs, predictions)\n",
        "\n",
        "  gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "  train_loss(loss)\n",
        "  # train_accuracy(accuracy_function(tar_real, predictions))\n",
        "\n",
        "train_step_signature = [\n",
        "    tf.TensorSpec(shape=(None, 80, 64), dtype=tf.float32),\n",
        "    tf.TensorSpec(shape=(None, 161, 1), dtype=tf.float32),\n",
        "]\n",
        "\n",
        "valid_step_signature = [\n",
        "    tf.TensorSpec(shape=(None, 80, 64), dtype=tf.float32),\n",
        "    tf.TensorSpec(shape=(None, 161, 1), dtype=tf.float32),\n",
        "]\n",
        "@tf.function(input_signature=valid_step_signature)\n",
        "def valid_step(inp, tar):\n",
        "  tar_inp = tar[:, :80, :]\n",
        "  tar_real = tar[:, 1:81, :]\n",
        "  tar_real_u_outs = tar[:, 1:, :]\n",
        "  # print(inp.shape, tar_inp.shape, tar_real.shape, tar_real_u_outs.shape)\n",
        "\n",
        "  predictions, _ = transformer([inp, tar_inp],\n",
        "                                training = False)\n",
        "  loss = loss_function(tar_real_u_outs, predictions)\n",
        "\n",
        "  valid_loss(loss)\n",
        "  return loss"
      ],
      "id": "RDUQqHUv_YOb",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ko54Rz-DAayH",
        "outputId": "84162fa4-a833-4b8b-f987-08e91a5b4ce5"
      },
      "source": [
        "def make_batches(ds):\n",
        "  return (\n",
        "      ds\n",
        "      .cache()\n",
        "      # .shuffle(BUFFER_SIZE)\n",
        "      .batch(BATCH_SIZE)\n",
        "      # .map(tokenize_pairs, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "      .prefetch(tf.data.AUTOTUNE))\n",
        "\n",
        "# Please don't use multiple GUP or TPU for train here\n",
        "# with strategy.scope():\n",
        "\n",
        "kf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=2021)\n",
        "test_preds = []\n",
        "train_preds = []\n",
        "for fold, (train_idx, test_idx) in enumerate(kf.split(train, targets)):\n",
        "  print('-'*15, '>', f'Fold {fold+1}', '<', '-'*15)\n",
        "  X_train, X_valid = train[train_idx], train[test_idx]\n",
        "  y_train, y_valid = targets[train_idx], targets[test_idx]\n",
        "  u_out_train, u_out_valid = u_outs[train_idx], u_outs[test_idx]\n",
        "\n",
        "  train_dataset = tf.data.Dataset.from_tensor_slices((tf.cast(X_train, dtype=tf.float32), tf.cast(add_Start_to_targets(y_train, u_out_train), dtype=tf.float32)))\n",
        "  valid_dataset = tf.data.Dataset.from_tensor_slices((tf.cast(X_valid, dtype=tf.float32), tf.cast(add_Start_to_targets(y_valid, u_out_valid), dtype=tf.float32)))\n",
        "\n",
        "  train_batches = make_batches(train_dataset)\n",
        "  val_batches = make_batches(valid_dataset)\n",
        "\n",
        "  for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    train_loss.reset_states()\n",
        "    valid_loss.reset_states()\n",
        "    # train_accuracy.reset_states()\n",
        "\n",
        "    # Train batch loop\n",
        "    for (batch, (inp, tar)) in enumerate(train_batches):\n",
        "      # print(inp.shape, tar.shape)\n",
        "      train_step(inp, tar)\n",
        "\n",
        "      if batch % 50 == 0:\n",
        "        current_lr = optimizer._decayed_lr('float32').numpy()\n",
        "        print(f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f} Learning rate {current_lr:.4f}')\n",
        "      # else:\n",
        "      #   print(f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f}')\n",
        "\n",
        "    # Valid batch loop\n",
        "    for (batch, (inp, tar)) in enumerate(val_batches):\n",
        "      # print(inp.shape, tar.shape)\n",
        "      valid_step(inp, tar)\n",
        "\n",
        "      if batch % 50 == 0:\n",
        "        print(f'Epoch {epoch + 1} Batch {batch} Valid Loss {valid_loss.result():.4f}')\n",
        "    \n",
        "\n",
        "    if (epoch + 1) % 2 == 0:\n",
        "      ckpt_save_path = ckpt_manager.save(options=save_options)\n",
        "      print(f'Saving checkpoint for epoch {epoch+1} at {ckpt_save_path}')\n",
        "      if IN_COLAB:\n",
        "        !cp /tmp/ckpt/train/transformer/* /gdrive/MyDrive/ventilator-pressure-prediction/transformer/checkpoints/train\n",
        "\n",
        "    print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Valid Loss {valid_loss.result():.4f}')\n",
        "\n",
        "    print(f'Time taken for 1 epoch: {time.time() - start:.2f} secs\\n')\n",
        "  break\n"
      ],
      "id": "ko54Rz-DAayH",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------- > Fold 1 < ---------------\n",
            "Epoch 1 Batch 0 Loss 0.8203 Learning rate 0.0004\n",
            "Epoch 1 Batch 50 Loss 0.8344 Learning rate 0.0005\n",
            "Epoch 1 Batch 100 Loss 0.8396 Learning rate 0.0005\n",
            "Epoch 1 Batch 150 Loss 0.8372 Learning rate 0.0005\n",
            "Epoch 1 Batch 200 Loss 0.8337 Learning rate 0.0005\n",
            "Epoch 1 Batch 250 Loss 0.8365 Learning rate 0.0005\n",
            "Epoch 1 Batch 0 Valid Loss 0.8874\n",
            "Epoch 1 Loss 0.8365 Valid Loss 0.8513\n",
            "Time taken for 1 epoch: 2839.91 secs\n",
            "\n",
            "Epoch 2 Batch 0 Loss 0.8566 Learning rate 0.0005\n",
            "Epoch 2 Batch 50 Loss 0.8649 Learning rate 0.0005\n",
            "Epoch 2 Batch 100 Loss 0.8235 Learning rate 0.0005\n",
            "Epoch 2 Batch 150 Loss 0.8288 Learning rate 0.0005\n",
            "Epoch 2 Batch 200 Loss 0.8264 Learning rate 0.0005\n",
            "Epoch 2 Batch 250 Loss 1.6590 Learning rate 0.0005\n",
            "Epoch 2 Batch 0 Valid Loss 4.7205\n",
            "Saving checkpoint for epoch 2 at /tmp/ckpt/train/transformer/ckpt-3\n",
            "Epoch 2 Loss 1.6814 Valid Loss 4.5223\n",
            "Time taken for 1 epoch: 2843.56 secs\n",
            "\n",
            "Epoch 3 Batch 0 Loss 4.7976 Learning rate 0.0005\n",
            "Epoch 3 Batch 50 Loss 4.0091 Learning rate 0.0005\n",
            "Epoch 3 Batch 100 Loss 4.2910 Learning rate 0.0005\n",
            "Epoch 3 Batch 150 Loss 5.3511 Learning rate 0.0006\n",
            "Epoch 3 Batch 200 Loss 5.4217 Learning rate 0.0006\n",
            "Epoch 3 Batch 250 Loss 5.3641 Learning rate 0.0006\n",
            "Epoch 3 Batch 0 Valid Loss 5.2049\n",
            "Epoch 3 Loss 5.3610 Valid Loss 5.0477\n",
            "Time taken for 1 epoch: 2848.57 secs\n",
            "\n",
            "Epoch 4 Batch 0 Loss 5.2378 Learning rate 0.0006\n",
            "Epoch 4 Batch 50 Loss 4.9116 Learning rate 0.0006\n",
            "Epoch 4 Batch 100 Loss 4.8777 Learning rate 0.0006\n",
            "Epoch 4 Batch 150 Loss 4.7768 Learning rate 0.0006\n",
            "Epoch 4 Batch 200 Loss 4.5818 Learning rate 0.0006\n",
            "Epoch 4 Batch 250 Loss 4.3960 Learning rate 0.0006\n",
            "Epoch 4 Batch 0 Valid Loss 10.4902\n",
            "Saving checkpoint for epoch 4 at /tmp/ckpt/train/transformer/ckpt-4\n",
            "Epoch 4 Loss 4.3886 Valid Loss 10.4529\n",
            "Time taken for 1 epoch: 2827.68 secs\n",
            "\n",
            "Epoch 5 Batch 0 Loss 3.6355 Learning rate 0.0006\n",
            "Epoch 5 Batch 50 Loss 3.6139 Learning rate 0.0006\n",
            "Epoch 5 Batch 100 Loss 3.6012 Learning rate 0.0006\n",
            "Epoch 5 Batch 150 Loss 3.5624 Learning rate 0.0006\n",
            "Epoch 5 Batch 200 Loss 3.5213 Learning rate 0.0007\n",
            "Epoch 5 Batch 250 Loss 3.5221 Learning rate 0.0007\n",
            "Epoch 5 Batch 0 Valid Loss 11.0577\n",
            "Epoch 5 Loss 3.5185 Valid Loss 11.0191\n",
            "Time taken for 1 epoch: 2930.72 secs\n",
            "\n",
            "Epoch 6 Batch 0 Loss 3.1644 Learning rate 0.0007\n",
            "Epoch 6 Batch 50 Loss 3.3638 Learning rate 0.0007\n",
            "Epoch 6 Batch 100 Loss 3.2871 Learning rate 0.0007\n",
            "Epoch 6 Batch 150 Loss 3.2559 Learning rate 0.0007\n",
            "Epoch 6 Batch 200 Loss 3.2746 Learning rate 0.0007\n",
            "Epoch 6 Batch 250 Loss 3.2830 Learning rate 0.0007\n",
            "Epoch 6 Batch 0 Valid Loss 11.4394\n",
            "Saving checkpoint for epoch 6 at /tmp/ckpt/train/transformer/ckpt-5\n",
            "Epoch 6 Loss 3.2820 Valid Loss 11.4070\n",
            "Time taken for 1 epoch: 2964.43 secs\n",
            "\n",
            "Epoch 7 Batch 0 Loss 3.5868 Learning rate 0.0007\n",
            "Epoch 7 Batch 50 Loss 3.2564 Learning rate 0.0007\n",
            "Epoch 7 Batch 100 Loss 3.2151 Learning rate 0.0007\n",
            "Epoch 7 Batch 150 Loss 3.1919 Learning rate 0.0007\n",
            "Epoch 7 Batch 200 Loss 3.1943 Learning rate 0.0007\n",
            "Epoch 7 Batch 250 Loss 3.1888 Learning rate 0.0007\n",
            "Epoch 7 Batch 0 Valid Loss 11.1581\n",
            "Epoch 7 Loss 3.1877 Valid Loss 11.1225\n",
            "Time taken for 1 epoch: 3185.27 secs\n",
            "\n",
            "Epoch 8 Batch 0 Loss 3.3493 Learning rate 0.0007\n",
            "Epoch 8 Batch 50 Loss 3.1394 Learning rate 0.0007\n",
            "Epoch 8 Batch 100 Loss 3.1218 Learning rate 0.0007\n",
            "Epoch 8 Batch 150 Loss 3.0893 Learning rate 0.0007\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sU9eDPVuBaUM"
      },
      "source": [
        "print(optimizer._decayed_lr('float32').numpy())"
      ],
      "id": "sU9eDPVuBaUM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lfZdYMsC-G3"
      },
      "source": [
        "#reset Keras Session\n",
        "def reset_keras():\n",
        "    sess = tf.compat.v1.keras.backend.get_session()\n",
        "    tf.compat.v1.keras.backend.clear_session()\n",
        "    sess.close()\n",
        "    sess = tf.compat.v1.keras.backend.get_session()\n",
        "\n",
        "    # use the same config as you used to create the session\n",
        "    config = tf.compat.v1.ConfigProto()\n",
        "    config.gpu_options.per_process_gpu_memory_fraction = 1\n",
        "    config.gpu_options.visible_device_list = \"0\"\n",
        "    tf.compat.v1.keras.backend.set_session(tf.compat.v1.Session(config=config))\n",
        "    gc.collect()\n",
        "\n",
        "reset_keras()"
      ],
      "id": "3lfZdYMsC-G3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZR_KtDCXk-c"
      },
      "source": [
        "print('Train MAE:', mae(targets.flatten(), np.median(np.vstack(train_preds),axis=0)))\n",
        "PRESSURE_STEP = 0.07030214545120961\n",
        "PRESSURE_MIN = -1.8957442945646408\n",
        "PRESSURE_MAX = 64.82099173863948\n",
        "# ENSEMBLE FOLDS WITH MEAN\n",
        "submission[\"pressure\"] = sum(test_preds)/len(test_preds)\n",
        "submission.to_csv('submission_mean.csv', index=False)\n",
        "# ENSEMBLE FOLDS WITH MEDIAN\n",
        "submission[\"pressure\"] = np.median(np.vstack(test_preds),axis=0)\n",
        "submission.to_csv('submission_median.csv', index=False)\n",
        "# ENSEMBLE FOLDS WITH MEDIAN AND ROUND PREDICTIONS\n",
        "submission[\"pressure\"] =\\\n",
        "    np.round( (submission.pressure - PRESSURE_MIN)/PRESSURE_STEP ) * PRESSURE_STEP + PRESSURE_MIN\n",
        "submission.pressure = np.clip(submission.pressure, PRESSURE_MIN, PRESSURE_MAX)\n",
        "submission.to_csv('submission_median_round.csv', index=False)"
      ],
      "id": "0ZR_KtDCXk-c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5z9iaJ--P8P"
      },
      "source": [
        "if IN_COLAB:\n",
        "  !zip submission.zip submission_median_round.csv\n",
        "  with open(model_folder + 'submission.zip', 'wb') as f:\n",
        "    f.write(open('submission.zip', 'rb').read())\n",
        "  # !zip cnn_models.zip *.hdf5\n",
        "  # with open(model_folder + 'cnn_models.zip', 'wb') as f:\n",
        "  #   f.write(open('cnn_models.zip', 'rb').read())"
      ],
      "id": "K5z9iaJ--P8P"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezlVLAWUoX1C"
      },
      "source": [
        "!cp /tmp/ckpt/train/transformer/* /gdrive/MyDrive/ventilator-pressure-prediction/transformer/checkpoints/train"
      ],
      "id": "ezlVLAWUoX1C"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0k2rVJujHEDo"
      },
      "source": [
        ""
      ],
      "id": "0k2rVJujHEDo",
      "execution_count": null,
      "outputs": []
    }
  ]
}