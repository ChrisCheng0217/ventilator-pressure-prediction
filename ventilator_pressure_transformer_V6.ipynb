{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "ventilator-pressure-transformer_V6.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 5851.539198,
      "end_time": "2021-10-05T21:57:27.162714",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2021-10-05T20:19:55.623516",
      "version": "2.3.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bobbercheng/ventilator-pressure-prediction/blob/master/ventilator_pressure_transformer_V6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRRDmcGpsh4u"
      },
      "source": [
        "V1: Add PositionalEncoding\n",
        "\n",
        "V2: Adjust Transformer parameters\n",
        "\n",
        "V3: Use Transformer class instead of Tensorflow version\n",
        "\n",
        "V4: Without EDA and only use 400 features that is very similar to Keras time serial transformer sample if features and time serial are flatten. TPU v2 supports 16 heads, 512 key_dim, 512 batch size. But converge is not good\n",
        "\n",
        "V4: Max TPU V2 config, 128 heads, 512 key_dim, 512 batch size. Try build_complex_transfer_model. LP: 0.217, CX: 0.21, Train: 0.13. It looks complex transfer model can remember train data.\n",
        "\n"
      ],
      "id": "TRRDmcGpsh4u"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPJR-B_AhTBC"
      },
      "source": [
        "# Update pandas version for Colab TPU\n",
        "#!pip install pandas==1.3.2"
      ],
      "id": "EPJR-B_AhTBC",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJaAgiAOkUEt"
      },
      "source": [
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False"
      ],
      "id": "mJaAgiAOkUEt",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oH5W_yWm-P8P",
        "outputId": "5c56f48a-d4a2-4cf6-ed4d-23c5ccf51660"
      },
      "source": [
        "if IN_COLAB:\n",
        "  from google.colab import drive\n",
        "  drive.mount(\"/gdrive\", force_remount=True)"
      ],
      "id": "oH5W_yWm-P8P",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhjzjzp5etAG",
        "outputId": "8f50844f-b329-47ce-84c1-d0c1b4315253"
      },
      "source": [
        "if IN_COLAB:\n",
        "  !pip install kaggle\n",
        "  !mkdir /root/.kaggle\n",
        "  !cp /gdrive/MyDrive/ventilator-pressure-prediction/kaggle.json /root/.kaggle\n",
        "  !kaggle competitions download -c ventilator-pressure-prediction\n",
        "  !mkdir -p /kaggle/input/ventilator-pressure-prediction\n",
        "  !unzip '*.zip' -d /kaggle/input/ventilator-pressure-prediction\n",
        "  !ls /kaggle/input/ventilator-pressure-prediction"
      ],
      "id": "mhjzjzp5etAG",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.5.30)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.62.3)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n",
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4)\n",
            "Downloading train.csv.zip to /content\n",
            " 98% 137M/139M [00:01<00:00, 141MB/s]\n",
            "100% 139M/139M [00:01<00:00, 137MB/s]\n",
            "Downloading test.csv.zip to /content\n",
            " 95% 72.0M/75.4M [00:00<00:00, 102MB/s] \n",
            "100% 75.4M/75.4M [00:00<00:00, 119MB/s]\n",
            "Downloading sample_submission.csv.zip to /content\n",
            "  0% 0.00/8.50M [00:00<?, ?B/s]\n",
            "100% 8.50M/8.50M [00:00<00:00, 78.2MB/s]\n",
            "Archive:  test.csv.zip\n",
            "  inflating: /kaggle/input/ventilator-pressure-prediction/test.csv  \n",
            "\n",
            "Archive:  sample_submission.csv.zip\n",
            "  inflating: /kaggle/input/ventilator-pressure-prediction/sample_submission.csv  \n",
            "\n",
            "Archive:  train.csv.zip\n",
            "  inflating: /kaggle/input/ventilator-pressure-prediction/train.csv  \n",
            "\n",
            "3 archives were successfully processed.\n",
            "sample_submission.csv  test.csv  train.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a16844ae"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# import optuna\n",
        "import matplotlib.pyplot as plt\n",
        "import gc\n",
        "import time\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler, ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error as mae\n",
        "from sklearn.preprocessing import RobustScaler, normalize\n",
        "from sklearn.model_selection import train_test_split, GroupKFold, KFold\n",
        "\n",
        "from IPython.display import display\n",
        "\n",
        "model_folder = '/gdrive/MyDrive/ventilator-pressure-prediction/transformer/'\n",
        "\n"
      ],
      "id": "a16844ae",
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEHQxSneTcVX"
      },
      "source": [
        "import os\n",
        "import random\n",
        "# Deep Learning\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "# Metrics\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Random Seed Initialize\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "def seed_everything(seed=RANDOM_SEED):\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    \n",
        "seed_everything()"
      ],
      "id": "LEHQxSneTcVX",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "564144b2"
      },
      "source": [
        "DEBUG = True\n",
        "\n",
        "train = pd.read_csv('/kaggle/input/ventilator-pressure-prediction/train.csv')\n",
        "test = pd.read_csv('/kaggle/input/ventilator-pressure-prediction/test.csv')\n",
        "submission = pd.read_csv('/kaggle/input/ventilator-pressure-prediction/sample_submission.csv')\n",
        "\n",
        "if DEBUG:\n",
        "    train = train[:80*1000]"
      ],
      "id": "564144b2",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08fd8094",
        "outputId": "5e731bf3-a7fe-4d3b-e14f-3c01e9d76cd9"
      },
      "source": [
        "def add_features(df):\n",
        "    df['cross']= df['u_in'] * df['u_out']\n",
        "    df['cross2']= df['time_step'] * df['u_out']\n",
        "    df['area'] = df['time_step'] * df['u_in']\n",
        "    df['area'] = df.groupby('breath_id')['area'].cumsum()\n",
        "    df['time_step_cumsum'] = df.groupby(['breath_id'])['time_step'].cumsum()\n",
        "    df['u_in_cumsum'] = (df['u_in']).groupby(df['breath_id']).cumsum()\n",
        "    print(\"Step-1...Completed\")\n",
        "    \n",
        "    df['u_in_lag1'] = df.groupby('breath_id')['u_in'].shift(1)\n",
        "    df['u_out_lag1'] = df.groupby('breath_id')['u_out'].shift(1)\n",
        "    df['u_in_lag_back1'] = df.groupby('breath_id')['u_in'].shift(-1)\n",
        "    df['u_out_lag_back1'] = df.groupby('breath_id')['u_out'].shift(-1)\n",
        "    df['u_in_lag2'] = df.groupby('breath_id')['u_in'].shift(2)\n",
        "    df['u_out_lag2'] = df.groupby('breath_id')['u_out'].shift(2)\n",
        "    df['u_in_lag_back2'] = df.groupby('breath_id')['u_in'].shift(-2)\n",
        "    df['u_out_lag_back2'] = df.groupby('breath_id')['u_out'].shift(-2)\n",
        "    df['u_in_lag3'] = df.groupby('breath_id')['u_in'].shift(3)\n",
        "    df['u_out_lag3'] = df.groupby('breath_id')['u_out'].shift(3)\n",
        "    df['u_in_lag_back3'] = df.groupby('breath_id')['u_in'].shift(-3)\n",
        "    df['u_out_lag_back3'] = df.groupby('breath_id')['u_out'].shift(-3)\n",
        "    df['u_in_lag4'] = df.groupby('breath_id')['u_in'].shift(4)\n",
        "    df['u_out_lag4'] = df.groupby('breath_id')['u_out'].shift(4)\n",
        "    df['u_in_lag_back4'] = df.groupby('breath_id')['u_in'].shift(-4)\n",
        "    df['u_out_lag_back4'] = df.groupby('breath_id')['u_out'].shift(-4)\n",
        "    df = df.fillna(0)\n",
        "    print(\"Step-2...Completed\")\n",
        "    \n",
        "    df['breath_id__u_in__max'] = df.groupby(['breath_id'])['u_in'].transform('max')\n",
        "    df['breath_id__u_out__max'] = df.groupby(['breath_id'])['u_out'].transform('max')\n",
        "    df['breath_id__u_in__diffmax'] = df.groupby(['breath_id'])['u_in'].transform('max') - df['u_in']\n",
        "    df['breath_id__u_in__diffmean'] = df.groupby(['breath_id'])['u_in'].transform('mean') - df['u_in']\n",
        "    df['breath_id__u_in__diffmax'] = df.groupby(['breath_id'])['u_in'].transform('max') - df['u_in']\n",
        "    df['breath_id__u_in__diffmean'] = df.groupby(['breath_id'])['u_in'].transform('mean') - df['u_in']\n",
        "    print(\"Step-3...Completed\")\n",
        "    \n",
        "    df['u_in_diff1'] = df['u_in'] - df['u_in_lag1']\n",
        "    df['u_out_diff1'] = df['u_out'] - df['u_out_lag1']\n",
        "    df['u_in_diff2'] = df['u_in'] - df['u_in_lag2']\n",
        "    df['u_out_diff2'] = df['u_out'] - df['u_out_lag2']\n",
        "    df['u_in_diff3'] = df['u_in'] - df['u_in_lag3']\n",
        "    df['u_out_diff3'] = df['u_out'] - df['u_out_lag3']\n",
        "    df['u_in_diff4'] = df['u_in'] - df['u_in_lag4']\n",
        "    df['u_out_diff4'] = df['u_out'] - df['u_out_lag4']\n",
        "    print(\"Step-4...Completed\")\n",
        "    \n",
        "    df['one'] = 1\n",
        "    df['count'] = (df['one']).groupby(df['breath_id']).cumsum()\n",
        "    df['u_in_cummean'] =df['u_in_cumsum'] /df['count']\n",
        "    \n",
        "    df['breath_id_lag']=df['breath_id'].shift(1).fillna(0)\n",
        "    df['breath_id_lag2']=df['breath_id'].shift(2).fillna(0)\n",
        "    df['breath_id_lagsame']=np.select([df['breath_id_lag']==df['breath_id']],[1],0)\n",
        "    df['breath_id_lag2same']=np.select([df['breath_id_lag2']==df['breath_id']],[1],0)\n",
        "    df['breath_id__u_in_lag'] = df['u_in'].shift(1).fillna(0)\n",
        "    df['breath_id__u_in_lag'] = df['breath_id__u_in_lag'] * df['breath_id_lagsame']\n",
        "    df['breath_id__u_in_lag2'] = df['u_in'].shift(2).fillna(0)\n",
        "    df['breath_id__u_in_lag2'] = df['breath_id__u_in_lag2'] * df['breath_id_lag2same']\n",
        "    print(\"Step-5...Completed\")\n",
        "    \n",
        "    df['time_step_diff'] = df.groupby('breath_id')['time_step'].diff().fillna(0)\n",
        "    df['ewm_u_in_mean'] = (df\\\n",
        "                           .groupby('breath_id')['u_in']\\\n",
        "                           .ewm(halflife=9)\\\n",
        "                           .mean()\\\n",
        "                           .reset_index(level=0,drop=True))\n",
        "    df[[\"15_in_sum\",\"15_in_min\",\"15_in_max\",\"15_in_mean\"]] = (df\\\n",
        "                                                              .groupby('breath_id')['u_in']\\\n",
        "                                                              .rolling(window=15,min_periods=1)\\\n",
        "                                                              .agg({\"15_in_sum\":\"sum\",\n",
        "                                                                    \"15_in_min\":\"min\",\n",
        "                                                                    \"15_in_max\":\"max\",\n",
        "                                                                    \"15_in_mean\":\"mean\"\n",
        "                                                                    #\"15_in_std\":\"std\"\n",
        "                                                               })\\\n",
        "                                                               .reset_index(level=0,drop=True))\n",
        "    print(\"Step-6...Completed\")\n",
        "    \n",
        "    #df['u_in_diff_1_2'] = df['u_in_lag1'] - df['u_in_lag2']\n",
        "    #df['u_out_diff_1_2'] = df['u_out_lag1'] - df['u_out_lag2']\n",
        "    #df['u_in_lagback_diff_1_2'] = df['u_in_lag_back1'] - df['u_in_lag_back2']\n",
        "    #df['u_out_lagback_diff_1_2'] = df['u_out_lag_back1'] - df['u_out_lag_back2']\n",
        "    \n",
        "    df['u_in_lagback_diff1'] = df['u_in'] - df['u_in_lag_back1']\n",
        "    df['u_out_lagback_diff1'] = df['u_out'] - df['u_out_lag_back1']\n",
        "    df['u_in_lagback_diff2'] = df['u_in'] - df['u_in_lag_back2']\n",
        "    df['u_out_lagback_diff2'] = df['u_out'] - df['u_out_lag_back2']\n",
        "    print(\"Step-7...Completed\")\n",
        "    \n",
        "    df['R'] = df['R'].astype(str)\n",
        "    df['C'] = df['C'].astype(str)\n",
        "    df['R__C'] = df[\"R\"].astype(str) + '__' + df[\"C\"].astype(str)\n",
        "    df = pd.get_dummies(df)\n",
        "    print(\"Step-8...Completed\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "print(\"Train data...\\n\")\n",
        "train = add_features(train)\n",
        "\n",
        "print(\"\\nTest data...\\n\")\n",
        "test = add_features(test)\n",
        "\n"
      ],
      "id": "08fd8094",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train data...\n",
            "\n",
            "Step-1...Completed\n",
            "Step-2...Completed\n",
            "Step-3...Completed\n",
            "Step-4...Completed\n",
            "Step-5...Completed\n",
            "Step-6...Completed\n",
            "Step-7...Completed\n",
            "Step-8...Completed\n",
            "\n",
            "Test data...\n",
            "\n",
            "Step-1...Completed\n",
            "Step-2...Completed\n",
            "Step-3...Completed\n",
            "Step-4...Completed\n",
            "Step-5...Completed\n",
            "Step-6...Completed\n",
            "Step-7...Completed\n",
            "Step-8...Completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "431a7ee7",
        "outputId": "91463375-6122-4c47-b215-9b87e7d5830c"
      },
      "source": [
        "targets = train[['pressure']].to_numpy().reshape(-1, 80)\n",
        "\n",
        "train.drop(['pressure','id', 'breath_id'], axis=1, inplace=True)\n",
        "\n",
        "test = test.drop(['id', 'breath_id'], axis=1)\n",
        "\n",
        "train.head(), train.shape"
      ],
      "id": "431a7ee7",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(   time_step       u_in  u_out  ...  R__C_5__10  R__C_5__20  R__C_5__50\n",
              " 0   0.000000   0.083334      0  ...           0           0           0\n",
              " 1   0.033652  18.383041      0  ...           0           0           0\n",
              " 2   0.067514  22.509278      0  ...           0           0           0\n",
              " 3   0.101542  22.808822      0  ...           0           0           0\n",
              " 4   0.135756  25.355850      0  ...           0           0           0\n",
              " \n",
              " [5 rows x 70 columns], (80000, 70))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEAbCWmOoLH6"
      },
      "source": [
        "train.drop(['one','count',\n",
        "            'breath_id_lag','breath_id_lag2','breath_id_lagsame',\n",
        "            'breath_id_lag2same'], axis=1, inplace=True)\n",
        "\n",
        "test = test.drop(['one','count','breath_id_lag',\n",
        "                  'breath_id_lag2','breath_id_lagsame',\n",
        "                  'breath_id_lag2same'], axis=1)"
      ],
      "id": "IEAbCWmOoLH6",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d24ef65c"
      },
      "source": [
        "RS = RobustScaler()\n",
        "train = RS.fit_transform(train)\n",
        "test = RS.transform(test)"
      ],
      "id": "d24ef65c",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6f1a3e19"
      },
      "source": [
        "train = train.reshape(-1, 80, train.shape[-1])\n",
        "test = test.reshape(-1, 80, train.shape[-1])"
      ],
      "id": "6f1a3e19",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4lOIi9HdExc"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras"
      ],
      "id": "m4lOIi9HdExc",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqA2324UJKb_"
      },
      "source": [
        "# refer to https://www.tensorflow.org/text/tutorials/transformer#encoder\n",
        "def get_angles(pos, i, d_model):\n",
        "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "  return pos * angle_rates\n",
        "\n",
        "def positional_encoding(position, d_model):\n",
        "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model)\n",
        "\n",
        "  # apply sin to even indices in the array; 2i\n",
        "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "  # apply cos to odd indices in the array; 2i+1\n",
        "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "  pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "  \"\"\"Calculate the attention weights.\n",
        "  q, k, v must have matching leading dimensions.\n",
        "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "  The mask has different shapes depending on its type(padding or look ahead)\n",
        "  but it must be broadcastable for addition.\n",
        "\n",
        "  Args:\n",
        "    q: query shape == (..., seq_len_q, depth)\n",
        "    k: key shape == (..., seq_len_k, depth)\n",
        "    v: value shape == (..., seq_len_v, depth_v)\n",
        "    mask: Float tensor with shape broadcastable\n",
        "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "\n",
        "  Returns:\n",
        "    output, attention_weights\n",
        "  \"\"\"\n",
        "\n",
        "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "  # scale matmul_qk\n",
        "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "  # add the mask to the scaled tensor.\n",
        "  if mask is not None:\n",
        "    scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "  # add up to 1.\n",
        "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "\n",
        "  return output, attention_weights\n",
        "\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, **kwargs):\n",
        "    super(MultiHeadAttention, self).__init__(**kwargs)\n",
        "    self.num_heads = num_heads\n",
        "    self.d_model = d_model\n",
        "    assert self.d_model % self.num_heads == 0\n",
        "    self.depth = d_model // self.num_heads\n",
        "    self.build_components()\n",
        "\n",
        "  def build_components(self):\n",
        "    self.wq = tf.keras.layers.Dense(self.d_model)\n",
        "    self.wk = tf.keras.layers.Dense(self.d_model)\n",
        "    self.wv = tf.keras.layers.Dense(self.d_model)\n",
        "\n",
        "    self.dense = tf.keras.layers.Dense(self.d_model)\n",
        "\n",
        "  def split_heads(self, x, batch_size):\n",
        "    \"\"\"Split the last dimension into (num_heads, depth).\n",
        "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "    \"\"\"\n",
        "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "  def call(self, v, k, q, mask):\n",
        "    batch_size = tf.shape(q)[0]\n",
        "\n",
        "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "\n",
        "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "        q, k, v, mask)\n",
        "\n",
        "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
        "\n",
        "    concat_attention = tf.reshape(scaled_attention,\n",
        "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "    return output, attention_weights\n",
        "\n",
        "  # def get_config(self):\n",
        "  #   config = super(MultiHeadAttention, self).get_config()\n",
        "  #   config.update({\"num_heads\": self.num_heads,\n",
        "  #           \"d_model\": self.d_model})\n",
        "  #   return config\n",
        "\n",
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "  # Feed forward can be Den lay or CNN as long as it keep final dimension\n",
        "  #x = layers.Conv1D(filters=ff_dims, kernel_size=1, activation=\"selu\")(x)\n",
        "  #x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
        "  return tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
        "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
        "  ])\n",
        "\n",
        "def create_look_ahead_mask(size):\n",
        "  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "  return mask  # (seq_len, seq_len)\n",
        "\n",
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, dff, rate=0.1, **kwargs):\n",
        "    super(EncoderLayer, self).__init__(**kwargs)\n",
        "    self.d_model = d_model\n",
        "    self.num_heads = num_heads\n",
        "    self.dff = dff\n",
        "    self.rate = rate\n",
        "    self.build_components()\n",
        "\n",
        "  def build_components(self):\n",
        "    self.mha = MultiHeadAttention(self.d_model, self.num_heads)\n",
        "    self.ffn = point_wise_feed_forward_network(self.d_model, self.dff)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(self.rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(self.rate)\n",
        "\n",
        "  def call(self, x, training, mask):\n",
        "\n",
        "    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
        "    attn_output = self.dropout1(attn_output, training=training)\n",
        "    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "    ffn_output = self.dropout2(ffn_output, training=training)\n",
        "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    return out2\n",
        "\n",
        "  # def get_config(self):\n",
        "  #   config = super(EncoderLayer, self).get_config()\n",
        "  #   config.update({\"num_heads\": self.num_heads,\n",
        "  #           \"d_model\": self.d_model,\n",
        "  #           'dff': self.dff,\n",
        "  #           'rate': self.rate\n",
        "  #           })\n",
        "  #   return config\n",
        "\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, \n",
        "               #input_vocab_size,\n",
        "               maximum_position_encoding, rate=0.1, **kwargs):\n",
        "    super(Encoder, self).__init__(**kwargs)\n",
        "\n",
        "    self.num_layers = num_layers\n",
        "    self.d_model = d_model\n",
        "    self.num_heads = num_heads\n",
        "    self.dff = dff\n",
        "    self.rate = rate\n",
        "    self.maximum_position_encoding = maximum_position_encoding\n",
        "    self.build_components()\n",
        "\n",
        "  def build_components(self):\n",
        "    # self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "    self.embedding = tf.keras.layers.Dense(self.d_model)\n",
        "    self.pos_encoding = positional_encoding(self.maximum_position_encoding,\n",
        "                                            self.d_model)\n",
        "\n",
        "    self.enc_layers = [EncoderLayer(self.d_model, self.num_heads, self.dff, self.rate)\n",
        "                       for _ in range(self.num_layers)]\n",
        "\n",
        "    self.dropout = tf.keras.layers.Dropout(self.rate)\n",
        "\n",
        "  def call(self, x, training, mask):\n",
        "\n",
        "    seq_len = tf.shape(x)[1]\n",
        "\n",
        "    # adding embedding and position encoding.\n",
        "    x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x, training, mask)\n",
        "\n",
        "    return x  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "  # def get_config(self):\n",
        "  #   config = super(Encoder, self).get_config()\n",
        "  #   config.update({'num_layers': self.num_layers,\n",
        "  #           \"num_heads\": self.num_heads,\n",
        "  #           \"d_model\": self.d_model,\n",
        "  #           'dff': self.dff,\n",
        "  #           'rate': self.rate,\n",
        "  #           'maximum_position_encoding': self.maximum_position_encoding\n",
        "  #           })\n",
        "  #   return config\n",
        "\n",
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, dff, rate=0.1, **kwargs):\n",
        "    super(DecoderLayer, self).__init__(**kwargs)\n",
        "    self.d_model = d_model\n",
        "    self.num_heads = num_heads\n",
        "    self.dff = dff\n",
        "    self.rate = rate\n",
        "    self.build_components()\n",
        "\n",
        "  def build_components(self):\n",
        "    self.mha1 = MultiHeadAttention(self.d_model, self.num_heads)\n",
        "    self.mha2 = MultiHeadAttention(self.d_model, self.num_heads)\n",
        "\n",
        "    self.ffn = point_wise_feed_forward_network(self.d_model, self.dff)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(self.rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(self.rate)\n",
        "    self.dropout3 = tf.keras.layers.Dropout(self.rate)\n",
        "\n",
        "  def call(self, x, enc_output, training,\n",
        "           look_ahead_mask, padding_mask):\n",
        "    # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
        "    attn1 = self.dropout1(attn1, training=training)\n",
        "    out1 = self.layernorm1(attn1 + x)\n",
        "\n",
        "    attn2, attn_weights_block2 = self.mha2(\n",
        "        enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
        "    attn2 = self.dropout2(attn2, training=training)\n",
        "    out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "    ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
        "    ffn_output = self.dropout3(ffn_output, training=training)\n",
        "    out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "    return out3, attn_weights_block1, attn_weights_block2\n",
        "\n",
        "  # def get_config(self):\n",
        "  #   config = super(DecoderLayer, self).get_config()\n",
        "  #   config.update({\"num_heads\": self.num_heads,\n",
        "  #           \"d_model\": self.d_model,\n",
        "  #           'dff': self.dff,\n",
        "  #           'rate': self.rate\n",
        "  #           })\n",
        "  #   return config\n",
        "\n",
        "\n",
        "\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, \n",
        "              #  target_vocab_size,\n",
        "               maximum_position_encoding, rate=0.1, **kwargs):\n",
        "    super(Decoder, self).__init__(**kwargs)\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "    self.num_heads = num_heads\n",
        "    self.dff = dff\n",
        "    self.rate = rate\n",
        "    self.maximum_position_encoding = maximum_position_encoding\n",
        "    self.build_components()\n",
        "\n",
        "  def build_components(self):\n",
        "    # self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "    self.embedding = tf.keras.layers.Dense(self.d_model)\n",
        "    self.pos_encoding = positional_encoding(self.maximum_position_encoding, self.d_model)\n",
        "\n",
        "    self.dec_layers = [DecoderLayer(self.d_model, self.num_heads, self.dff, self.rate)\n",
        "                       for _ in range(self.num_layers)]\n",
        "    self.dropout = tf.keras.layers.Dropout(self.rate)\n",
        "\n",
        "  def call(self, x, enc_output, training,\n",
        "           look_ahead_mask, padding_mask):\n",
        "\n",
        "    seq_len = tf.shape(x)[1]\n",
        "    attention_weights = {}\n",
        "\n",
        "    x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
        "                                             look_ahead_mask, padding_mask)\n",
        "\n",
        "      attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
        "      attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
        "\n",
        "    # x.shape == (batch_size, target_seq_len, d_model)\n",
        "    return x, attention_weights\n",
        "\n",
        "  # def get_config(self):\n",
        "  #   config = super(Decoder, self).get_config()\n",
        "  #   config.update({'num_layers': self.num_layers,\n",
        "  #           \"num_heads\": self.num_heads,\n",
        "  #           \"d_model\": self.d_model,\n",
        "  #           'dff': self.dff,\n",
        "  #           'rate': self.rate,\n",
        "  #           'maximum_position_encoding': self.maximum_position_encoding\n",
        "  #           })\n",
        "  #   return config\n",
        "\n",
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, \n",
        "              #  input_vocab_size, target_vocab_size,\n",
        "               pe_input, pe_target, rate=0.1, target_dim=1, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "    self.num_heads = num_heads\n",
        "    self.dff = dff\n",
        "    self.rate = rate\n",
        "    self.pe_input = pe_input\n",
        "    self.pe_target = pe_target\n",
        "    self.target_dim = target_dim\n",
        "    self.build_components()\n",
        "\n",
        "  def build_components(self):\n",
        "    self.encoder = Encoder(self.num_layers, self.d_model, self.num_heads, self.dff,\n",
        "                            #  input_vocab_size, \n",
        "                           self.pe_input, self.rate)\n",
        "\n",
        "    self.decoder = Decoder(self.num_layers, self.d_model, self.num_heads, self.dff,\n",
        "                          #  target_vocab_size, \n",
        "                           self.pe_target, self.rate)\n",
        "\n",
        "    self.final_layer = tf.keras.layers.Dense(self.target_dim)\n",
        "\n",
        "  def call(self, inputs, training):\n",
        "    # Keras models prefer if you pass all your inputs in the first argument\n",
        "    inp, tar = inputs\n",
        "\n",
        "    enc_padding_mask, look_ahead_mask, dec_padding_mask = self.create_masks(inp, tar)\n",
        "\n",
        "    enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
        "\n",
        "    # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
        "    dec_output, attention_weights = self.decoder(\n",
        "        tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "\n",
        "    final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
        "\n",
        "    return final_output, attention_weights\n",
        "\n",
        "  def create_masks(self, inp, tar):\n",
        "    # Encoder padding mask\n",
        "    # enc_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "    # Used in the 2nd attention block in the decoder.\n",
        "    # This padding mask is used to mask the encoder outputs.\n",
        "    # dec_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "    # Used in the 1st attention block in the decoder.\n",
        "    # It is used to pad and mask future tokens in the input received by\n",
        "    # the decoder.\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "    # dec_target_padding_mask = create_padding_mask(tar)\n",
        "    # look_ahead_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "\n",
        "    # return enc_padding_mask, look_ahead_mask, dec_padding_mask\n",
        "    return None, look_ahead_mask, None\n",
        "\n",
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=4000, **kwargs):\n",
        "    super(CustomSchedule, self).__init__(**kwargs)\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "\n",
        "  def __call__(self, step):\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "id": "dqA2324UJKb_",
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJLs2RP98Zir"
      },
      "source": [
        "n, d = 80, 64\n",
        "pos_encoding = positional_encoding(n, d)\n",
        "print(pos_encoding.shape)\n",
        "pos_encoding = pos_encoding[0]\n",
        "\n",
        "# Juggle the dimensions for the plot\n",
        "pos_encoding = tf.reshape(pos_encoding, (n, d//2, 2))\n",
        "pos_encoding = tf.transpose(pos_encoding, (2, 1, 0))\n",
        "pos_encoding = tf.reshape(pos_encoding, (d, n))\n",
        "\n",
        "plt.pcolormesh(pos_encoding, cmap='RdBu')\n",
        "plt.ylabel('Depth')\n",
        "plt.xlabel('Position')\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "id": "AJLs2RP98Zir"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8U60IUEd6kal"
      },
      "source": [
        "def loss_function(real, pred):\n",
        "  loss_ = loss_object(real, pred)\n",
        "  return loss_\n",
        "\n",
        "def accuracy_function(real, pred):\n",
        "  accuracies = tf.equal(real, pred)\n",
        "  accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
        "  # print(accuracies)\n",
        "  return tf.reduce_mean(accuracies)"
      ],
      "id": "8U60IUEd6kal",
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7wCaptl1KWt"
      },
      "source": [
        "temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
        "y = tf.random.uniform((1, 60, 512))  # (batch_size, encoder_sequence, d_model)\n",
        "out, attn = temp_mha(y, k=y, q=y, mask=None)\n",
        "out.shape, attn.shape"
      ],
      "id": "v7wCaptl1KWt"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSeBs1ni4Q5d"
      },
      "source": [
        "sample_ffn = point_wise_feed_forward_network(512, 2048)\n",
        "sample_ffn(tf.random.uniform((64, 50, 512))).shape"
      ],
      "id": "aSeBs1ni4Q5d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5cQ-rK586oj"
      },
      "source": [
        "sample_encoder_layer = EncoderLayer(512, 8, 2048)\n",
        "\n",
        "sample_encoder_layer_output = sample_encoder_layer(\n",
        "    tf.random.uniform((64, 43, 512)), False, None)\n",
        "\n",
        "sample_encoder_layer_output.shape  # (batch_size, input_seq_len, d_model)"
      ],
      "id": "r5cQ-rK586oj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQRCtoP0-TOl"
      },
      "source": [
        "sample_decoder_layer = DecoderLayer(512, 8, 2048)\n",
        "config = sample_decoder_layer.get_config()\n",
        "print(config)\n",
        "new_layer = DecoderLayer.from_config(config)\n",
        "\n",
        "sample_decoder_layer_output, _, _ = new_layer(\n",
        "    tf.random.uniform((64, 50, 512)), sample_encoder_layer_output,\n",
        "    False, None, None)\n",
        "\n",
        "sample_decoder_layer_output.shape  # (batch_size, target_seq_len, d_model)\n"
      ],
      "id": "dQRCtoP0-TOl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYb6A00LMSJY"
      },
      "source": [
        "sample_encoder = Encoder(num_layers=2, d_model=512, num_heads=8,\n",
        "                         dff=2048, \n",
        "                        #  input_vocab_size=8500,\n",
        "                         maximum_position_encoding=10000)\n",
        "temp_input = tf.random.uniform((64, 62, 512), dtype=tf.int64, minval=0, maxval=200)\n",
        "\n",
        "sample_encoder_output = sample_encoder(temp_input, training=False, mask=None)\n",
        "\n",
        "print(sample_encoder_output.shape)  # (batch_size, input_seq_len, d_model)"
      ],
      "id": "YYb6A00LMSJY"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzrVsWnHUIpH"
      },
      "source": [
        "sample_decoder = Decoder(num_layers=2, d_model=512, num_heads=8,\n",
        "                         dff=2048, \n",
        "                        #  target_vocab_size=8000,\n",
        "                         maximum_position_encoding=5000)\n",
        "temp_input = tf.random.uniform((64, 26, 512), dtype=tf.int64, minval=0, maxval=200)\n",
        "\n",
        "output, attn = sample_decoder(temp_input,\n",
        "                              enc_output=sample_encoder_output,\n",
        "                              training=False,\n",
        "                              look_ahead_mask=None,\n",
        "                              padding_mask=None)\n",
        "\n",
        "output.shape, attn['decoder_layer2_block2'].shape"
      ],
      "id": "OzrVsWnHUIpH"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4d62Did6ZOIo"
      },
      "source": [
        "\n",
        "d_model = 512\n",
        "temp_learning_rate_schedule = CustomSchedule(d_model)\n",
        "\n",
        "plt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))\n",
        "plt.ylabel(\"Learning Rate\")\n",
        "plt.xlabel(\"Train Step\")"
      ],
      "id": "4d62Did6ZOIo"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEyIJqVcYiKv",
        "outputId": "aab637d8-1cba-474e-aca7-49704c00fb85"
      },
      "source": [
        "sample_transformer = Transformer(\n",
        "    num_layers=2, d_model=512, num_heads=8, dff=2048,\n",
        "    # input_vocab_size=8500, target_vocab_size=8000,\n",
        "    pe_input=10000, pe_target=6000)\n",
        "\n",
        "temp_input = tf.random.uniform((64, 38, 512), dtype=tf.int64, minval=0, maxval=200)\n",
        "temp_target = tf.random.uniform((64, 36, 512), dtype=tf.int64, minval=0, maxval=200)\n",
        "\n",
        "fn_out, _ = sample_transformer([temp_input, temp_target], training=False)\n",
        "\n",
        "fn_out.shape  # (batch_size, tar_seq_len, target_vocab_size)"
      ],
      "id": "jEyIJqVcYiKv",
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 36, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOcstL-LZuZZ"
      },
      "source": [
        "num_layers=6\n",
        "d_model = 512\n",
        "dff = 2048\n",
        "num_heads = 8\n",
        "dropout_rate = 0.1"
      ],
      "id": "jOcstL-LZuZZ",
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wovssFGA1eet"
      },
      "source": [
        "START = -2\n",
        "def add_Start_to_targets(targets):\n",
        "  new_targets = np.insert(targets, 0, START, axis=1).reshape(-1, 81, 1)\n",
        "  return new_targets\n",
        "\n",
        "def get_targets_without_Start(targets):\n",
        "  return targets[:, 1:]"
      ],
      "id": "wovssFGA1eet",
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_zBkOsH31PO"
      },
      "source": [
        "new_targets = add_Start_to_targets(targets)\n",
        "new_targets.shape, targets.shape"
      ],
      "id": "C_zBkOsH31PO"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--4JMOdJ4fxF"
      },
      "source": [
        "learning_rate = CustomSchedule(d_model)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
        "                                     epsilon=1e-9)"
      ],
      "id": "--4JMOdJ4fxF",
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvnVusZg4f6X"
      },
      "source": [
        "loss_object = tf.keras.losses.MeanAbsoluteError()"
      ],
      "id": "EvnVusZg4f6X",
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-StJ03bs9To2"
      },
      "source": [
        "y_true = tf.constant([[[0.], [1.]], [[0.], [0.]]])\n",
        "y_pred = tf.constant([[[2.], [1.]], [[2.], [0.]]])\n",
        "y_true.shape, loss_function(y_true, y_pred)"
      ],
      "id": "-StJ03bs9To2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InsS81eV8HEW"
      },
      "source": [
        "y_true = tf.constant([[[0.], [1.]], [[0.], [0.]]])\n",
        "y_pred = tf.constant([[[2.], [1.]], [[2.], [0.]]])\n",
        "y_true.shape, accuracy_function(y_true, y_pred)"
      ],
      "id": "InsS81eV8HEW"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7dNXdoL_FDH"
      },
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')"
      ],
      "id": "c7dNXdoL_FDH",
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3L1_ptyU_Gw1"
      },
      "source": [
        "def get_transformer():\n",
        "  transformer = Transformer(\n",
        "      num_layers=num_layers,\n",
        "      d_model=d_model,\n",
        "      num_heads=num_heads,\n",
        "      dff=dff,\n",
        "      # input_vocab_size=tokenizers.pt.get_vocab_size().numpy(),\n",
        "      # target_vocab_size=tokenizers.en.get_vocab_size().numpy(),\n",
        "      pe_input=80,\n",
        "      pe_target=80,\n",
        "      rate=dropout_rate)\n",
        "  return transformer"
      ],
      "id": "3L1_ptyU_Gw1",
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqNFfg8X_G0E"
      },
      "source": [
        "checkpoint_path = \"./checkpoints/train\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "# if a checkpoint exists, restore the latest checkpoint.\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "  print('Latest checkpoint restored!!')"
      ],
      "id": "MqNFfg8X_G0E",
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nt7ahcTE_YFV"
      },
      "source": [
        "EPOCHS = 20\n",
        "BATCH_SIZE = 64#1024\n",
        "NUM_FOLDS = 7"
      ],
      "id": "nt7ahcTE_YFV",
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WeSVrUJUWs-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd43b45c-2f5f-4b9d-f490-96c8d6fe9af2"
      },
      "source": [
        "# Function to get hardware strategy\n",
        "def get_hardware_strategy():\n",
        "    try:\n",
        "        # TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
        "        # set: this is always the case on Kaggle.\n",
        "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "        print('Running on TPU ', tpu.master())\n",
        "    except ValueError:\n",
        "        tpu = None\n",
        "\n",
        "    if tpu:\n",
        "        tf.config.experimental_connect_to_cluster(tpu)\n",
        "        tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "        tf.config.optimizer.set_jit(True)\n",
        "    else:\n",
        "        # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
        "        strategy = tf.distribute.get_strategy()\n",
        "\n",
        "    return tpu, strategy\n",
        "\n",
        "tpu, strategy = get_hardware_strategy()"
      ],
      "id": "2WeSVrUJUWs-",
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on TPU  grpc://10.84.87.178:8470\n",
            "INFO:tensorflow:Clearing out eager caches\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:TPU system grpc://10.84.87.178:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:TPU system grpc://10.84.87.178:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.84.87.178:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.84.87.178:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n",
            "WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDUQqHUv_YOb"
      },
      "source": [
        "# The @tf.function trace-compiles train_step into a TF graph for faster\n",
        "# execution. The function specializes to the precise shape of the argument\n",
        "# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n",
        "# batch sizes (the last batch is smaller), use input_signature to specify\n",
        "# more generic shapes.\n",
        "\n",
        "train_step_signature = [\n",
        "    tf.TensorSpec(shape=(None, 80, 64), dtype=tf.float32),\n",
        "    tf.TensorSpec(shape=(None, 81, 1), dtype=tf.float32),\n",
        "]\n",
        "\n",
        "\n",
        "@tf.function(input_signature=train_step_signature)\n",
        "def train_step(inp, tar):\n",
        "  tar_inp = tar[:, :-1, :]\n",
        "  tar_real = tar[:, 1:, :]\n",
        "  print(inp.shape, tar_inp.shape, tar_real.shape)\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions, _ = transformer([inp, tar_inp],\n",
        "                                 training = True)\n",
        "    loss = loss_function(tar_real, predictions)\n",
        "\n",
        "  gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "  train_loss(loss)\n",
        "  train_accuracy(accuracy_function(tar_real, predictions))"
      ],
      "id": "RDUQqHUv_YOb",
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ko54Rz-DAayH",
        "outputId": "76902bc9-1461-4453-8600-7adf0c15bce3"
      },
      "source": [
        "def make_batches(ds):\n",
        "  return (\n",
        "      ds\n",
        "      .cache()\n",
        "      # .shuffle(BUFFER_SIZE)\n",
        "      .batch(BATCH_SIZE)\n",
        "      # .map(tokenize_pairs, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "      .prefetch(tf.data.AUTOTUNE))\n",
        "\n",
        "# with strategy.scope():\n",
        "\n",
        "  # loss_object = tf.keras.losses.MeanAbsoluteError(reduction=tf.keras.losses.Reduction.NONE)\n",
        "\n",
        "transformer = get_transformer()\n",
        "\n",
        "kf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=2021)\n",
        "test_preds = []\n",
        "train_preds = []\n",
        "for fold, (train_idx, test_idx) in enumerate(kf.split(train, targets)):\n",
        "  print('-'*15, '>', f'Fold {fold+1}', '<', '-'*15)\n",
        "  X_train, X_valid = train[train_idx], train[test_idx]\n",
        "  y_train, y_valid = targets[train_idx], targets[test_idx]\n",
        "\n",
        "  train_dataset = tf.data.Dataset.from_tensor_slices((tf.cast(X_train, dtype=tf.float32), tf.cast(add_Start_to_targets(y_train), dtype=tf.float32)))\n",
        "  print(train_dataset)\n",
        "  valid_dataset = tf.data.Dataset.from_tensor_slices((X_valid, y_valid))\n",
        "\n",
        "  train_batches = make_batches(train_dataset)\n",
        "  val_batches = make_batches(valid_dataset)\n",
        "\n",
        "  for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "\n",
        "    # inp -> portuguese, tar -> english\n",
        "    for (batch, (inp, tar)) in enumerate(train_batches):\n",
        "      # print(inp.shape, tar.shape)\n",
        "      train_step(inp, tar)\n",
        "\n",
        "      if batch % 50 == 0:\n",
        "        print(f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
        "\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "      ckpt_save_path = ckpt_manager.save()\n",
        "      print(f'Saving checkpoint for epoch {epoch+1} at {ckpt_save_path}')\n",
        "\n",
        "    print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
        "\n",
        "    print(f'Time taken for 1 epoch: {time.time() - start:.2f} secs\\n')\n",
        "    break\n",
        "  break\n"
      ],
      "id": "ko54Rz-DAayH",
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------- > Fold 1 < ---------------\n",
            "<TensorSliceDataset shapes: ((80, 64), (81, 1)), types: (tf.float32, tf.float32)>\n",
            "(None, 80, 64) (None, 80, 1) (None, 80, 1)\n",
            "(None, 80, 64) (None, 80, 1) (None, 80, 1)\n",
            "Epoch 1 Batch 0 Loss 10.6926 Accuracy 0.0000\n",
            "Epoch 1 Loss 9.3230 Accuracy 0.0000\n",
            "Time taken for 1 epoch: 43.25 secs\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1c-i5izW_ykz"
      },
      "source": [
        ""
      ],
      "id": "1c-i5izW_ykz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lfZdYMsC-G3"
      },
      "source": [
        "#reset Keras Session\n",
        "def reset_keras():\n",
        "    sess = tf.compat.v1.keras.backend.get_session()\n",
        "    tf.compat.v1.keras.backend.clear_session()\n",
        "    sess.close()\n",
        "    sess = tf.compat.v1.keras.backend.get_session()\n",
        "\n",
        "    # use the same config as you used to create the session\n",
        "    config = tf.compat.v1.ConfigProto()\n",
        "    config.gpu_options.per_process_gpu_memory_fraction = 1\n",
        "    config.gpu_options.visible_device_list = \"0\"\n",
        "    tf.compat.v1.keras.backend.set_session(tf.compat.v1.Session(config=config))\n",
        "    gc.collect()"
      ],
      "id": "3lfZdYMsC-G3",
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZR_KtDCXk-c"
      },
      "source": [
        "print('Train MAE:', mae(targets.flatten(), np.median(np.vstack(train_preds),axis=0)))"
      ],
      "id": "0ZR_KtDCXk-c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7025c5f6"
      },
      "source": [
        "PRESSURE_STEP = 0.07030214545120961\n",
        "PRESSURE_MIN = -1.8957442945646408\n",
        "PRESSURE_MAX = 64.82099173863948"
      ],
      "id": "7025c5f6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41dffad5"
      },
      "source": [
        "# ENSEMBLE FOLDS WITH MEAN\n",
        "submission[\"pressure\"] = sum(test_preds)/len(test_preds)\n",
        "submission.to_csv('submission_mean.csv', index=False)"
      ],
      "id": "41dffad5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bb56003"
      },
      "source": [
        "# ENSEMBLE FOLDS WITH MEDIAN\n",
        "submission[\"pressure\"] = np.median(np.vstack(test_preds),axis=0)\n",
        "submission.to_csv('submission_median.csv', index=False)"
      ],
      "id": "6bb56003",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9236b88e"
      },
      "source": [
        "# ENSEMBLE FOLDS WITH MEDIAN AND ROUND PREDICTIONS\n",
        "submission[\"pressure\"] =\\\n",
        "    np.round( (submission.pressure - PRESSURE_MIN)/PRESSURE_STEP ) * PRESSURE_STEP + PRESSURE_MIN\n",
        "submission.pressure = np.clip(submission.pressure, PRESSURE_MIN, PRESSURE_MAX)\n",
        "submission.to_csv('submission_median_round.csv', index=False)"
      ],
      "id": "9236b88e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5z9iaJ--P8P"
      },
      "source": [
        "if IN_COLAB:\n",
        "  !zip submission.zip submission_median_round.csv\n",
        "  with open(model_folder + 'submission.zip', 'wb') as f:\n",
        "    f.write(open('submission.zip', 'rb').read())\n",
        "  # !zip cnn_models.zip *.hdf5\n",
        "  # with open(model_folder + 'cnn_models.zip', 'wb') as f:\n",
        "  #   f.write(open('cnn_models.zip', 'rb').read())"
      ],
      "id": "K5z9iaJ--P8P",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEBQkXgWYR51"
      },
      "source": [
        ""
      ],
      "id": "qEBQkXgWYR51",
      "execution_count": null,
      "outputs": []
    }
  ]
}